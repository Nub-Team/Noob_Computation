{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecommendationSystems.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txi63Qo7TPr9"
      },
      "source": [
        "<i>Recommendation Systems</i><br>\r\n",
        "--\r\n",
        "Author by : \r\n",
        "* Nub-T\r\n",
        "* D. Johanes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zip_HWB9SUCQ"
      },
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-latest-small.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24P0ZzHMTbia"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "CUR_DIR = os.path.abspath(os.path.curdir)\r\n",
        "movie_zip = zipfile.ZipFile(CUR_DIR + '/ml-latest-small.zip')\r\n",
        "movie_zip.extractall()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncuuwB2hTfiZ"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from scipy import  sparse, linalg"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJmaixtITmLS"
      },
      "source": [
        "links = pd.read_csv(CUR_DIR + '/ml-latest-small/links.csv')\r\n",
        "movies = pd.read_csv(CUR_DIR + '/ml-latest-small/movies.csv')\r\n",
        "ratings = pd.read_csv(CUR_DIR + '/ml-latest-small/ratings.csv')\r\n",
        "tags = pd.read_csv(CUR_DIR + '/ml-latest-small/tags.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5WH-2aJToE9"
      },
      "source": [
        "# Content base Filtering"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiHIe5m3Tp06"
      },
      "source": [
        "movies_genres = pd.concat([movies.loc[:,['movieId','title']],movies.genres.str.split('|', expand=False)], axis=1)\r\n",
        "movies_genres = movies_genres.explode('genres')\r\n",
        "movies_genres = pd.get_dummies(movies_genres,columns=['genres'])\r\n",
        "movies_genres = movies_genres.groupby(['movieId'], as_index=False).sum()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "odTR9P0zTtOI",
        "outputId": "36d41534-86b4-47fc-949b-4eed77c7dfb5"
      },
      "source": [
        "assert movies_genres.iloc[:,1:].max().max() == 1\r\n",
        "movies_genres.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>genres_(no genres listed)</th>\n",
              "      <th>genres_Action</th>\n",
              "      <th>genres_Adventure</th>\n",
              "      <th>genres_Animation</th>\n",
              "      <th>genres_Children</th>\n",
              "      <th>genres_Comedy</th>\n",
              "      <th>genres_Crime</th>\n",
              "      <th>genres_Documentary</th>\n",
              "      <th>genres_Drama</th>\n",
              "      <th>genres_Fantasy</th>\n",
              "      <th>genres_Film-Noir</th>\n",
              "      <th>genres_Horror</th>\n",
              "      <th>genres_IMAX</th>\n",
              "      <th>genres_Musical</th>\n",
              "      <th>genres_Mystery</th>\n",
              "      <th>genres_Romance</th>\n",
              "      <th>genres_Sci-Fi</th>\n",
              "      <th>genres_Thriller</th>\n",
              "      <th>genres_War</th>\n",
              "      <th>genres_Western</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   movieId  genres_(no genres listed)  ...  genres_War  genres_Western\n",
              "0        1                          0  ...           0               0\n",
              "1        2                          0  ...           0               0\n",
              "2        3                          0  ...           0               0\n",
              "3        4                          0  ...           0               0\n",
              "4        5                          0  ...           0               0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZ8iqBRTu_p"
      },
      "source": [
        "ratings = pd.read_csv(CUR_DIR + '/ml-latest-small/ratings.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmK0beL4Tw_V"
      },
      "source": [
        "C = 3\r\n",
        "total_mean = ratings.rating.mean()\r\n",
        "ratings['normalized_rating'] = ratings.rating - total_mean"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eud_2Wv8TzHs"
      },
      "source": [
        "b_item = ratings.groupby('movieId').normalized_rating.sum() / (ratings.groupby('movieId').userId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_item, columns=['b_item']), left_on='movieId', right_index=True, how='inner')\r\n",
        "ratings['norm_item_rating'] = ratings.normalized_rating - ratings.b_item"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uTLBSsDT2BX",
        "outputId": "5d98e659-c511-4051-9b4c-3bc891b4406e"
      },
      "source": [
        "b_item"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "movieId\n",
              "1         0.413602\n",
              "2        -0.067887\n",
              "3        -0.228745\n",
              "4        -0.801090\n",
              "5        -0.405313\n",
              "            ...   \n",
              "193581    0.124611\n",
              "193583   -0.000389\n",
              "193585   -0.000389\n",
              "193587   -0.000389\n",
              "193609    0.124611\n",
              "Length: 9724, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rowbvRu5T5-Z"
      },
      "source": [
        "b_user = ratings.groupby('userId').norm_item_rating.sum() / (ratings.groupby('userId').movieId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_user, columns=['b_user']), left_on='userId', right_index=True, how='inner')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrkxpbcPT8Ro",
        "outputId": "f458c03c-1a47-451d-add4-97ae6e04df03"
      },
      "source": [
        "b_user"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "userId\n",
              "1      0.791627\n",
              "2      0.055496\n",
              "3     -0.985596\n",
              "4     -0.169825\n",
              "5     -0.060708\n",
              "         ...   \n",
              "606    0.089329\n",
              "607    0.266008\n",
              "608   -0.199666\n",
              "609   -0.230366\n",
              "610    0.193503\n",
              "Length: 610, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPMgoXanT-3z"
      },
      "source": [
        "ratings['normr_user_item_rating'] = total_mean + ratings.b_item + ratings.b_user\r\n",
        "urm = ratings.pivot(index='userId', columns='movieId', values='normr_user_item_rating').fillna(0.).values"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E3l_mVLUBHq"
      },
      "source": [
        "shrink_term = 3\r\n",
        "movies_genres_mat = sparse.csr_matrix(movies_genres.iloc[:,1:].values)\r\n",
        "\r\n",
        "movie_norms = np.sqrt(movies_genres_mat.sum(axis=1)).reshape(-1,1)\r\n",
        "xy, yx = np.meshgrid(movie_norms, movie_norms)\r\n",
        "xy, yx = np.array(xy), np.array(yx)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdh12IgOUExt"
      },
      "source": [
        "cbf_similarity_mat = movies_genres_mat.dot(movies_genres_mat.transpose())\r\n",
        "cbf_similarity_mat = np.array(cbf_similarity_mat / (xy * yx + shrink_term))\r\n",
        "np.fill_diagonal(cbf_similarity_mat, 0.)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-e8WzKCUGFW",
        "outputId": "7db38b04-5dca-43c1-f0aa-0f0166c234fa"
      },
      "source": [
        "cbf_similarity_mat"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.43649167, 0.16227766, ..., 0.        , 0.16227766,\n",
              "        0.19098301],\n",
              "       [0.43649167, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.16227766, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.22654092],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.16227766, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.19098301, 0.        , 0.22654092, ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-VpGJWhUHRp"
      },
      "source": [
        "movies['idx'] = movies.index"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb0RNaRUUO6U"
      },
      "source": [
        "def get_similar_movies(k, movie_name):\r\n",
        "  movie_idx = movies.set_index('title').loc[movie_name,'idx']\r\n",
        "  movie_idxs = np.argsort(cbf_similarity_mat[movie_idx,:])[-k:]\r\n",
        "  return movies.loc[np.flip(movie_idxs),['title','genres']]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m_b6B3PUSgW"
      },
      "source": [
        "def cbf_get_rating_given_user(u_ix, item_ix, k):\r\n",
        "  movie_idxs = np.argsort(cbf_similarity_mat[item_ix,:])[-k:].squeeze()\r\n",
        "  subusers_items = urm[u_ix,movie_idxs].squeeze()\r\n",
        "  masked_subusers_items = np.ma.array(subusers_items, mask=subusers_items == 0.) \r\n",
        "  weights = cbf_similarity_mat[item_ix, movie_idxs].squeeze()\r\n",
        "\r\n",
        "  w_avg = np.ma.average(a=masked_subusers_items, weights=weights)\r\n",
        "  return np.where(w_avg == np.ma.masked, 0., w_avg), masked_subusers_items, weights"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiTfpC3wUTvc",
        "outputId": "6c4454fe-49ce-41e9-c3b9-b9c8456fc1e5"
      },
      "source": [
        "cbf_get_rating_given_user(0,0,100)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(4.16856985),\n",
              " masked_array(data=[4.270988316863774, --, --, --, 4.723657525010937, --,\n",
              "                    --, 4.17545658424152, --, 4.243639930145966, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, 3.54171043708161, --, --,\n",
              "                    --, --, --, --, --, --, --, --, --, --, --, --, --, --,\n",
              "                    --, --, --, --, --, --, --, --],\n",
              "              mask=[False,  True,  True,  True, False,  True,  True, False,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True],\n",
              "        fill_value=1e+20),\n",
              " array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
              "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.53532217, 0.53532217, 0.53532217,\n",
              "        0.53532217, 0.53532217, 0.56078457, 0.56078457, 0.56078457,\n",
              "        0.58981561, 0.58981561, 0.58981561, 0.58981561, 0.58981561,\n",
              "        0.58981561, 0.58981561, 0.58981561, 0.58981561, 0.58981561,\n",
              "        0.58981561, 0.58981561, 0.58981561, 0.625     , 0.625     ,\n",
              "        0.625     , 0.625     , 0.625     , 0.625     , 0.625     ,\n",
              "        0.625     , 0.625     , 0.625     , 0.625     , 0.625     ]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "bamKRU33UWQm",
        "outputId": "97f40e59-803f-4bce-9fa7-16334f2d25a4"
      },
      "source": [
        "get_similar_movies(10, 'Toy Story 2 (1999)')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6948</th>\n",
              "      <td>Tale of Despereaux, The (2008)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1706</th>\n",
              "      <td>Antz (1998)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2809</th>\n",
              "      <td>Adventures of Rocky and Bullwinkle, The (2000)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3000</th>\n",
              "      <td>Emperor's New Groove, The (2000)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8927</th>\n",
              "      <td>The Good Dinosaur (2015)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3568</th>\n",
              "      <td>Monsters, Inc. (2001)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8219</th>\n",
              "      <td>Turbo (2013)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6194</th>\n",
              "      <td>Wild, The (2006)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6486</th>\n",
              "      <td>Shrek the Third (2007)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title                                       genres\n",
              "0                                   Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "6948                  Tale of Despereaux, The (2008)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "1706                                     Antz (1998)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "2809  Adventures of Rocky and Bullwinkle, The (2000)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "3000                Emperor's New Groove, The (2000)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "8927                        The Good Dinosaur (2015)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "3568                           Monsters, Inc. (2001)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "8219                                    Turbo (2013)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "6194                                Wild, The (2006)  Adventure|Animation|Children|Comedy|Fantasy\n",
              "6486                          Shrek the Third (2007)  Adventure|Animation|Children|Comedy|Fantasy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "4dI4mRa1UZv6",
        "outputId": "63bee004-af39-4028-e100-b8d50dd791ad"
      },
      "source": [
        "# Collaborative Filtering\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "fig = plt.figure(figsize=(10,10))\r\n",
        "sns.distplot(ratings.rating, bins=50)\r\n",
        "fig.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "a258iCXVUeaO",
        "outputId": "56c4e9c7-d186-4995-9f6b-fa2b4f8356c3"
      },
      "source": [
        "ratings.groupby('movieId').agg({'userId':'count'}).sort_values('userId',ascending=False).loc[:500,:].plot.bar()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f530aef92b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "XnRV_EwFUgxt",
        "outputId": "5ee52818-46a5-4321-a6cc-dd64413bd906"
      },
      "source": [
        "ratings.groupby('movieId').agg({'rating':np.mean}).sort_values('rating',ascending=False).plot.hist()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f530ae0e208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD4CAYAAAAdIcpQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS6ElEQVR4nO3df/BddX3n8eeLmBoJKUGgLJIsSXcy/CyE+BVxaFepUwzQiu1ORS0FGYZ0xjjqdMdtdDqL244rOyOKdFoHWjJAW2RpqZUtsTbSFMeZFZLYNBDQSUbC8g2RBKgSKojY9/5xzxe+QJJzv/C933OT+3zM3LnnfO45575z/vi+8vmczz0nVYUkSftzSNcFSJKGn2EhSWplWEiSWhkWkqRWhoUkqdXrui5gEI466qhatGhR12VI0gFl48aNj1fV0Xv77KAMi0WLFrFhw4auy5CkA0qSh/f1mcNQkqRWhoUkqZVhIUlqdVBes5Ck/fnJT37C+Pg4zz77bNeldGLOnDksWLCA2bNn972PYSFp5IyPjzNv3jwWLVpEkq7LmVFVxRNPPMH4+DiLFy/uez+HoSSNnGeffZYjjzxy5IICIAlHHnnklHtVhoWkkTSKQTHh1fzbDQtJUiuvWUgaeYtW3Tmtx9t+1QXTcpxrrrmGFStWcOihhwJw/vnnc8sttzB//vxpOf5UGBaSBm66/xj3a7r+aA9SVVFVHHLIKwd6rrnmGi6++OIXwmLNmjUzXd4LHIaSpBm2fft2TjjhBC655BJOPfVULr/8csbGxjjllFO48sorAbj22mt59NFHOeecczjnnHOA3q2MHn/8cbZv385JJ53EFVdcwSmnnMK5557LM888A8D69es57bTTWLp0KR//+Mc59dRTp6Vmw0KSOrB161Y+9KEPsWXLFq6++mo2bNjA5s2bufvuu9m8eTMf+chHeNOb3sS6detYt27dXvdfuXIlW7ZsYf78+dx+++0AXHbZZVx33XVs2rSJWbNmTVu9hoUkdeD444/nrLPOAuC2225j2bJlnHHGGWzZsoUHHnigdf/FixezdOlSAN785jezfft2fvCDH7Bnzx7e9ra3AfCBD3xg2ur1moUkdWDu3LkAPPTQQ3z2s59l/fr1HHHEEXzwgx/s6zcQr3/9619YnjVr1gvDUINiz0KSOvTUU08xd+5cDj/8cB577DG++tWvvvDZvHnz2LNnT9/Hmj9/PvPmzeOee+4B4NZbb522Ou1ZSCOiqxlJB4IuZ02dfvrpnHHGGZx44oksXLiQs88++4XPVqxYwfLly1+4dtGPG264gSuuuIJDDjmEt7/97Rx++OHTUmeqaloONEzGxsbKhx9JLzWKYbGvEHjwwQc56aSTZriamfH0009z2GGHAXDVVVexc+dOvvCFL7xiu72dgyQbq2psb8e1ZyFJB5E777yTz3zmMzz//PMcf/zx3HjjjdNyXMNCkg4iF110ERdddNG0H9cL3JJG0sE4BN+vV/NvNywkjZw5c+bwxBNPjGRgTDzPYs6cOVPaz2EoSSNnwYIFjI+Ps3v37q5L6cTEk/KmwrCQNHJmz549pafEyWEoSVIfDAtJUivDQpLUyrCQJLUyLCRJrQwLSVIrw0KS1MqwkCS1GlhYJFmYZF2SB5JsSfLRpv2NSdYm2dq8H9G0J8m1SbYl2Zxk2aRjXdpsvzXJpYOqWZK0d4PsWTwP/NeqOhk4C1iZ5GRgFXBXVS0B7mrWAc4DljSvFcAXoRcuwJXAW4EzgSsnAkaSNDMGFhZVtbOqvt0s7wEeBI4DLgRuaja7CXhPs3whcHP1fAuYn+RY4F3A2qp6sqr+FVgLLB9U3ZKkV5qRaxZJFgFnAPcAx1TVzuaj7wPHNMvHAY9M2m28adtX+8u/Y0WSDUk2jOrNwSRpUAYeFkkOA24HPlZVT03+rHr3B56WewRX1fVVNVZVY0cfffR0HFKS1BhoWCSZTS8o/rKq/qZpfqwZXqJ539W07wAWTtp9QdO2r3ZJ0gwZ5GyoADcAD1bV5yZ9dAcwMaPpUuArk9ovaWZFnQX8sBmu+hpwbpIjmgvb5zZtkqQZMsjnWZwN/DZwX5JNTdsngauA25JcDjwMvLf5bA1wPrAN+BFwGUBVPZnkD4H1zXZ/UFVPDrBuSdLLDCwsquqbQPbx8Tv3sn0BK/dxrNXA6umrTpI0Ff6CW5LUyrCQJLUyLCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0MC0lSq0H+glvSXixadWfXJUhTZs9CktTKsJAktTIsJEmtDAtJUivDQpLUyrCQJLUyLCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0MC0lSK8NCktTKsJAktTIsJEmtDAtJUivDQpLUyrCQJLUyLCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0MC0lSK8NCktTKsJAktTIsJEmtBhYWSVYn2ZXk/kltn0qyI8mm5nX+pM8+kWRbku8medek9uVN27YkqwZVryRp3wbZs7gRWL6X9s9X1dLmtQYgycnA+4BTmn3+JMmsJLOAPwbOA04G3t9sK0maQa8b1IGr6htJFvW5+YXArVX1Y+ChJNuAM5vPtlXV9wCS3Nps+8A0lytJ2o8urll8OMnmZpjqiKbtOOCRSduMN237an+FJCuSbEiyYffu3YOoW5JG1kyHxReB/wQsBXYCV0/Xgavq+qoaq6qxo48+eroOK0ligMNQe1NVj00sJ/lT4O+a1R3AwkmbLmja2E+7JGmGzGjPIsmxk1Z/HZiYKXUH8L4kr0+yGFgC3AusB5YkWZzkZ+hdBL9jJmuWJA2wZ5HkS8A7gKOSjANXAu9IshQoYDvwOwBVtSXJbfQuXD8PrKyqnzbH+TDwNWAWsLqqtgyqZknS3vUVFkl+oarum8qBq+r9e2m+YT/bfxr49F7a1wBrpvLdkqTp1e8w1J8kuTfJh5IcPtCKJElDp6+wqKpfAn6L3sXmjUluSfIrA61MkjQ0+r7AXVVbgd8Hfg94O3Btku8k+Y1BFSdJGg59hUWS05J8HngQ+GXg16rqpGb58wOsT5I0BPqdDfVHwJ8Bn6yqZyYaq+rRJL8/kMokSUOj37C4AHhm0nTWQ4A5VfWjqvrzgVUnSRoK/V6z+DrwhknrhzZtkqQR0G9YzKmqpydWmuVDB1OSJGnY9BsW/5Zk2cRKkjcDz+xne0nSQaTfaxYfA/4qyaNAgP8AXDSwqiRJQ6WvsKiq9UlOBE5omr5bVT8ZXFmSpGEylRsJvgVY1OyzLAlVdfNAqpIkDZV+byT45/QeWrQJ+GnTXIBhIUkjoN+exRhwclXVIIuRJA2nfmdD3U/vorYkaQT127M4Cnggyb3Ajycaq+rdA6lKkjRU+g2LTw2yCEnScOt36uzdSY4HllTV15McSu8xp5KkEdDvLcqvAP4auK5pOg7420EVJUkaLv1e4F4JnA08BS88COnnBlWUJGm49BsWP66q5yZWkryO3u8sJEkjoN+wuDvJJ4E3NM/e/ivg/wyuLEnSMOk3LFYBu4H7gN8B1tB7HrckaQT0Oxvq34E/bV6SpBHT772hHmIv1yiq6uenvSJJ0tCZyr2hJswBfhN44/SXI0kaRn1ds6iqJya9dlTVNcAFA65NkjQk+h2GWjZp9RB6PY2pPAtDknQA6/cP/tWTlp8HtgPvnfZqJElDqd/ZUOcMuhBJ0vDqdxjqd/f3eVV9bnrKkSQNo6nMhnoLcEez/mvAvcDWQRQlSRou/YbFAmBZVe0BSPIp4M6qunhQhUmShke/t/s4Bnhu0vpzTZskaQT027O4Gbg3yZeb9fcANw2mJEnSsOl3NtSnk3wV+KWm6bKq+ufBlSVJGib9DkMBHAo8VVVfAMaTLB5QTZKkIdPvY1WvBH4P+ETTNBv4i5Z9VifZleT+SW1vTLI2ydbm/YimPUmuTbItyebJvxhPcmmz/dYkl071HyhJeu367Vn8OvBu4N8AqupRYF7LPjcCy1/Wtgq4q6qWAHc16wDnAUua1wrgi9ALF+BK4K3AmcCVEwEjSZo5/YbFc1VVNLcpTzK3bYeq+gbw5MuaL+TFC+M30btQPtF+c/V8C5if5FjgXcDaqnqyqv4VWMsrA0iSNGD9hsVtSa6j90f8CuDrvLoHIR1TVTub5e/z4vTb44BHJm033rTtq/0VkqxIsiHJht27d7+K0iRJ+9I6GypJgP8NnAg8BZwA/PeqWvtavriqKskrHqj0Go53PXA9wNjY2LQdV5LUR1g0f9TXVNUv0BsGei0eS3JsVe1shpl2Ne07gIWTtlvQtO0A3vGy9n96jTVIkqao3x/lfTvJW6pq/Wv8vjuAS4GrmvevTGr/cJJb6V3M/mETKF8D/ueki9rn8uKMLOlVW7Tqzq5LkA4o/YbFW4GLk2ynNyMq9Dodp+1rhyRfotcrOCrJOL1ZTVfRu/5xOfAwLz4TYw1wPrAN+BFwGb0veDLJHwITIfUHVfXyi+aSpAHbb1gk+Y9V9f/ozUqakqp6/z4+eudeti1g5T6OsxpYPdXvlyRNn7aexd/Su9vsw0lur6r/MhNFSZKGS9vU2Uxa/vlBFiJJGl5tYVH7WJYkjZC2YajTkzxFr4fxhmYZXrzA/bMDrU6SNBT2GxZVNWumCpEkDa+p3KJckjSiDAtJUivDQpLUyrCQJLUyLCRJrQwLSVIrw0KS1Krfu85K0gGny1vRb7/qgs6+exDsWUiSWhkWkqRWhoUkqZVhIUlqZVhIkloZFpKkVoaFJKmVYSFJamVYSJJa+QtudarLX9hK6p89C0lSK8NCktTKsJAktTIsJEmtDAtJUivDQpLUyrCQJLUyLCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0MC0lSq07CIsn2JPcl2ZRkQ9P2xiRrk2xt3o9o2pPk2iTbkmxOsqyLmiVplHXZszinqpZW1Vizvgq4q6qWAHc16wDnAUua1wrgizNeqSSNuGEahroQuKlZvgl4z6T2m6vnW8D8JMd2UaAkjaquwqKAf0iyMcmKpu2YqtrZLH8fOKZZPg54ZNK+403bSyRZkWRDkg27d+8eVN2SNJK6elLeL1bVjiQ/B6xN8p3JH1ZVJampHLCqrgeuBxgbG5vSvpKk/eukZ1FVO5r3XcCXgTOBxyaGl5r3Xc3mO4CFk3Zf0LRJkmbIjIdFkrlJ5k0sA+cC9wN3AJc2m10KfKVZvgO4pJkVdRbww0nDVZKkGdDFMNQxwJeTTHz/LVX190nWA7cluRx4GHhvs/0a4HxgG/Aj4LKZL1mSRtuMh0VVfQ84fS/tTwDv3Et7AStnoDRJ0j4M09RZSdKQMiwkSa0MC0lSK8NCktTKsJAktTIsJEmtDAtJUivDQpLUyrCQJLXq6q6zknRQW7Tqzk6+d/tVFwzkuPYsJEmtDAtJUiuHoQR012WWdGCwZyFJamVYSJJaGRaSpFaGhSSplWEhSWplWEiSWhkWkqRWhoUkqZVhIUlqZVhIklp5u48h4i03JA0rexaSpFaGhSSplWEhSWplWEiSWhkWkqRWhoUkqZVTZ/fCKayS9FL2LCRJrQwLSVIrw0KS1MqwkCS1MiwkSa0MC0lSqwMmLJIsT/LdJNuSrOq6HkkaJQdEWCSZBfwxcB5wMvD+JCd3W5UkjY4DIiyAM4FtVfW9qnoOuBW4sOOaJGlkHCi/4D4OeGTS+jjw1skbJFkBrGhWn07y3RmqbVCOAh7vuogh4vl4Kc/HizwXk+R/vabzcfy+PjhQwqJVVV0PXN91HdMlyYaqGuu6jmHh+Xgpz8eLPBcvNajzcaAMQ+0AFk5aX9C0SZJmwIESFuuBJUkWJ/kZ4H3AHR3XJEkj44AYhqqq55N8GPgaMAtYXVVbOi5r0A6aIbVp4vl4Kc/HizwXLzWQ85GqGsRxJUkHkQNlGEqS1CHDQpLUyrAYMklWJ9mV5P6uaxkGSRYmWZfkgSRbkny065q6kmROknuT/EtzLv5H1zUNgySzkvxzkr/rupauJdme5L4km5JsmNZje81iuCT5z8DTwM1VdWrX9XQtybHAsVX17STzgI3Ae6rqgY5Lm3FJAsytqqeTzAa+CXy0qr7VcWmdSvK7wBjws1X1q13X06Uk24Gxqpr2HynasxgyVfUN4Mmu6xgWVbWzqr7dLO8BHqT3i/6RUz1PN6uzm9dI/28vyQLgAuDPuq7lYGdY6ICRZBFwBnBPt5V0pxly2QTsAtZW1ciei8Y1wH8D/r3rQoZEAf+QZGNzC6RpY1jogJDkMOB24GNV9VTX9XSlqn5aVUvp3cXgzCQjO1SZ5FeBXVW1setahsgvVtUyenfoXtkMa08Lw0JDrxmfvx34y6r6m67rGQZV9QNgHbC861o6dDbw7mac/lbgl5P8RbcldauqdjTvu4Av07tj97QwLDTUmou6NwAPVtXnuq6nS0mOTjK/WX4D8CvAd7qtqjtV9YmqWlBVi+jdAugfq+rijsvqTJK5zSQQkswFzgWmbValYTFkknwJ+L/ACUnGk1zedU0dOxv4bXr/a9zUvM7vuqiOHAusS7KZ3v3S1lbVyE8X1QuOAb6Z5F+Ae4E7q+rvp+vgTp2VJLWyZyFJamVYSJJaGRaSpFaGhSSplWEhSWplWEiSWhkWkqRW/x/IcP1F9g9RhgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8N3yIytUiyO"
      },
      "source": [
        "ratings = pd.read_csv(CUR_DIR + '/ml-latest-small/ratings.csv')\r\n",
        "\r\n",
        "C = 3\r\n",
        "total_mean = ratings.rating.mean()\r\n",
        "ratings['normalized_rating'] = ratings.rating - total_mean\r\n",
        "\r\n",
        "b_item = ratings.groupby('movieId').normalized_rating.sum() / (ratings.groupby('movieId').userId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_item, columns=['b_item']), left_on='movieId', right_index=True, how='inner')\r\n",
        "ratings['norm_item_rating'] = ratings.normalized_rating - ratings.b_item\r\n",
        "\r\n",
        "b_user = ratings.groupby('userId').norm_item_rating.sum() / (ratings.groupby('userId').movieId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_user, columns=['b_user']), left_on='userId', right_index=True, how='inner')\r\n",
        "\r\n",
        "ratings['normr_user_item_rating'] = total_mean + ratings.b_item + ratings.b_user"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBYsSbADUlPZ",
        "outputId": "068f0e6e-2a79-4894-f02f-d3b0f0bf15d2"
      },
      "source": [
        "b_user"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "userId\n",
              "1      0.791627\n",
              "2      0.055496\n",
              "3     -0.985596\n",
              "4     -0.169825\n",
              "5     -0.060708\n",
              "         ...   \n",
              "606    0.089329\n",
              "607    0.266008\n",
              "608   -0.199666\n",
              "609   -0.230366\n",
              "610    0.193503\n",
              "Length: 610, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj0p0WXEUmvD"
      },
      "source": [
        "urm = ratings.pivot(index='userId', columns='movieId', values='normr_user_item_rating').fillna(0.).values\r\n",
        "\r\n",
        "user_bias = urm.mean(axis=1, keepdims=True)\r\n",
        "urm_diff = ((urm - user_bias) / np.std(urm, axis=1, keepdims=True)) / np.sqrt(urm.shape[1]) # With this trick I can do dot product for pearson corr\r\n",
        "cf_user_similarity_mat = urm_diff.dot(urm_diff.T)\r\n",
        "np.fill_diagonal(cf_user_similarity_mat, 0.)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6d0rZbRUpK9",
        "outputId": "b2537016-018d-41a7-f9ac-138e134d3f32"
      },
      "source": [
        "def ucf_get_rating_given_user(u_ix, item_ix, k):\r\n",
        "  u_ixs = np.argsort(cf_user_similarity_mat[u_ix,:])[-k:].squeeze()\r\n",
        "  subusers_item = urm_diff[u_ixs,item_ix].squeeze()\r\n",
        "  masked_subusers_item = np.ma.array(subusers_item, mask=subusers_item == 0) \r\n",
        "  weights = cf_user_similarity_mat[u_ixs, item_ix].squeeze()\r\n",
        "  w_avg = np.ma.average(a=masked_subusers_item, weights=weights) + user_bias[u_ix]\r\n",
        "  return np.where(w_avg == np.ma.masked, 0., w_avg), masked_subusers_item, weights\r\n",
        "\r\n",
        "ucf_get_rating_given_user(25,15,100)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.02519258]),\n",
              " masked_array(data=[-0.0004936160458041074, -0.0007958872268398191,\n",
              "                    -0.0004900642475044084, -0.0015557993826386743,\n",
              "                    -0.0007734350208141885, -0.0007614762523634445,\n",
              "                    -0.0005393084694960352, -0.0007968306421449825,\n",
              "                    -0.0006145799220825793, 0.12948623270153714,\n",
              "                    0.17226759591983262, -0.0011733783998427363,\n",
              "                    -0.0009026541665167581, -0.0011649901645166651,\n",
              "                    0.06541344360957123, -0.0006482707922516596,\n",
              "                    -0.0007831505543319766, -0.0010429242282300735,\n",
              "                    -0.0005124802936055692, -0.0010543455952586117,\n",
              "                    -0.0008637520802346666, -0.0006876895787634235,\n",
              "                    -0.000709639846711266, -0.0012148375068572188,\n",
              "                    -0.0010270941710860172, -0.0008192387821373582,\n",
              "                    -0.0009047869572797788, -0.0006807815231717682,\n",
              "                    -0.0011620487324293884, 0.10021711375304666,\n",
              "                    -0.0007080037709797033, -0.000884335655254621,\n",
              "                    -0.0006244389295485521, 0.10303054322747048,\n",
              "                    -0.0005975974203334681, 0.10995265517648764,\n",
              "                    -0.0009563871891140427, -0.001323823604388824,\n",
              "                    -0.001089039702551945, 0.11393880770071706,\n",
              "                    -0.0007457922426684293, -0.0009625519206665982,\n",
              "                    -0.0007941618723034337, -0.0005872947889490799,\n",
              "                    0.09488389003139644, -0.0006221320310526944,\n",
              "                    -0.0008542613509845348, 0.16557028741663773,\n",
              "                    -0.0009723219048809611, -0.0009944853124817575,\n",
              "                    -0.0011578928128458247, -0.0006477947041936146,\n",
              "                    -0.0004901387143749502, 0.1443699333689382,\n",
              "                    -0.0009068723363490453, -0.0010995528818710013,\n",
              "                    -0.0007234775347922023, 0.11243675187155222,\n",
              "                    -0.0007682052344168325, 0.14003672487772537,\n",
              "                    -0.0008269728060911871, -0.0007742877342195566,\n",
              "                    -0.0006050905200002461, -0.0005337212126599568,\n",
              "                    0.12180724671472842, -0.0007903499613210365,\n",
              "                    -0.0008401339792962194, -0.0007025180540934087,\n",
              "                    -0.0007655377341190247, -0.0005881516010219995,\n",
              "                    -0.0006877888065449344, -0.0007670010785715624,\n",
              "                    -0.0009179250153789597, -0.0005419324552799162,\n",
              "                    -0.0005507736711585297, -0.000663858771234815,\n",
              "                    -0.00045824185756015805, -0.0004699278078432742,\n",
              "                    -0.0004899930465165861, -0.0008276397002170441,\n",
              "                    -0.000605645716757273, -0.0009364220134757692,\n",
              "                    -0.0006155416114525007, -0.000632139803668708,\n",
              "                    -0.0007255697380485269, -0.0005873215180206188,\n",
              "                    -0.0004911424644027473, -0.0008522145257949509,\n",
              "                    -0.0007642184378537006, -0.0005199355761650468,\n",
              "                    -0.000469640620867313, 0.14558922251016324,\n",
              "                    -0.0005427776041106738, -0.000614118942480337,\n",
              "                    -0.0006057499735530349, -0.0006946366084958493,\n",
              "                    -0.0004918877421550758, -0.0004804116933263876,\n",
              "                    -0.00060670431262155, -0.0006310820084324112],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False],\n",
              "        fill_value=1e+20),\n",
              " array([3.73893630e-01, 1.25606678e-01, 7.29733569e-02, 2.11616442e-01,\n",
              "        9.62317848e-02, 5.82782617e-02, 1.60464512e-01, 4.51207770e-02,\n",
              "        3.73332428e-02, 2.09086274e-01, 1.13614000e-01, 1.90773874e-01,\n",
              "        2.89186184e-01, 1.88446077e-01, 5.35429405e-02, 1.11453498e-01,\n",
              "        7.34957986e-05, 2.88157872e-02, 6.96460966e-02, 6.19952562e-02,\n",
              "        1.33499686e-01, 3.12149318e-01, 9.03223691e-02, 6.03708991e-02,\n",
              "        4.12732834e-02, 5.66401692e-02, 8.13718234e-02, 8.67491424e-02,\n",
              "        9.37324715e-02, 7.88943262e-02, 1.06015091e-01, 8.07555948e-02,\n",
              "        7.54012822e-02, 3.05853993e-02, 6.09892796e-02, 5.01444534e-02,\n",
              "        9.86306232e-02, 9.56160314e-02, 7.27543126e-02, 9.53943894e-02,\n",
              "        5.58283408e-02, 4.71635890e-02, 9.73862758e-02, 1.24095285e-01,\n",
              "        1.02294154e-01, 5.93761804e-02, 3.88916631e-02, 1.44987151e-01,\n",
              "        7.46852766e-02, 6.49555169e-02, 8.67059693e-02, 5.41133467e-02,\n",
              "        1.02383693e-01, 7.61580763e-02, 6.42332282e-02, 3.69488969e-02,\n",
              "        6.81368285e-02, 4.43088992e-02, 2.68995046e-02, 1.22332563e-01,\n",
              "        7.12929217e-02, 9.37331431e-02, 5.75303838e-02, 4.20146750e-02,\n",
              "        7.41420389e-02, 2.44356426e-02, 5.48869992e-02, 1.21589360e-01,\n",
              "        1.02426619e-01, 8.09925897e-02, 2.90924758e-02, 9.19562244e-02,\n",
              "        9.10474469e-02, 4.56621002e-02, 1.32852559e-01, 7.26739338e-02,\n",
              "        7.91262031e-02, 7.20094758e-02, 4.88915325e-02, 7.14508140e-02,\n",
              "        1.39691691e-01, 6.49146844e-02, 7.66627341e-02, 1.11076171e-01,\n",
              "        1.15586838e-01, 8.26923581e-02, 9.85919060e-02, 8.33262747e-02,\n",
              "        8.04939555e-02, 9.45892120e-02, 1.03421433e-01, 1.17932245e-01,\n",
              "        6.60465759e-02, 9.91518461e-02, 9.99228783e-02, 8.70859397e-02,\n",
              "        4.63115044e-02, 9.75799095e-02, 7.76709305e-02, 9.59772415e-02]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15trH1SjUq_F"
      },
      "source": [
        "urm = ratings.pivot(index='userId', columns='movieId', values='normr_user_item_rating').fillna(0.).values\r\n",
        "\r\n",
        "user_bias = urm.mean(axis=1, keepdims=True)\r\n",
        "urm_diff = urm - user_bias\r\n",
        "urm_diff = urm_diff / np.sqrt((urm_diff ** 2).sum(axis=0, keepdims=True)) \r\n",
        "cf_item_similarity_mat = urm_diff.T.dot(urm_diff)\r\n",
        "np.fill_diagonal(cf_item_similarity_mat, 0.)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaKLw6fvUz37"
      },
      "source": [
        "def icf_get_rating_given_user(u_ix, item_ix, k):\r\n",
        "  i_ixs = np.argsort(cf_item_similarity_mat[item_ix,:])[-k:]\r\n",
        "  user_subitems = urm[u_ix,i_ixs].squeeze()\r\n",
        "  masked_user_subitems = np.ma.array(user_subitems, mask=user_subitems == 0.) \r\n",
        "  weights = cf_item_similarity_mat[item_ix, i_ixs].squeeze()\r\n",
        "  w_avg = np.ma.average(a=masked_user_subitems, weights=weights)\r\n",
        "  return np.where(w_avg == np.ma.masked, 0., w_avg), masked_user_subitems, weights"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAYsJmNnU5er",
        "outputId": "7c786991-ee27-441c-db65-54c79b1f0aea"
      },
      "source": [
        "icf_get_rating_given_user(0,55,200)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(4.55176035),\n",
              " masked_array(data=[--, --, --, --, --, --, --, --, --, 4.555866101789344,\n",
              "                    --, --, --, --, --, --, --, --, 4.551664394851795,\n",
              "                    4.8266247262019855, --, --, 4.504442675658295,\n",
              "                    4.27131418950725, --, --, --, 4.997256330753637,\n",
              "                    3.8615143136483026, --, 4.760096303705427, --, --,\n",
              "                    4.69988561807868, 4.945898910049426, --, --,\n",
              "                    4.601262049884294, --, --, 4.5905312910478715, --, --,\n",
              "                    --, --, --, --, --, --, 4.325652958806646, --, --, --,\n",
              "                    --, --, 4.9886943718304595, --, --, --,\n",
              "                    4.939142645596432, --, --, --, --, 4.919791202877402,\n",
              "                    4.462923889585411, 4.243639930145966, --, --, --, --,\n",
              "                    --, --, --, --, 4.215435009462799, --, --, --, --, --,\n",
              "                    --, 4.897630673717002, --, --, --, 5.00689460998423,\n",
              "                    --, --, 5.008900620009475, --, --, 5.014086361697158,\n",
              "                    --, --, 4.1282553147613505, --, --, 3.9822812206878053,\n",
              "                    --, 4.820389503974014, --, 4.396541074839357, --, --,\n",
              "                    --, 4.726590526908349, --, --, --, 4.16928206885933,\n",
              "                    4.39343827459765, --, --, --, 4.981964675473542, --,\n",
              "                    4.3342796006673465, --, 4.064439226351367, --, --,\n",
              "                    4.428761470397011, --, --, --, 5.018702732321562, --,\n",
              "                    4.637465073514622, --, --, --, --, 4.538534375588767,\n",
              "                    4.816646489540198, 4.182815265005878, --, --, --, --,\n",
              "                    --, --, --, --, --, --, 4.949773626494542, --, --, --,\n",
              "                    --, --, --, --, --, --, 4.593166305933972, --, --, --,\n",
              "                    --, 4.725004845824519, --, --, 4.221338855114003, --,\n",
              "                    --, --, --, --, 4.080619715041674, --,\n",
              "                    4.776107187612089, --, --, --, 4.656419412088027,\n",
              "                    4.237991275785724, --, --, --, 4.706786068303935, --,\n",
              "                    --, --, --, --, --, 4.62099631437583, --, --,\n",
              "                    4.32801897240166, --, --, --, --, --, --, --,\n",
              "                    4.117060923680313],\n",
              "              mask=[ True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False, False,  True,  True, False, False,\n",
              "                     True,  True,  True, False, False,  True, False,  True,\n",
              "                     True, False, False,  True,  True, False,  True,  True,\n",
              "                    False,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True, False,  True,  True,  True,  True,  True, False,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                    False, False, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True, False,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True, False,  True,\n",
              "                     True, False,  True,  True, False,  True,  True, False,\n",
              "                     True,  True, False,  True, False,  True, False,  True,\n",
              "                     True,  True, False,  True,  True,  True, False, False,\n",
              "                     True,  True,  True, False,  True, False,  True, False,\n",
              "                     True,  True, False,  True,  True,  True, False,  True,\n",
              "                    False,  True,  True,  True,  True, False, False, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True,  True,\n",
              "                     True,  True, False,  True,  True,  True,  True,  True,\n",
              "                     True,  True,  True,  True, False,  True,  True,  True,\n",
              "                     True, False,  True,  True, False,  True,  True,  True,\n",
              "                     True,  True, False,  True, False,  True,  True,  True,\n",
              "                    False, False,  True,  True,  True, False,  True,  True,\n",
              "                     True,  True,  True,  True, False,  True,  True, False,\n",
              "                     True,  True,  True,  True,  True,  True,  True, False],\n",
              "        fill_value=1e+20),\n",
              " array([0.18552331, 0.18554812, 0.18563138, 0.18573184, 0.18662283,\n",
              "        0.18721765, 0.18727896, 0.18936935, 0.18948532, 0.18957395,\n",
              "        0.19096776, 0.19126669, 0.19178579, 0.19196989, 0.19237893,\n",
              "        0.19253276, 0.19370512, 0.19578881, 0.1967189 , 0.19686776,\n",
              "        0.19689397, 0.19695208, 0.19767388, 0.19841635, 0.1985626 ,\n",
              "        0.19876468, 0.19897404, 0.20193767, 0.202609  , 0.20262764,\n",
              "        0.20276651, 0.20283771, 0.20302799, 0.20319252, 0.20325968,\n",
              "        0.20360819, 0.20390991, 0.20416724, 0.20437213, 0.20502569,\n",
              "        0.20575733, 0.20791272, 0.20793536, 0.20974783, 0.20993285,\n",
              "        0.21004478, 0.21051453, 0.21196796, 0.21284716, 0.21371193,\n",
              "        0.21446102, 0.21507406, 0.21523772, 0.21585859, 0.21591571,\n",
              "        0.2168867 , 0.21701513, 0.21714394, 0.21725585, 0.21784779,\n",
              "        0.21796703, 0.21926503, 0.21952095, 0.22079798, 0.22473275,\n",
              "        0.22568756, 0.22586616, 0.22662243, 0.22744285, 0.22771621,\n",
              "        0.22824159, 0.2293706 , 0.23109561, 0.23218463, 0.23267692,\n",
              "        0.23306642, 0.23458074, 0.23509521, 0.23602653, 0.23649937,\n",
              "        0.23650185, 0.2366828 , 0.23786255, 0.23827798, 0.23979078,\n",
              "        0.24027302, 0.24146433, 0.24154546, 0.24345999, 0.24456877,\n",
              "        0.24491863, 0.24556848, 0.24583968, 0.24863886, 0.24864215,\n",
              "        0.25011112, 0.25078121, 0.25096669, 0.25203795, 0.25353592,\n",
              "        0.25382826, 0.25407262, 0.25409277, 0.25604089, 0.25604623,\n",
              "        0.25694165, 0.25797466, 0.25930918, 0.25953502, 0.26067246,\n",
              "        0.26309331, 0.26382004, 0.26401062, 0.26466352, 0.26533486,\n",
              "        0.26720475, 0.26736295, 0.26853233, 0.26886865, 0.26913668,\n",
              "        0.26968292, 0.27175105, 0.27284474, 0.27351086, 0.27536165,\n",
              "        0.27597425, 0.27696335, 0.27827284, 0.28327728, 0.28636549,\n",
              "        0.28661472, 0.28707058, 0.28783143, 0.28968138, 0.29005601,\n",
              "        0.29023551, 0.29100891, 0.29209478, 0.29311027, 0.29468423,\n",
              "        0.29579306, 0.29718048, 0.29722689, 0.29839819, 0.29981101,\n",
              "        0.30130921, 0.30221217, 0.30252858, 0.30333745, 0.30334648,\n",
              "        0.30464496, 0.30489291, 0.30844424, 0.30926655, 0.30978624,\n",
              "        0.31092125, 0.31094084, 0.31258002, 0.31284453, 0.31450939,\n",
              "        0.31476337, 0.31561414, 0.3177871 , 0.31794392, 0.31841969,\n",
              "        0.31938398, 0.31952961, 0.32058511, 0.32427365, 0.32675767,\n",
              "        0.32839218, 0.33306079, 0.33985471, 0.34466993, 0.34605567,\n",
              "        0.3470679 , 0.35219094, 0.35354778, 0.35380166, 0.35764485,\n",
              "        0.35981237, 0.36142918, 0.36168474, 0.36725266, 0.36972732,\n",
              "        0.371566  , 0.37214023, 0.37687139, 0.3793573 , 0.37989245,\n",
              "        0.38536964, 0.39283522, 0.3941906 , 0.39591262, 0.39598904,\n",
              "        0.40411349, 0.40522727, 0.40772359, 0.43593273, 0.44054341]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpIP-ThFU7Im"
      },
      "source": [
        "# Optimize using CF\r\n",
        "\r\n",
        "ratings = pd.read_csv(CUR_DIR + '/ml-latest-small/ratings.csv')\r\n",
        "\r\n",
        "C = 3\r\n",
        "total_mean = ratings.rating.mean()\r\n",
        "ratings['normalized_rating'] = ratings.rating - total_mean\r\n",
        "\r\n",
        "b_item = ratings.groupby('movieId').normalized_rating.sum() / (ratings.groupby('movieId').userId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_item, columns=['b_item']), left_on='movieId', right_index=True, how='inner')\r\n",
        "ratings['norm_item_rating'] = ratings.normalized_rating - ratings.b_item\r\n",
        "\r\n",
        "b_user = ratings.groupby('userId').norm_item_rating.sum() / (ratings.groupby('userId').movieId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_user, columns=['b_user']), left_on='userId', right_index=True, how='inner')\r\n",
        "\r\n",
        "ratings['normr_user_item_rating'] = total_mean + ratings.b_item + ratings.b_user"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s2pH-efU_xI",
        "outputId": "287c9a7c-cef4-438f-8196-ed200d9150f1"
      },
      "source": [
        "total_mean"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.501556983616962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWPvllO0VEH2"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def masked_mse(y_pred, y_true, mask, weights, lamb):\r\n",
        "    y_pred_masked = tf.gather_nd(y_pred,tf.where(mask))\r\n",
        "    y_true_masked = tf.gather_nd(y_true,tf.where(mask))\r\n",
        "    return tf.losses.mean_squared_error(y_true_masked, y_pred_masked) + lamb * tf.norm(weights)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocRT6LghVHu_",
        "outputId": "62271af2-7f74-4fef-81e7-d299fa94c42c"
      },
      "source": [
        "urm = ratings.pivot(index='userId', columns='movieId', values='normr_user_item_rating').fillna(0.).values\r\n",
        "urm = tf.constant(urm, dtype=tf.float32)\r\n",
        "\r\n",
        "sim_matrix = tf.Variable(tf.random.uniform(shape=[urm.shape[1], urm.shape[1]]), trainable=True)\r\n",
        "epochs = 600\r\n",
        "opti = tf.optimizers.Adam(0.01)\r\n",
        "\r\n",
        "mask = tf.not_equal(urm, 0.)\r\n",
        "loss = masked_mse\r\n",
        "mses = []\r\n",
        "for e in range(epochs):\r\n",
        "  with tf.GradientTape() as gt:\r\n",
        "    gt.watch(sim_matrix)\r\n",
        "    preds = tf.matmul(urm, sim_matrix)\r\n",
        "    preds = tf.clip_by_value(preds, 0., 5.)\r\n",
        "\r\n",
        "    mse = loss(preds, urm, mask, sim_matrix, 0.9)\r\n",
        "    grads = gt.gradient(mse, sim_matrix)\r\n",
        "    opti.apply_gradients(grads_and_vars=zip([grads], [sim_matrix])) \r\n",
        "    mses.append(loss(preds, urm, mask, sim_matrix, 0.))\r\n",
        "    print(f'Epoch:{e} - Loss: {mses[-1]}')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0 - Loss: 2.613279342651367\n",
            "Epoch:1 - Loss: 2.613279342651367\n",
            "Epoch:2 - Loss: 2.613279342651367\n",
            "Epoch:3 - Loss: 2.613279342651367\n",
            "Epoch:4 - Loss: 2.613279342651367\n",
            "Epoch:5 - Loss: 2.613279342651367\n",
            "Epoch:6 - Loss: 2.613279342651367\n",
            "Epoch:7 - Loss: 2.613279342651367\n",
            "Epoch:8 - Loss: 2.613279342651367\n",
            "Epoch:9 - Loss: 2.613279342651367\n",
            "Epoch:10 - Loss: 2.613279342651367\n",
            "Epoch:11 - Loss: 2.613279342651367\n",
            "Epoch:12 - Loss: 2.613279342651367\n",
            "Epoch:13 - Loss: 2.613279342651367\n",
            "Epoch:14 - Loss: 2.613279342651367\n",
            "Epoch:15 - Loss: 2.613279342651367\n",
            "Epoch:16 - Loss: 2.613279342651367\n",
            "Epoch:17 - Loss: 2.613279342651367\n",
            "Epoch:18 - Loss: 2.613279342651367\n",
            "Epoch:19 - Loss: 2.613279342651367\n",
            "Epoch:20 - Loss: 2.613279342651367\n",
            "Epoch:21 - Loss: 2.613279342651367\n",
            "Epoch:22 - Loss: 2.613279342651367\n",
            "Epoch:23 - Loss: 2.613279342651367\n",
            "Epoch:24 - Loss: 2.613279342651367\n",
            "Epoch:25 - Loss: 2.613279342651367\n",
            "Epoch:26 - Loss: 2.613279342651367\n",
            "Epoch:27 - Loss: 2.613279342651367\n",
            "Epoch:28 - Loss: 2.613279342651367\n",
            "Epoch:29 - Loss: 2.613279342651367\n",
            "Epoch:30 - Loss: 2.613279342651367\n",
            "Epoch:31 - Loss: 2.613276958465576\n",
            "Epoch:32 - Loss: 2.613259792327881\n",
            "Epoch:33 - Loss: 2.613237142562866\n",
            "Epoch:34 - Loss: 2.6132023334503174\n",
            "Epoch:35 - Loss: 2.6131584644317627\n",
            "Epoch:36 - Loss: 2.613110065460205\n",
            "Epoch:37 - Loss: 2.6130716800689697\n",
            "Epoch:38 - Loss: 2.613042116165161\n",
            "Epoch:39 - Loss: 2.6130123138427734\n",
            "Epoch:40 - Loss: 2.6129627227783203\n",
            "Epoch:41 - Loss: 2.6128907203674316\n",
            "Epoch:42 - Loss: 2.6127896308898926\n",
            "Epoch:43 - Loss: 2.6126203536987305\n",
            "Epoch:44 - Loss: 2.6123850345611572\n",
            "Epoch:45 - Loss: 2.6121487617492676\n",
            "Epoch:46 - Loss: 2.6119120121002197\n",
            "Epoch:47 - Loss: 2.6116299629211426\n",
            "Epoch:48 - Loss: 2.6113204956054688\n",
            "Epoch:49 - Loss: 2.611032485961914\n",
            "Epoch:50 - Loss: 2.610743522644043\n",
            "Epoch:51 - Loss: 2.610334873199463\n",
            "Epoch:52 - Loss: 2.6098690032958984\n",
            "Epoch:53 - Loss: 2.6093742847442627\n",
            "Epoch:54 - Loss: 2.6087629795074463\n",
            "Epoch:55 - Loss: 2.60790753364563\n",
            "Epoch:56 - Loss: 2.6066856384277344\n",
            "Epoch:57 - Loss: 2.6053144931793213\n",
            "Epoch:58 - Loss: 2.6037161350250244\n",
            "Epoch:59 - Loss: 2.6019065380096436\n",
            "Epoch:60 - Loss: 2.5998055934906006\n",
            "Epoch:61 - Loss: 2.5972280502319336\n",
            "Epoch:62 - Loss: 2.5943093299865723\n",
            "Epoch:63 - Loss: 2.5907199382781982\n",
            "Epoch:64 - Loss: 2.5863749980926514\n",
            "Epoch:65 - Loss: 2.581526279449463\n",
            "Epoch:66 - Loss: 2.576016902923584\n",
            "Epoch:67 - Loss: 2.569898843765259\n",
            "Epoch:68 - Loss: 2.563971519470215\n",
            "Epoch:69 - Loss: 2.5564332008361816\n",
            "Epoch:70 - Loss: 2.54665470123291\n",
            "Epoch:71 - Loss: 2.535766363143921\n",
            "Epoch:72 - Loss: 2.5248873233795166\n",
            "Epoch:73 - Loss: 2.5129470825195312\n",
            "Epoch:74 - Loss: 2.4985899925231934\n",
            "Epoch:75 - Loss: 2.4808833599090576\n",
            "Epoch:76 - Loss: 2.459094762802124\n",
            "Epoch:77 - Loss: 2.4360156059265137\n",
            "Epoch:78 - Loss: 2.4063847064971924\n",
            "Epoch:79 - Loss: 2.3703513145446777\n",
            "Epoch:80 - Loss: 2.326416492462158\n",
            "Epoch:81 - Loss: 2.278470039367676\n",
            "Epoch:82 - Loss: 2.2242069244384766\n",
            "Epoch:83 - Loss: 2.1465301513671875\n",
            "Epoch:84 - Loss: 2.0742135047912598\n",
            "Epoch:85 - Loss: 2.3134405612945557\n",
            "Epoch:86 - Loss: 2.955437183380127\n",
            "Epoch:87 - Loss: 3.249049186706543\n",
            "Epoch:88 - Loss: 3.1094748973846436\n",
            "Epoch:89 - Loss: 3.1199283599853516\n",
            "Epoch:90 - Loss: 3.149266004562378\n",
            "Epoch:91 - Loss: 3.1606478691101074\n",
            "Epoch:92 - Loss: 3.145237684249878\n",
            "Epoch:93 - Loss: 3.1078884601593018\n",
            "Epoch:94 - Loss: 3.071863889694214\n",
            "Epoch:95 - Loss: 3.0271809101104736\n",
            "Epoch:96 - Loss: 2.9581239223480225\n",
            "Epoch:97 - Loss: 2.894139289855957\n",
            "Epoch:98 - Loss: 2.80055570602417\n",
            "Epoch:99 - Loss: 2.6840462684631348\n",
            "Epoch:100 - Loss: 2.529674768447876\n",
            "Epoch:101 - Loss: 2.404740810394287\n",
            "Epoch:102 - Loss: 2.328268051147461\n",
            "Epoch:103 - Loss: 2.2797691822052\n",
            "Epoch:104 - Loss: 2.332228899002075\n",
            "Epoch:105 - Loss: 2.4532837867736816\n",
            "Epoch:106 - Loss: 2.5571200847625732\n",
            "Epoch:107 - Loss: 2.5251591205596924\n",
            "Epoch:108 - Loss: 2.508052110671997\n",
            "Epoch:109 - Loss: 2.399142265319824\n",
            "Epoch:110 - Loss: 2.30558705329895\n",
            "Epoch:111 - Loss: 2.225710868835449\n",
            "Epoch:112 - Loss: 2.170778751373291\n",
            "Epoch:113 - Loss: 2.2231483459472656\n",
            "Epoch:114 - Loss: 2.358616590499878\n",
            "Epoch:115 - Loss: 2.6110687255859375\n",
            "Epoch:116 - Loss: 2.7888689041137695\n",
            "Epoch:117 - Loss: 2.8331997394561768\n",
            "Epoch:118 - Loss: 2.715470552444458\n",
            "Epoch:119 - Loss: 2.5888466835021973\n",
            "Epoch:120 - Loss: 2.4915378093719482\n",
            "Epoch:121 - Loss: 2.405266046524048\n",
            "Epoch:122 - Loss: 2.398768663406372\n",
            "Epoch:123 - Loss: 2.514643907546997\n",
            "Epoch:124 - Loss: 2.672067165374756\n",
            "Epoch:125 - Loss: 2.7904534339904785\n",
            "Epoch:126 - Loss: 2.7922797203063965\n",
            "Epoch:127 - Loss: 2.6955881118774414\n",
            "Epoch:128 - Loss: 2.5762505531311035\n",
            "Epoch:129 - Loss: 2.509546995162964\n",
            "Epoch:130 - Loss: 2.570674419403076\n",
            "Epoch:131 - Loss: 2.667459487915039\n",
            "Epoch:132 - Loss: 2.8162827491760254\n",
            "Epoch:133 - Loss: 2.8279762268066406\n",
            "Epoch:134 - Loss: 2.776742935180664\n",
            "Epoch:135 - Loss: 2.708421230316162\n",
            "Epoch:136 - Loss: 2.648160219192505\n",
            "Epoch:137 - Loss: 2.673020124435425\n",
            "Epoch:138 - Loss: 2.7082107067108154\n",
            "Epoch:139 - Loss: 2.7554197311401367\n",
            "Epoch:140 - Loss: 2.779386281967163\n",
            "Epoch:141 - Loss: 2.7394561767578125\n",
            "Epoch:142 - Loss: 2.713902711868286\n",
            "Epoch:143 - Loss: 2.696983814239502\n",
            "Epoch:144 - Loss: 2.7841978073120117\n",
            "Epoch:145 - Loss: 2.7647621631622314\n",
            "Epoch:146 - Loss: 2.781435251235962\n",
            "Epoch:147 - Loss: 2.7868494987487793\n",
            "Epoch:148 - Loss: 2.762326955795288\n",
            "Epoch:149 - Loss: 2.7538130283355713\n",
            "Epoch:150 - Loss: 2.772033214569092\n",
            "Epoch:151 - Loss: 2.8110108375549316\n",
            "Epoch:152 - Loss: 2.7810637950897217\n",
            "Epoch:153 - Loss: 2.7674448490142822\n",
            "Epoch:154 - Loss: 2.7691915035247803\n",
            "Epoch:155 - Loss: 2.7774131298065186\n",
            "Epoch:156 - Loss: 2.7981133460998535\n",
            "Epoch:157 - Loss: 2.806396007537842\n",
            "Epoch:158 - Loss: 2.803683280944824\n",
            "Epoch:159 - Loss: 2.7682297229766846\n",
            "Epoch:160 - Loss: 2.79742169380188\n",
            "Epoch:161 - Loss: 2.763068914413452\n",
            "Epoch:162 - Loss: 2.8197078704833984\n",
            "Epoch:163 - Loss: 2.775240659713745\n",
            "Epoch:164 - Loss: 2.8031165599823\n",
            "Epoch:165 - Loss: 2.8468174934387207\n",
            "Epoch:166 - Loss: 2.7962706089019775\n",
            "Epoch:167 - Loss: 2.817808151245117\n",
            "Epoch:168 - Loss: 2.804271697998047\n",
            "Epoch:169 - Loss: 2.7964484691619873\n",
            "Epoch:170 - Loss: 2.7567801475524902\n",
            "Epoch:171 - Loss: 2.7702879905700684\n",
            "Epoch:172 - Loss: 2.803748846054077\n",
            "Epoch:173 - Loss: 2.7976367473602295\n",
            "Epoch:174 - Loss: 2.798489570617676\n",
            "Epoch:175 - Loss: 2.8176043033599854\n",
            "Epoch:176 - Loss: 2.7930197715759277\n",
            "Epoch:177 - Loss: 2.7972629070281982\n",
            "Epoch:178 - Loss: 2.7664434909820557\n",
            "Epoch:179 - Loss: 2.7255618572235107\n",
            "Epoch:180 - Loss: 2.7288453578948975\n",
            "Epoch:181 - Loss: 2.757657289505005\n",
            "Epoch:182 - Loss: 2.8055782318115234\n",
            "Epoch:183 - Loss: 2.768425226211548\n",
            "Epoch:184 - Loss: 2.749586582183838\n",
            "Epoch:185 - Loss: 2.7391743659973145\n",
            "Epoch:186 - Loss: 2.762237071990967\n",
            "Epoch:187 - Loss: 2.7480287551879883\n",
            "Epoch:188 - Loss: 2.7919058799743652\n",
            "Epoch:189 - Loss: 2.7517950534820557\n",
            "Epoch:190 - Loss: 2.7562878131866455\n",
            "Epoch:191 - Loss: 2.7275071144104004\n",
            "Epoch:192 - Loss: 2.7502169609069824\n",
            "Epoch:193 - Loss: 2.756761074066162\n",
            "Epoch:194 - Loss: 2.741589069366455\n",
            "Epoch:195 - Loss: 2.753300428390503\n",
            "Epoch:196 - Loss: 2.733062505722046\n",
            "Epoch:197 - Loss: 2.728703737258911\n",
            "Epoch:198 - Loss: 2.7011563777923584\n",
            "Epoch:199 - Loss: 2.7388110160827637\n",
            "Epoch:200 - Loss: 2.7096707820892334\n",
            "Epoch:201 - Loss: 2.742753744125366\n",
            "Epoch:202 - Loss: 2.760375499725342\n",
            "Epoch:203 - Loss: 2.725066661834717\n",
            "Epoch:204 - Loss: 2.6975550651550293\n",
            "Epoch:205 - Loss: 2.7422640323638916\n",
            "Epoch:206 - Loss: 2.7128548622131348\n",
            "Epoch:207 - Loss: 2.734285831451416\n",
            "Epoch:208 - Loss: 2.78586745262146\n",
            "Epoch:209 - Loss: 2.762176275253296\n",
            "Epoch:210 - Loss: 2.7437002658843994\n",
            "Epoch:211 - Loss: 2.7479796409606934\n",
            "Epoch:212 - Loss: 2.727048635482788\n",
            "Epoch:213 - Loss: 2.7074854373931885\n",
            "Epoch:214 - Loss: 2.726207971572876\n",
            "Epoch:215 - Loss: 2.748605489730835\n",
            "Epoch:216 - Loss: 2.715571641921997\n",
            "Epoch:217 - Loss: 2.7173094749450684\n",
            "Epoch:218 - Loss: 2.706939697265625\n",
            "Epoch:219 - Loss: 2.704880952835083\n",
            "Epoch:220 - Loss: 2.7033345699310303\n",
            "Epoch:221 - Loss: 2.68733549118042\n",
            "Epoch:222 - Loss: 2.7226383686065674\n",
            "Epoch:223 - Loss: 2.7336153984069824\n",
            "Epoch:224 - Loss: 2.6977813243865967\n",
            "Epoch:225 - Loss: 2.6961705684661865\n",
            "Epoch:226 - Loss: 2.6883156299591064\n",
            "Epoch:227 - Loss: 2.6972172260284424\n",
            "Epoch:228 - Loss: 2.713015556335449\n",
            "Epoch:229 - Loss: 2.7431282997131348\n",
            "Epoch:230 - Loss: 2.704216480255127\n",
            "Epoch:231 - Loss: 2.6940197944641113\n",
            "Epoch:232 - Loss: 2.701540470123291\n",
            "Epoch:233 - Loss: 2.7075626850128174\n",
            "Epoch:234 - Loss: 2.725489377975464\n",
            "Epoch:235 - Loss: 2.7055249214172363\n",
            "Epoch:236 - Loss: 2.7081828117370605\n",
            "Epoch:237 - Loss: 2.6731936931610107\n",
            "Epoch:238 - Loss: 2.7002980709075928\n",
            "Epoch:239 - Loss: 2.6839981079101562\n",
            "Epoch:240 - Loss: 2.7024643421173096\n",
            "Epoch:241 - Loss: 2.7126777172088623\n",
            "Epoch:242 - Loss: 2.6602301597595215\n",
            "Epoch:243 - Loss: 2.710968494415283\n",
            "Epoch:244 - Loss: 2.7091012001037598\n",
            "Epoch:245 - Loss: 2.6814777851104736\n",
            "Epoch:246 - Loss: 2.6548213958740234\n",
            "Epoch:247 - Loss: 2.698336362838745\n",
            "Epoch:248 - Loss: 2.714677095413208\n",
            "Epoch:249 - Loss: 2.6814610958099365\n",
            "Epoch:250 - Loss: 2.7086024284362793\n",
            "Epoch:251 - Loss: 2.659766912460327\n",
            "Epoch:252 - Loss: 2.6352763175964355\n",
            "Epoch:253 - Loss: 2.6386570930480957\n",
            "Epoch:254 - Loss: 2.6973440647125244\n",
            "Epoch:255 - Loss: 2.679487943649292\n",
            "Epoch:256 - Loss: 2.6786904335021973\n",
            "Epoch:257 - Loss: 2.6658642292022705\n",
            "Epoch:258 - Loss: 2.683126449584961\n",
            "Epoch:259 - Loss: 2.686997890472412\n",
            "Epoch:260 - Loss: 2.6950523853302\n",
            "Epoch:261 - Loss: 2.6582190990448\n",
            "Epoch:262 - Loss: 2.673722267150879\n",
            "Epoch:263 - Loss: 2.6589250564575195\n",
            "Epoch:264 - Loss: 2.700416326522827\n",
            "Epoch:265 - Loss: 2.7004480361938477\n",
            "Epoch:266 - Loss: 2.6547350883483887\n",
            "Epoch:267 - Loss: 2.664667844772339\n",
            "Epoch:268 - Loss: 2.641105890274048\n",
            "Epoch:269 - Loss: 2.698713541030884\n",
            "Epoch:270 - Loss: 2.6882667541503906\n",
            "Epoch:271 - Loss: 2.7236714363098145\n",
            "Epoch:272 - Loss: 2.692481517791748\n",
            "Epoch:273 - Loss: 2.6652045249938965\n",
            "Epoch:274 - Loss: 2.656660556793213\n",
            "Epoch:275 - Loss: 2.682109832763672\n",
            "Epoch:276 - Loss: 2.646392345428467\n",
            "Epoch:277 - Loss: 2.6550140380859375\n",
            "Epoch:278 - Loss: 2.6912238597869873\n",
            "Epoch:279 - Loss: 2.6714529991149902\n",
            "Epoch:280 - Loss: 2.6732895374298096\n",
            "Epoch:281 - Loss: 2.6946499347686768\n",
            "Epoch:282 - Loss: 2.6823878288269043\n",
            "Epoch:283 - Loss: 2.666128158569336\n",
            "Epoch:284 - Loss: 2.652785539627075\n",
            "Epoch:285 - Loss: 2.6796092987060547\n",
            "Epoch:286 - Loss: 2.667551040649414\n",
            "Epoch:287 - Loss: 2.6748664379119873\n",
            "Epoch:288 - Loss: 2.7010669708251953\n",
            "Epoch:289 - Loss: 2.695089340209961\n",
            "Epoch:290 - Loss: 2.6729488372802734\n",
            "Epoch:291 - Loss: 2.6453216075897217\n",
            "Epoch:292 - Loss: 2.653048276901245\n",
            "Epoch:293 - Loss: 2.6818981170654297\n",
            "Epoch:294 - Loss: 2.6676087379455566\n",
            "Epoch:295 - Loss: 2.680147171020508\n",
            "Epoch:296 - Loss: 2.6316404342651367\n",
            "Epoch:297 - Loss: 2.644205331802368\n",
            "Epoch:298 - Loss: 2.66447377204895\n",
            "Epoch:299 - Loss: 2.6835789680480957\n",
            "Epoch:300 - Loss: 2.68257999420166\n",
            "Epoch:301 - Loss: 2.6494452953338623\n",
            "Epoch:302 - Loss: 2.646090030670166\n",
            "Epoch:303 - Loss: 2.657640218734741\n",
            "Epoch:304 - Loss: 2.6546106338500977\n",
            "Epoch:305 - Loss: 2.701030731201172\n",
            "Epoch:306 - Loss: 2.664449691772461\n",
            "Epoch:307 - Loss: 2.668696880340576\n",
            "Epoch:308 - Loss: 2.6643826961517334\n",
            "Epoch:309 - Loss: 2.66020131111145\n",
            "Epoch:310 - Loss: 2.6749603748321533\n",
            "Epoch:311 - Loss: 2.6615614891052246\n",
            "Epoch:312 - Loss: 2.6722071170806885\n",
            "Epoch:313 - Loss: 2.6576359272003174\n",
            "Epoch:314 - Loss: 2.6937170028686523\n",
            "Epoch:315 - Loss: 2.6735658645629883\n",
            "Epoch:316 - Loss: 2.638397455215454\n",
            "Epoch:317 - Loss: 2.632795810699463\n",
            "Epoch:318 - Loss: 2.6727347373962402\n",
            "Epoch:319 - Loss: 2.669121265411377\n",
            "Epoch:320 - Loss: 2.6795620918273926\n",
            "Epoch:321 - Loss: 2.6568222045898438\n",
            "Epoch:322 - Loss: 2.6427860260009766\n",
            "Epoch:323 - Loss: 2.6574673652648926\n",
            "Epoch:324 - Loss: 2.6738529205322266\n",
            "Epoch:325 - Loss: 2.6673121452331543\n",
            "Epoch:326 - Loss: 2.6393861770629883\n",
            "Epoch:327 - Loss: 2.6262712478637695\n",
            "Epoch:328 - Loss: 2.6140620708465576\n",
            "Epoch:329 - Loss: 2.6544535160064697\n",
            "Epoch:330 - Loss: 2.6657655239105225\n",
            "Epoch:331 - Loss: 2.664992570877075\n",
            "Epoch:332 - Loss: 2.617743730545044\n",
            "Epoch:333 - Loss: 2.6413092613220215\n",
            "Epoch:334 - Loss: 2.663600444793701\n",
            "Epoch:335 - Loss: 2.647911548614502\n",
            "Epoch:336 - Loss: 2.6637091636657715\n",
            "Epoch:337 - Loss: 2.6281282901763916\n",
            "Epoch:338 - Loss: 2.629539966583252\n",
            "Epoch:339 - Loss: 2.628355026245117\n",
            "Epoch:340 - Loss: 2.6618452072143555\n",
            "Epoch:341 - Loss: 2.6578662395477295\n",
            "Epoch:342 - Loss: 2.6608543395996094\n",
            "Epoch:343 - Loss: 2.648059606552124\n",
            "Epoch:344 - Loss: 2.637321710586548\n",
            "Epoch:345 - Loss: 2.6141631603240967\n",
            "Epoch:346 - Loss: 2.638012409210205\n",
            "Epoch:347 - Loss: 2.627553701400757\n",
            "Epoch:348 - Loss: 2.676414966583252\n",
            "Epoch:349 - Loss: 2.661519765853882\n",
            "Epoch:350 - Loss: 2.624436616897583\n",
            "Epoch:351 - Loss: 2.6323909759521484\n",
            "Epoch:352 - Loss: 2.626446008682251\n",
            "Epoch:353 - Loss: 2.659865617752075\n",
            "Epoch:354 - Loss: 2.6678144931793213\n",
            "Epoch:355 - Loss: 2.643897294998169\n",
            "Epoch:356 - Loss: 2.621072292327881\n",
            "Epoch:357 - Loss: 2.6152007579803467\n",
            "Epoch:358 - Loss: 2.6390533447265625\n",
            "Epoch:359 - Loss: 2.65325665473938\n",
            "Epoch:360 - Loss: 2.6641170978546143\n",
            "Epoch:361 - Loss: 2.648965358734131\n",
            "Epoch:362 - Loss: 2.6193106174468994\n",
            "Epoch:363 - Loss: 2.62754487991333\n",
            "Epoch:364 - Loss: 2.6347877979278564\n",
            "Epoch:365 - Loss: 2.6299338340759277\n",
            "Epoch:366 - Loss: 2.6126351356506348\n",
            "Epoch:367 - Loss: 2.622786045074463\n",
            "Epoch:368 - Loss: 2.630147695541382\n",
            "Epoch:369 - Loss: 2.6213271617889404\n",
            "Epoch:370 - Loss: 2.652996778488159\n",
            "Epoch:371 - Loss: 2.619246244430542\n",
            "Epoch:372 - Loss: 2.6107616424560547\n",
            "Epoch:373 - Loss: 2.628584384918213\n",
            "Epoch:374 - Loss: 2.5905022621154785\n",
            "Epoch:375 - Loss: 2.617915391921997\n",
            "Epoch:376 - Loss: 2.6375017166137695\n",
            "Epoch:377 - Loss: 2.6315786838531494\n",
            "Epoch:378 - Loss: 2.6407017707824707\n",
            "Epoch:379 - Loss: 2.6066629886627197\n",
            "Epoch:380 - Loss: 2.5998353958129883\n",
            "Epoch:381 - Loss: 2.618147850036621\n",
            "Epoch:382 - Loss: 2.6391468048095703\n",
            "Epoch:383 - Loss: 2.646267890930176\n",
            "Epoch:384 - Loss: 2.6093831062316895\n",
            "Epoch:385 - Loss: 2.6300220489501953\n",
            "Epoch:386 - Loss: 2.6360373497009277\n",
            "Epoch:387 - Loss: 2.622286796569824\n",
            "Epoch:388 - Loss: 2.654279947280884\n",
            "Epoch:389 - Loss: 2.6187968254089355\n",
            "Epoch:390 - Loss: 2.645954132080078\n",
            "Epoch:391 - Loss: 2.63596773147583\n",
            "Epoch:392 - Loss: 2.6125760078430176\n",
            "Epoch:393 - Loss: 2.6292941570281982\n",
            "Epoch:394 - Loss: 2.6349592208862305\n",
            "Epoch:395 - Loss: 2.6294150352478027\n",
            "Epoch:396 - Loss: 2.6370179653167725\n",
            "Epoch:397 - Loss: 2.6376607418060303\n",
            "Epoch:398 - Loss: 2.6453263759613037\n",
            "Epoch:399 - Loss: 2.624107837677002\n",
            "Epoch:400 - Loss: 2.6302778720855713\n",
            "Epoch:401 - Loss: 2.6412720680236816\n",
            "Epoch:402 - Loss: 2.638200044631958\n",
            "Epoch:403 - Loss: 2.629952907562256\n",
            "Epoch:404 - Loss: 2.59702205657959\n",
            "Epoch:405 - Loss: 2.6262006759643555\n",
            "Epoch:406 - Loss: 2.6416757106781006\n",
            "Epoch:407 - Loss: 2.6549248695373535\n",
            "Epoch:408 - Loss: 2.646773338317871\n",
            "Epoch:409 - Loss: 2.666801691055298\n",
            "Epoch:410 - Loss: 2.636591911315918\n",
            "Epoch:411 - Loss: 2.6445727348327637\n",
            "Epoch:412 - Loss: 2.614048480987549\n",
            "Epoch:413 - Loss: 2.607105016708374\n",
            "Epoch:414 - Loss: 2.6289432048797607\n",
            "Epoch:415 - Loss: 2.635528326034546\n",
            "Epoch:416 - Loss: 2.6439199447631836\n",
            "Epoch:417 - Loss: 2.616630792617798\n",
            "Epoch:418 - Loss: 2.6306874752044678\n",
            "Epoch:419 - Loss: 2.630148410797119\n",
            "Epoch:420 - Loss: 2.6143341064453125\n",
            "Epoch:421 - Loss: 2.6165435314178467\n",
            "Epoch:422 - Loss: 2.60917067527771\n",
            "Epoch:423 - Loss: 2.6400794982910156\n",
            "Epoch:424 - Loss: 2.6293742656707764\n",
            "Epoch:425 - Loss: 2.6196773052215576\n",
            "Epoch:426 - Loss: 2.6313865184783936\n",
            "Epoch:427 - Loss: 2.6157734394073486\n",
            "Epoch:428 - Loss: 2.630833148956299\n",
            "Epoch:429 - Loss: 2.618619680404663\n",
            "Epoch:430 - Loss: 2.5961251258850098\n",
            "Epoch:431 - Loss: 2.6004538536071777\n",
            "Epoch:432 - Loss: 2.634641647338867\n",
            "Epoch:433 - Loss: 2.625905752182007\n",
            "Epoch:434 - Loss: 2.6536448001861572\n",
            "Epoch:435 - Loss: 2.6006722450256348\n",
            "Epoch:436 - Loss: 2.613964080810547\n",
            "Epoch:437 - Loss: 2.6125311851501465\n",
            "Epoch:438 - Loss: 2.606483221054077\n",
            "Epoch:439 - Loss: 2.639953136444092\n",
            "Epoch:440 - Loss: 2.6345200538635254\n",
            "Epoch:441 - Loss: 2.6305336952209473\n",
            "Epoch:442 - Loss: 2.5760154724121094\n",
            "Epoch:443 - Loss: 2.604818344116211\n",
            "Epoch:444 - Loss: 2.637467622756958\n",
            "Epoch:445 - Loss: 2.6436307430267334\n",
            "Epoch:446 - Loss: 2.6406571865081787\n",
            "Epoch:447 - Loss: 2.6310389041900635\n",
            "Epoch:448 - Loss: 2.619168758392334\n",
            "Epoch:449 - Loss: 2.6225900650024414\n",
            "Epoch:450 - Loss: 2.6016414165496826\n",
            "Epoch:451 - Loss: 2.6234726905822754\n",
            "Epoch:452 - Loss: 2.644998550415039\n",
            "Epoch:453 - Loss: 2.622159957885742\n",
            "Epoch:454 - Loss: 2.6217753887176514\n",
            "Epoch:455 - Loss: 2.595534086227417\n",
            "Epoch:456 - Loss: 2.6448707580566406\n",
            "Epoch:457 - Loss: 2.6504127979278564\n",
            "Epoch:458 - Loss: 2.642853021621704\n",
            "Epoch:459 - Loss: 2.61391019821167\n",
            "Epoch:460 - Loss: 2.6173532009124756\n",
            "Epoch:461 - Loss: 2.6250498294830322\n",
            "Epoch:462 - Loss: 2.618112802505493\n",
            "Epoch:463 - Loss: 2.633113384246826\n",
            "Epoch:464 - Loss: 2.6099467277526855\n",
            "Epoch:465 - Loss: 2.6196129322052\n",
            "Epoch:466 - Loss: 2.611658811569214\n",
            "Epoch:467 - Loss: 2.608186721801758\n",
            "Epoch:468 - Loss: 2.6254630088806152\n",
            "Epoch:469 - Loss: 2.6297614574432373\n",
            "Epoch:470 - Loss: 2.6145403385162354\n",
            "Epoch:471 - Loss: 2.6173787117004395\n",
            "Epoch:472 - Loss: 2.621741533279419\n",
            "Epoch:473 - Loss: 2.62955904006958\n",
            "Epoch:474 - Loss: 2.6262080669403076\n",
            "Epoch:475 - Loss: 2.6142685413360596\n",
            "Epoch:476 - Loss: 2.6034560203552246\n",
            "Epoch:477 - Loss: 2.5962636470794678\n",
            "Epoch:478 - Loss: 2.6005468368530273\n",
            "Epoch:479 - Loss: 2.631183385848999\n",
            "Epoch:480 - Loss: 2.6209983825683594\n",
            "Epoch:481 - Loss: 2.628624677658081\n",
            "Epoch:482 - Loss: 2.583601474761963\n",
            "Epoch:483 - Loss: 2.601170063018799\n",
            "Epoch:484 - Loss: 2.631730079650879\n",
            "Epoch:485 - Loss: 2.6134274005889893\n",
            "Epoch:486 - Loss: 2.6069750785827637\n",
            "Epoch:487 - Loss: 2.6063387393951416\n",
            "Epoch:488 - Loss: 2.601588249206543\n",
            "Epoch:489 - Loss: 2.619061231613159\n",
            "Epoch:490 - Loss: 2.627687931060791\n",
            "Epoch:491 - Loss: 2.595754623413086\n",
            "Epoch:492 - Loss: 2.606755256652832\n",
            "Epoch:493 - Loss: 2.5945069789886475\n",
            "Epoch:494 - Loss: 2.6202127933502197\n",
            "Epoch:495 - Loss: 2.5921101570129395\n",
            "Epoch:496 - Loss: 2.6005823612213135\n",
            "Epoch:497 - Loss: 2.6247036457061768\n",
            "Epoch:498 - Loss: 2.6293749809265137\n",
            "Epoch:499 - Loss: 2.6376123428344727\n",
            "Epoch:500 - Loss: 2.62833833694458\n",
            "Epoch:501 - Loss: 2.6235506534576416\n",
            "Epoch:502 - Loss: 2.6165921688079834\n",
            "Epoch:503 - Loss: 2.616743326187134\n",
            "Epoch:504 - Loss: 2.585397958755493\n",
            "Epoch:505 - Loss: 2.6260592937469482\n",
            "Epoch:506 - Loss: 2.6612355709075928\n",
            "Epoch:507 - Loss: 2.624699592590332\n",
            "Epoch:508 - Loss: 2.598811388015747\n",
            "Epoch:509 - Loss: 2.6160695552825928\n",
            "Epoch:510 - Loss: 2.608823299407959\n",
            "Epoch:511 - Loss: 2.639390230178833\n",
            "Epoch:512 - Loss: 2.621062994003296\n",
            "Epoch:513 - Loss: 2.6130223274230957\n",
            "Epoch:514 - Loss: 2.605778932571411\n",
            "Epoch:515 - Loss: 2.602062940597534\n",
            "Epoch:516 - Loss: 2.6156249046325684\n",
            "Epoch:517 - Loss: 2.6135847568511963\n",
            "Epoch:518 - Loss: 2.6151907444000244\n",
            "Epoch:519 - Loss: 2.607412576675415\n",
            "Epoch:520 - Loss: 2.6011688709259033\n",
            "Epoch:521 - Loss: 2.6195085048675537\n",
            "Epoch:522 - Loss: 2.6167514324188232\n",
            "Epoch:523 - Loss: 2.633312702178955\n",
            "Epoch:524 - Loss: 2.6022002696990967\n",
            "Epoch:525 - Loss: 2.641948938369751\n",
            "Epoch:526 - Loss: 2.614447593688965\n",
            "Epoch:527 - Loss: 2.6028671264648438\n",
            "Epoch:528 - Loss: 2.5897209644317627\n",
            "Epoch:529 - Loss: 2.613232135772705\n",
            "Epoch:530 - Loss: 2.6123673915863037\n",
            "Epoch:531 - Loss: 2.621772289276123\n",
            "Epoch:532 - Loss: 2.611823797225952\n",
            "Epoch:533 - Loss: 2.6009175777435303\n",
            "Epoch:534 - Loss: 2.606156826019287\n",
            "Epoch:535 - Loss: 2.6129817962646484\n",
            "Epoch:536 - Loss: 2.6319947242736816\n",
            "Epoch:537 - Loss: 2.6244897842407227\n",
            "Epoch:538 - Loss: 2.611097812652588\n",
            "Epoch:539 - Loss: 2.6043848991394043\n",
            "Epoch:540 - Loss: 2.5979831218719482\n",
            "Epoch:541 - Loss: 2.6138148307800293\n",
            "Epoch:542 - Loss: 2.6194467544555664\n",
            "Epoch:543 - Loss: 2.630070924758911\n",
            "Epoch:544 - Loss: 2.6232783794403076\n",
            "Epoch:545 - Loss: 2.6195459365844727\n",
            "Epoch:546 - Loss: 2.609923839569092\n",
            "Epoch:547 - Loss: 2.6228573322296143\n",
            "Epoch:548 - Loss: 2.606684923171997\n",
            "Epoch:549 - Loss: 2.5951743125915527\n",
            "Epoch:550 - Loss: 2.6158390045166016\n",
            "Epoch:551 - Loss: 2.6111812591552734\n",
            "Epoch:552 - Loss: 2.6142969131469727\n",
            "Epoch:553 - Loss: 2.630042791366577\n",
            "Epoch:554 - Loss: 2.6271469593048096\n",
            "Epoch:555 - Loss: 2.6195788383483887\n",
            "Epoch:556 - Loss: 2.6188371181488037\n",
            "Epoch:557 - Loss: 2.6082897186279297\n",
            "Epoch:558 - Loss: 2.627915620803833\n",
            "Epoch:559 - Loss: 2.6219000816345215\n",
            "Epoch:560 - Loss: 2.5963423252105713\n",
            "Epoch:561 - Loss: 2.5937511920928955\n",
            "Epoch:562 - Loss: 2.6358890533447266\n",
            "Epoch:563 - Loss: 2.608320713043213\n",
            "Epoch:564 - Loss: 2.608564853668213\n",
            "Epoch:565 - Loss: 2.6240298748016357\n",
            "Epoch:566 - Loss: 2.626755952835083\n",
            "Epoch:567 - Loss: 2.610520124435425\n",
            "Epoch:568 - Loss: 2.6157195568084717\n",
            "Epoch:569 - Loss: 2.6290318965911865\n",
            "Epoch:570 - Loss: 2.634085178375244\n",
            "Epoch:571 - Loss: 2.6324849128723145\n",
            "Epoch:572 - Loss: 2.6118266582489014\n",
            "Epoch:573 - Loss: 2.616396903991699\n",
            "Epoch:574 - Loss: 2.6265933513641357\n",
            "Epoch:575 - Loss: 2.620164155960083\n",
            "Epoch:576 - Loss: 2.6398329734802246\n",
            "Epoch:577 - Loss: 2.6194911003112793\n",
            "Epoch:578 - Loss: 2.5925955772399902\n",
            "Epoch:579 - Loss: 2.6135005950927734\n",
            "Epoch:580 - Loss: 2.624580144882202\n",
            "Epoch:581 - Loss: 2.645054817199707\n",
            "Epoch:582 - Loss: 2.6268365383148193\n",
            "Epoch:583 - Loss: 2.617997884750366\n",
            "Epoch:584 - Loss: 2.598167896270752\n",
            "Epoch:585 - Loss: 2.6032793521881104\n",
            "Epoch:586 - Loss: 2.60957407951355\n",
            "Epoch:587 - Loss: 2.5987870693206787\n",
            "Epoch:588 - Loss: 2.6120715141296387\n",
            "Epoch:589 - Loss: 2.6032090187072754\n",
            "Epoch:590 - Loss: 2.6010701656341553\n",
            "Epoch:591 - Loss: 2.607015609741211\n",
            "Epoch:592 - Loss: 2.5974626541137695\n",
            "Epoch:593 - Loss: 2.6176209449768066\n",
            "Epoch:594 - Loss: 2.627821445465088\n",
            "Epoch:595 - Loss: 2.6006884574890137\n",
            "Epoch:596 - Loss: 2.5979883670806885\n",
            "Epoch:597 - Loss: 2.6138815879821777\n",
            "Epoch:598 - Loss: 2.629605293273926\n",
            "Epoch:599 - Loss: 2.625037431716919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "H9e66_GHVNN0",
        "outputId": "0393ee8a-0a16-4397-beb3-b0fa12effee1"
      },
      "source": [
        "plt.plot(mses)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5282231390>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXob0EPjVXZq",
        "outputId": "1f28ce43-09e3-4b01-b7c1-bdebfad41af9"
      },
      "source": [
        "tf.clip_by_value(urm @ sim_matrix, 0., 5.)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(610, 9724), dtype=float32, numpy=\n",
              "array([[4.1198769e+00, 4.2653818e+00, 4.1442504e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 5.1378205e-02],\n",
              "       [5.4507166e-01, 3.6893398e-01, 2.8853881e-01, ..., 4.3933537e-02,\n",
              "        5.1679142e-02, 5.2021250e-02],\n",
              "       [1.2345826e-01, 1.8353513e-01, 1.7450014e-01, ..., 1.6052402e-03,\n",
              "        7.5449137e-04, 2.6775086e-03],\n",
              "       ...,\n",
              "       [5.0000000e+00, 5.0000000e+00, 5.0000000e+00, ..., 1.5422268e-02,\n",
              "        2.4211006e-02, 2.4090995e-01],\n",
              "       [1.8412304e+00, 1.2207282e+00, 8.9727044e-01, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 1.0612406e-02],\n",
              "       [5.0000000e+00, 5.0000000e+00, 5.0000000e+00, ..., 3.1941333e-01,\n",
              "        3.0100960e-01, 4.6874857e-01]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbClLsQdVZBF",
        "outputId": "eacb3a10-3060-4dc2-cee1-565d08de9659"
      },
      "source": [
        "masked_mse(tf.clip_by_value(urm @ sim_matrix, 0., 5.), urm, mask, sim_matrix, 0.)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=2.590948>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmd6vTaGVagD"
      },
      "source": [
        "k=tf.keras"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImKqFuZFViIa"
      },
      "source": [
        "ratings = pd.read_csv(CUR_DIR + '/ml-latest-small/ratings.csv')\r\n",
        "\r\n",
        "C = 3\r\n",
        "total_mean = ratings.rating.mean()\r\n",
        "ratings['normalized_rating'] = ratings.rating - total_mean\r\n",
        "\r\n",
        "b_item = ratings.groupby('movieId').normalized_rating.sum() / (ratings.groupby('movieId').userId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_item, columns=['b_item']), left_on='movieId', right_index=True, how='inner')\r\n",
        "ratings['norm_item_rating'] = ratings.normalized_rating - ratings.b_item\r\n",
        "\r\n",
        "b_user = ratings.groupby('userId').norm_item_rating.sum() / (ratings.groupby('userId').movieId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_user, columns=['b_user']), left_on='userId', right_index=True, how='inner')\r\n",
        "\r\n",
        "ratings['normr_user_item_rating'] = total_mean + ratings.b_item + ratings.b_user"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRsF6ysSVil1",
        "outputId": "5dfe128a-ce9b-45a6-800c-d85e5316a8b2"
      },
      "source": [
        "b_item"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "movieId\n",
              "1         0.413602\n",
              "2        -0.067887\n",
              "3        -0.228745\n",
              "4        -0.801090\n",
              "5        -0.405313\n",
              "            ...   \n",
              "193581    0.124611\n",
              "193583   -0.000389\n",
              "193585   -0.000389\n",
              "193587   -0.000389\n",
              "193609    0.124611\n",
              "Length: 9724, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bESxRn4UVmOQ"
      },
      "source": [
        "urm = tf.constant(ratings.pivot(index='userId', columns='movieId', values='normr_user_item_rating').fillna(0.).values, dtype=tf.float32)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns0-2K6rVoFe"
      },
      "source": [
        "mask = tf.not_equal(urm, tf.constant(0., dtype=tf.float32))\r\n",
        "non_zero_rating_ixs = tf.where(mask)\r\n",
        "non_zero_ratings = tf.gather_nd(urm, non_zero_rating_ixs)\r\n",
        "\r\n",
        "split = 0.90\r\n",
        "split_ix = int(split * non_zero_rating_ixs.shape[0])\r\n",
        "\r\n",
        "non_zero_rating_ixs_shuffled = tf.random.shuffle(tf.range(non_zero_ratings.shape))\r\n",
        "\r\n",
        "train_urm_ratings = tf.gather(non_zero_ratings, non_zero_rating_ixs_shuffled[:split_ix])\r\n",
        "train_urm_ratings_ixs = tf.gather(non_zero_rating_ixs, non_zero_rating_ixs_shuffled[:split_ix])\r\n",
        "test_urm_ratings = tf.gather(non_zero_ratings, non_zero_rating_ixs_shuffled[split_ix:])\r\n",
        "test_urm_ratings_ixs = tf.gather(non_zero_rating_ixs, non_zero_rating_ixs_shuffled[split_ix:])\r\n",
        "\r\n",
        "train_urm = tf.scatter_nd(train_urm_ratings_ixs, train_urm_ratings, urm.shape)\r\n",
        "test_urm = tf.scatter_nd(test_urm_ratings_ixs, test_urm_ratings, urm.shape)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf1HFXDkVsLk",
        "outputId": "086d7993-2c8e-4e5f-c474-b013c74abcee"
      },
      "source": [
        "test_urm_ratings_ixs"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10084, 2), dtype=int64, numpy=\n",
              "array([[ 413, 3047],\n",
              "       [  73, 1257],\n",
              "       [ 387,  898],\n",
              "       ...,\n",
              "       [ 273, 5011],\n",
              "       [ 273, 1324],\n",
              "       [ 317,  508]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py8lcTClVs8x"
      },
      "source": [
        "@tf.function\r\n",
        "def masked_mse(y_pred, y_true, mask, weights_1, lamb1, weights_2, lamb2):\r\n",
        "    y_pred_masked = tf.boolean_mask(y_pred, mask)\r\n",
        "    y_true_masked = tf.boolean_mask(y_true, mask)\r\n",
        "    return tf.losses.mean_squared_error(y_true_masked, y_pred_masked) + lamb1 * tf.norm(weights_1) + lamb2 * tf.norm(weights_2)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5VAl4HnVxCH",
        "outputId": "1339f77c-5592-44db-afbb-e5af1279f366"
      },
      "source": [
        "emb_dim = 30\r\n",
        "user_emb = tf.Variable(tf.random.uniform(shape=(urm.shape[0],emb_dim)), trainable=True)\r\n",
        "item_emb = tf.Variable(tf.random.uniform(shape=(urm.shape[1],emb_dim)), trainable=True)\r\n",
        "\r\n",
        "mask = tf.not_equal(train_urm, tf.constant(0, dtype=tf.float32))\r\n",
        "test_mask = tf.not_equal(test_urm, 0.)\r\n",
        "\r\n",
        "epochs = 400\r\n",
        "opti = tf.optimizers.Adam()\r\n",
        "loss = masked_mse\r\n",
        "train_mses = []\r\n",
        "test_mses = []\r\n",
        "\r\n",
        "for e in range(epochs):\r\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as gt1:\r\n",
        "      \r\n",
        "    gt1.watch(user_emb)\r\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as gt2:\r\n",
        "\r\n",
        "      gt2.watch(item_emb)\r\n",
        "\r\n",
        "      preds = tf.matmul(user_emb, item_emb, transpose_b=True)\r\n",
        "      mse = loss(preds, train_urm, mask, user_emb, 0.5, item_emb, 0.4)\r\n",
        "      \r\n",
        "      grads = gt1.gradient(mse, user_emb)\r\n",
        "      opti.apply_gradients(grads_and_vars=zip([grads], [user_emb])) \r\n",
        "\r\n",
        "      grads = gt2.gradient(mse, item_emb)\r\n",
        "      opti.apply_gradients(grads_and_vars=zip([grads], [item_emb]))\r\n",
        "\r\n",
        "      test_mses.append(masked_mse(tf.matmul(user_emb, item_emb, transpose_b=True), test_urm, test_mask, 0.,0.,0.,0.))\r\n",
        "      train_mses.append(masked_mse(tf.matmul(user_emb, item_emb, transpose_b=True), train_urm, mask, 0.,0.,0.,0.))\r\n",
        "      print(f'Epoch: {e} - Train Loss: {train_mses[-1]} - Test Loss: {test_mses[-1]}')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 - Train Loss: 16.956300735473633 - Test Loss: 16.799762725830078\n",
            "Epoch: 1 - Train Loss: 16.762908935546875 - Test Loss: 16.607070922851562\n",
            "Epoch: 2 - Train Loss: 16.56734848022461 - Test Loss: 16.412214279174805\n",
            "Epoch: 3 - Train Loss: 16.36725425720215 - Test Loss: 16.212841033935547\n",
            "Epoch: 4 - Train Loss: 16.162073135375977 - Test Loss: 16.00840187072754\n",
            "Epoch: 5 - Train Loss: 15.95179271697998 - Test Loss: 15.7988862991333\n",
            "Epoch: 6 - Train Loss: 15.736639022827148 - Test Loss: 15.584519386291504\n",
            "Epoch: 7 - Train Loss: 15.51695728302002 - Test Loss: 15.365650177001953\n",
            "Epoch: 8 - Train Loss: 15.293163299560547 - Test Loss: 15.142688751220703\n",
            "Epoch: 9 - Train Loss: 15.065713882446289 - Test Loss: 14.916098594665527\n",
            "Epoch: 10 - Train Loss: 14.835090637207031 - Test Loss: 14.68635368347168\n",
            "Epoch: 11 - Train Loss: 14.601786613464355 - Test Loss: 14.453948974609375\n",
            "Epoch: 12 - Train Loss: 14.366297721862793 - Test Loss: 14.219379425048828\n",
            "Epoch: 13 - Train Loss: 14.129108428955078 - Test Loss: 13.983124732971191\n",
            "Epoch: 14 - Train Loss: 13.890693664550781 - Test Loss: 13.745662689208984\n",
            "Epoch: 15 - Train Loss: 13.651512145996094 - Test Loss: 13.507447242736816\n",
            "Epoch: 16 - Train Loss: 13.41199779510498 - Test Loss: 13.268914222717285\n",
            "Epoch: 17 - Train Loss: 13.172562599182129 - Test Loss: 13.030471801757812\n",
            "Epoch: 18 - Train Loss: 12.93359661102295 - Test Loss: 12.792508125305176\n",
            "Epoch: 19 - Train Loss: 12.695454597473145 - Test Loss: 12.555378913879395\n",
            "Epoch: 20 - Train Loss: 12.45847225189209 - Test Loss: 12.319416046142578\n",
            "Epoch: 21 - Train Loss: 12.222952842712402 - Test Loss: 12.084922790527344\n",
            "Epoch: 22 - Train Loss: 11.989173889160156 - Test Loss: 11.8521728515625\n",
            "Epoch: 23 - Train Loss: 11.75738525390625 - Test Loss: 11.621418952941895\n",
            "Epoch: 24 - Train Loss: 11.527814865112305 - Test Loss: 11.39288330078125\n",
            "Epoch: 25 - Train Loss: 11.300660133361816 - Test Loss: 11.166766166687012\n",
            "Epoch: 26 - Train Loss: 11.076101303100586 - Test Loss: 10.943244934082031\n",
            "Epoch: 27 - Train Loss: 10.854290962219238 - Test Loss: 10.722472190856934\n",
            "Epoch: 28 - Train Loss: 10.635366439819336 - Test Loss: 10.504583358764648\n",
            "Epoch: 29 - Train Loss: 10.419445037841797 - Test Loss: 10.289693832397461\n",
            "Epoch: 30 - Train Loss: 10.206623077392578 - Test Loss: 10.077901840209961\n",
            "Epoch: 31 - Train Loss: 9.996984481811523 - Test Loss: 9.86928939819336\n",
            "Epoch: 32 - Train Loss: 9.790597915649414 - Test Loss: 9.663923263549805\n",
            "Epoch: 33 - Train Loss: 9.587515830993652 - Test Loss: 9.461857795715332\n",
            "Epoch: 34 - Train Loss: 9.387782096862793 - Test Loss: 9.263134956359863\n",
            "Epoch: 35 - Train Loss: 9.191425323486328 - Test Loss: 9.06778335571289\n",
            "Epoch: 36 - Train Loss: 8.998467445373535 - Test Loss: 8.875825881958008\n",
            "Epoch: 37 - Train Loss: 8.808920860290527 - Test Loss: 8.687272071838379\n",
            "Epoch: 38 - Train Loss: 8.622787475585938 - Test Loss: 8.502126693725586\n",
            "Epoch: 39 - Train Loss: 8.44006633758545 - Test Loss: 8.320384979248047\n",
            "Epoch: 40 - Train Loss: 8.26074504852295 - Test Loss: 8.142036437988281\n",
            "Epoch: 41 - Train Loss: 8.084808349609375 - Test Loss: 7.967066287994385\n",
            "Epoch: 42 - Train Loss: 7.912234783172607 - Test Loss: 7.79545259475708\n",
            "Epoch: 43 - Train Loss: 7.74299955368042 - Test Loss: 7.627171039581299\n",
            "Epoch: 44 - Train Loss: 7.577073574066162 - Test Loss: 7.462191104888916\n",
            "Epoch: 45 - Train Loss: 7.414424896240234 - Test Loss: 7.30048131942749\n",
            "Epoch: 46 - Train Loss: 7.255014896392822 - Test Loss: 7.142005443572998\n",
            "Epoch: 47 - Train Loss: 7.098809242248535 - Test Loss: 6.986725330352783\n",
            "Epoch: 48 - Train Loss: 6.945764064788818 - Test Loss: 6.834601879119873\n",
            "Epoch: 49 - Train Loss: 6.795839309692383 - Test Loss: 6.6855902671813965\n",
            "Epoch: 50 - Train Loss: 6.648991107940674 - Test Loss: 6.539649963378906\n",
            "Epoch: 51 - Train Loss: 6.50517463684082 - Test Loss: 6.396733283996582\n",
            "Epoch: 52 - Train Loss: 6.364341735839844 - Test Loss: 6.256795883178711\n",
            "Epoch: 53 - Train Loss: 6.226449012756348 - Test Loss: 6.119791030883789\n",
            "Epoch: 54 - Train Loss: 6.091445446014404 - Test Loss: 5.985671043395996\n",
            "Epoch: 55 - Train Loss: 5.959285259246826 - Test Loss: 5.854388236999512\n",
            "Epoch: 56 - Train Loss: 5.829919815063477 - Test Loss: 5.725894451141357\n",
            "Epoch: 57 - Train Loss: 5.703301429748535 - Test Loss: 5.600141525268555\n",
            "Epoch: 58 - Train Loss: 5.579380035400391 - Test Loss: 5.477081775665283\n",
            "Epoch: 59 - Train Loss: 5.458108901977539 - Test Loss: 5.356665134429932\n",
            "Epoch: 60 - Train Loss: 5.3394389152526855 - Test Loss: 5.2388458251953125\n",
            "Epoch: 61 - Train Loss: 5.223322868347168 - Test Loss: 5.123574733734131\n",
            "Epoch: 62 - Train Loss: 5.109713077545166 - Test Loss: 5.010805130004883\n",
            "Epoch: 63 - Train Loss: 4.998561859130859 - Test Loss: 4.900489807128906\n",
            "Epoch: 64 - Train Loss: 4.8898234367370605 - Test Loss: 4.792582035064697\n",
            "Epoch: 65 - Train Loss: 4.783450126647949 - Test Loss: 4.687036514282227\n",
            "Epoch: 66 - Train Loss: 4.679398059844971 - Test Loss: 4.583806991577148\n",
            "Epoch: 67 - Train Loss: 4.577620983123779 - Test Loss: 4.482848167419434\n",
            "Epoch: 68 - Train Loss: 4.478073596954346 - Test Loss: 4.384116172790527\n",
            "Epoch: 69 - Train Loss: 4.38071346282959 - Test Loss: 4.287568092346191\n",
            "Epoch: 70 - Train Loss: 4.285496234893799 - Test Loss: 4.193159580230713\n",
            "Epoch: 71 - Train Loss: 4.192379951477051 - Test Loss: 4.10084867477417\n",
            "Epoch: 72 - Train Loss: 4.101322174072266 - Test Loss: 4.010592937469482\n",
            "Epoch: 73 - Train Loss: 4.012281894683838 - Test Loss: 3.9223523139953613\n",
            "Epoch: 74 - Train Loss: 3.9252185821533203 - Test Loss: 3.836085081100464\n",
            "Epoch: 75 - Train Loss: 3.8400917053222656 - Test Loss: 3.7517523765563965\n",
            "Epoch: 76 - Train Loss: 3.7568628787994385 - Test Loss: 3.6693153381347656\n",
            "Epoch: 77 - Train Loss: 3.6754934787750244 - Test Loss: 3.5887343883514404\n",
            "Epoch: 78 - Train Loss: 3.595944404602051 - Test Loss: 3.5099732875823975\n",
            "Epoch: 79 - Train Loss: 3.5181801319122314 - Test Loss: 3.4329934120178223\n",
            "Epoch: 80 - Train Loss: 3.4421634674072266 - Test Loss: 3.357759952545166\n",
            "Epoch: 81 - Train Loss: 3.367858648300171 - Test Loss: 3.2842366695404053\n",
            "Epoch: 82 - Train Loss: 3.2952311038970947 - Test Loss: 3.212388515472412\n",
            "Epoch: 83 - Train Loss: 3.22424578666687 - Test Loss: 3.142181158065796\n",
            "Epoch: 84 - Train Loss: 3.1548690795898438 - Test Loss: 3.0735814571380615\n",
            "Epoch: 85 - Train Loss: 3.087068557739258 - Test Loss: 3.006556272506714\n",
            "Epoch: 86 - Train Loss: 3.0208115577697754 - Test Loss: 2.941073179244995\n",
            "Epoch: 87 - Train Loss: 2.9560656547546387 - Test Loss: 2.8771002292633057\n",
            "Epoch: 88 - Train Loss: 2.8928005695343018 - Test Loss: 2.8146069049835205\n",
            "Epoch: 89 - Train Loss: 2.8309850692749023 - Test Loss: 2.7535626888275146\n",
            "Epoch: 90 - Train Loss: 2.77059006690979 - Test Loss: 2.6939377784729004\n",
            "Epoch: 91 - Train Loss: 2.7115859985351562 - Test Loss: 2.6357030868530273\n",
            "Epoch: 92 - Train Loss: 2.6539440155029297 - Test Loss: 2.578829288482666\n",
            "Epoch: 93 - Train Loss: 2.5976359844207764 - Test Loss: 2.523289680480957\n",
            "Epoch: 94 - Train Loss: 2.5426347255706787 - Test Loss: 2.4690561294555664\n",
            "Epoch: 95 - Train Loss: 2.4889132976531982 - Test Loss: 2.4161014556884766\n",
            "Epoch: 96 - Train Loss: 2.4364449977874756 - Test Loss: 2.3643999099731445\n",
            "Epoch: 97 - Train Loss: 2.385204553604126 - Test Loss: 2.3139259815216064\n",
            "Epoch: 98 - Train Loss: 2.3351664543151855 - Test Loss: 2.264653444290161\n",
            "Epoch: 99 - Train Loss: 2.2863054275512695 - Test Loss: 2.2165586948394775\n",
            "Epoch: 100 - Train Loss: 2.238598108291626 - Test Loss: 2.169616937637329\n",
            "Epoch: 101 - Train Loss: 2.1920204162597656 - Test Loss: 2.123804807662964\n",
            "Epoch: 102 - Train Loss: 2.1465492248535156 - Test Loss: 2.07909893989563\n",
            "Epoch: 103 - Train Loss: 2.102161407470703 - Test Loss: 2.0354766845703125\n",
            "Epoch: 104 - Train Loss: 2.058835506439209 - Test Loss: 1.9929161071777344\n",
            "Epoch: 105 - Train Loss: 2.0165488719940186 - Test Loss: 1.9513953924179077\n",
            "Epoch: 106 - Train Loss: 1.9752812385559082 - Test Loss: 1.9108930826187134\n",
            "Epoch: 107 - Train Loss: 1.9350106716156006 - Test Loss: 1.871388554573059\n",
            "Epoch: 108 - Train Loss: 1.8957175016403198 - Test Loss: 1.8328614234924316\n",
            "Epoch: 109 - Train Loss: 1.8573814630508423 - Test Loss: 1.7952916622161865\n",
            "Epoch: 110 - Train Loss: 1.8199831247329712 - Test Loss: 1.7586597204208374\n",
            "Epoch: 111 - Train Loss: 1.7835031747817993 - Test Loss: 1.7229467630386353\n",
            "Epoch: 112 - Train Loss: 1.7479232549667358 - Test Loss: 1.688133955001831\n",
            "Epoch: 113 - Train Loss: 1.71322500705719 - Test Loss: 1.654203176498413\n",
            "Epoch: 114 - Train Loss: 1.6793901920318604 - Test Loss: 1.621135950088501\n",
            "Epoch: 115 - Train Loss: 1.6464014053344727 - Test Loss: 1.5889153480529785\n",
            "Epoch: 116 - Train Loss: 1.6142412424087524 - Test Loss: 1.5575239658355713\n",
            "Epoch: 117 - Train Loss: 1.5828936100006104 - Test Loss: 1.5269453525543213\n",
            "Epoch: 118 - Train Loss: 1.5523414611816406 - Test Loss: 1.4971626996994019\n",
            "Epoch: 119 - Train Loss: 1.5225690603256226 - Test Loss: 1.4681600332260132\n",
            "Epoch: 120 - Train Loss: 1.4935604333877563 - Test Loss: 1.4399218559265137\n",
            "Epoch: 121 - Train Loss: 1.4653003215789795 - Test Loss: 1.4124325513839722\n",
            "Epoch: 122 - Train Loss: 1.437773585319519 - Test Loss: 1.3856772184371948\n",
            "Epoch: 123 - Train Loss: 1.4109655618667603 - Test Loss: 1.3596409559249878\n",
            "Epoch: 124 - Train Loss: 1.384861707687378 - Test Loss: 1.3343093395233154\n",
            "Epoch: 125 - Train Loss: 1.359447956085205 - Test Loss: 1.3096685409545898\n",
            "Epoch: 126 - Train Loss: 1.3347105979919434 - Test Loss: 1.285704493522644\n",
            "Epoch: 127 - Train Loss: 1.310636043548584 - Test Loss: 1.2624038457870483\n",
            "Epoch: 128 - Train Loss: 1.2872110605239868 - Test Loss: 1.239753246307373\n",
            "Epoch: 129 - Train Loss: 1.2644227743148804 - Test Loss: 1.2177398204803467\n",
            "Epoch: 130 - Train Loss: 1.2422585487365723 - Test Loss: 1.1963510513305664\n",
            "Epoch: 131 - Train Loss: 1.2207058668136597 - Test Loss: 1.1755743026733398\n",
            "Epoch: 132 - Train Loss: 1.199752688407898 - Test Loss: 1.1553975343704224\n",
            "Epoch: 133 - Train Loss: 1.1793874502182007 - Test Loss: 1.135809063911438\n",
            "Epoch: 134 - Train Loss: 1.1595978736877441 - Test Loss: 1.1167972087860107\n",
            "Epoch: 135 - Train Loss: 1.1403732299804688 - Test Loss: 1.0983506441116333\n",
            "Epoch: 136 - Train Loss: 1.1217020750045776 - Test Loss: 1.0804580450057983\n",
            "Epoch: 137 - Train Loss: 1.1035736799240112 - Test Loss: 1.0631088018417358\n",
            "Epoch: 138 - Train Loss: 1.085977554321289 - Test Loss: 1.0462920665740967\n",
            "Epoch: 139 - Train Loss: 1.0689030885696411 - Test Loss: 1.0299975872039795\n",
            "Epoch: 140 - Train Loss: 1.0523401498794556 - Test Loss: 1.0142152309417725\n",
            "Epoch: 141 - Train Loss: 1.0362786054611206 - Test Loss: 0.9989349246025085\n",
            "Epoch: 142 - Train Loss: 1.0207090377807617 - Test Loss: 0.9841469526290894\n",
            "Epoch: 143 - Train Loss: 1.0056219100952148 - Test Loss: 0.9698415994644165\n",
            "Epoch: 144 - Train Loss: 0.9910076260566711 - Test Loss: 0.9560098648071289\n",
            "Epoch: 145 - Train Loss: 0.9768571853637695 - Test Loss: 0.942642331123352\n",
            "Epoch: 146 - Train Loss: 0.9631617069244385 - Test Loss: 0.9297302961349487\n",
            "Epoch: 147 - Train Loss: 0.9499123692512512 - Test Loss: 0.9172648191452026\n",
            "Epoch: 148 - Train Loss: 0.9371008276939392 - Test Loss: 0.9052374362945557\n",
            "Epoch: 149 - Train Loss: 0.92471843957901 - Test Loss: 0.8936397433280945\n",
            "Epoch: 150 - Train Loss: 0.9127570986747742 - Test Loss: 0.8824634552001953\n",
            "Epoch: 151 - Train Loss: 0.9012089371681213 - Test Loss: 0.8717007637023926\n",
            "Epoch: 152 - Train Loss: 0.8900659680366516 - Test Loss: 0.8613437414169312\n",
            "Epoch: 153 - Train Loss: 0.8793207406997681 - Test Loss: 0.8513845801353455\n",
            "Epoch: 154 - Train Loss: 0.8689655065536499 - Test Loss: 0.8418159484863281\n",
            "Epoch: 155 - Train Loss: 0.8589931130409241 - Test Loss: 0.8326303958892822\n",
            "Epoch: 156 - Train Loss: 0.8493962287902832 - Test Loss: 0.8238208293914795\n",
            "Epoch: 157 - Train Loss: 0.8401678800582886 - Test Loss: 0.8153800368309021\n",
            "Epoch: 158 - Train Loss: 0.831301212310791 - Test Loss: 0.8073011636734009\n",
            "Epoch: 159 - Train Loss: 0.8227896094322205 - Test Loss: 0.7995776534080505\n",
            "Epoch: 160 - Train Loss: 0.8146263360977173 - Test Loss: 0.7922027707099915\n",
            "Epoch: 161 - Train Loss: 0.8068049550056458 - Test Loss: 0.7851700782775879\n",
            "Epoch: 162 - Train Loss: 0.799319326877594 - Test Loss: 0.7784731984138489\n",
            "Epoch: 163 - Train Loss: 0.792163074016571 - Test Loss: 0.7721061706542969\n",
            "Epoch: 164 - Train Loss: 0.7853302955627441 - Test Loss: 0.7660626173019409\n",
            "Epoch: 165 - Train Loss: 0.7788151502609253 - Test Loss: 0.7603369355201721\n",
            "Epoch: 166 - Train Loss: 0.772611677646637 - Test Loss: 0.7549231052398682\n",
            "Epoch: 167 - Train Loss: 0.766714334487915 - Test Loss: 0.7498155832290649\n",
            "Epoch: 168 - Train Loss: 0.7611176371574402 - Test Loss: 0.7450088858604431\n",
            "Epoch: 169 - Train Loss: 0.7558161020278931 - Test Loss: 0.7404974102973938\n",
            "Epoch: 170 - Train Loss: 0.7508044838905334 - Test Loss: 0.7362759113311768\n",
            "Epoch: 171 - Train Loss: 0.7460775375366211 - Test Loss: 0.7323393225669861\n",
            "Epoch: 172 - Train Loss: 0.7416303753852844 - Test Loss: 0.7286823987960815\n",
            "Epoch: 173 - Train Loss: 0.737457811832428 - Test Loss: 0.7253001928329468\n",
            "Epoch: 174 - Train Loss: 0.7335551381111145 - Test Loss: 0.7221879363059998\n",
            "Epoch: 175 - Train Loss: 0.7299174666404724 - Test Loss: 0.7193407416343689\n",
            "Epoch: 176 - Train Loss: 0.7265402674674988 - Test Loss: 0.7167540192604065\n",
            "Epoch: 177 - Train Loss: 0.7234190702438354 - Test Loss: 0.7144231796264648\n",
            "Epoch: 178 - Train Loss: 0.7205492854118347 - Test Loss: 0.7123438119888306\n",
            "Epoch: 179 - Train Loss: 0.7179266810417175 - Test Loss: 0.7105115056037903\n",
            "Epoch: 180 - Train Loss: 0.7155468463897705 - Test Loss: 0.7089217901229858\n",
            "Epoch: 181 - Train Loss: 0.7134057879447937 - Test Loss: 0.7075709700584412\n",
            "Epoch: 182 - Train Loss: 0.7114992737770081 - Test Loss: 0.7064544558525085\n",
            "Epoch: 183 - Train Loss: 0.7098234295845032 - Test Loss: 0.7055685520172119\n",
            "Epoch: 184 - Train Loss: 0.7083743214607239 - Test Loss: 0.7049091458320618\n",
            "Epoch: 185 - Train Loss: 0.7071480751037598 - Test Loss: 0.704472541809082\n",
            "Epoch: 186 - Train Loss: 0.7061410546302795 - Test Loss: 0.7042548060417175\n",
            "Epoch: 187 - Train Loss: 0.7053495645523071 - Test Loss: 0.7042525410652161\n",
            "Epoch: 188 - Train Loss: 0.704770028591156 - Test Loss: 0.704461932182312\n",
            "Epoch: 189 - Train Loss: 0.7043988108634949 - Test Loss: 0.7048795223236084\n",
            "Epoch: 190 - Train Loss: 0.7042327523231506 - Test Loss: 0.7055017948150635\n",
            "Epoch: 191 - Train Loss: 0.7042683362960815 - Test Loss: 0.7063255310058594\n",
            "Epoch: 192 - Train Loss: 0.7045022249221802 - Test Loss: 0.7073473930358887\n",
            "Epoch: 193 - Train Loss: 0.704931378364563 - Test Loss: 0.708564043045044\n",
            "Epoch: 194 - Train Loss: 0.7055525183677673 - Test Loss: 0.7099724411964417\n",
            "Epoch: 195 - Train Loss: 0.7063627243041992 - Test Loss: 0.7115694284439087\n",
            "Epoch: 196 - Train Loss: 0.7073588371276855 - Test Loss: 0.7133520841598511\n",
            "Epoch: 197 - Train Loss: 0.7085379958152771 - Test Loss: 0.7153172492980957\n",
            "Epoch: 198 - Train Loss: 0.7098972797393799 - Test Loss: 0.7174621820449829\n",
            "Epoch: 199 - Train Loss: 0.7114338874816895 - Test Loss: 0.7197840809822083\n",
            "Epoch: 200 - Train Loss: 0.7131451368331909 - Test Loss: 0.7222800850868225\n",
            "Epoch: 201 - Train Loss: 0.7150282263755798 - Test Loss: 0.7249473929405212\n",
            "Epoch: 202 - Train Loss: 0.7170805931091309 - Test Loss: 0.7277836203575134\n",
            "Epoch: 203 - Train Loss: 0.7192996740341187 - Test Loss: 0.7307859063148499\n",
            "Epoch: 204 - Train Loss: 0.7216827869415283 - Test Loss: 0.7339518666267395\n",
            "Epoch: 205 - Train Loss: 0.7242276668548584 - Test Loss: 0.737278938293457\n",
            "Epoch: 206 - Train Loss: 0.7269317507743835 - Test Loss: 0.7407646179199219\n",
            "Epoch: 207 - Train Loss: 0.7297927737236023 - Test Loss: 0.7444067001342773\n",
            "Epoch: 208 - Train Loss: 0.7328084111213684 - Test Loss: 0.7482028007507324\n",
            "Epoch: 209 - Train Loss: 0.7359762787818909 - Test Loss: 0.7521504163742065\n",
            "Epoch: 210 - Train Loss: 0.7392942905426025 - Test Loss: 0.7562476396560669\n",
            "Epoch: 211 - Train Loss: 0.7427602410316467 - Test Loss: 0.7604920864105225\n",
            "Epoch: 212 - Train Loss: 0.7463720440864563 - Test Loss: 0.7648816108703613\n",
            "Epoch: 213 - Train Loss: 0.7501275539398193 - Test Loss: 0.7694142460823059\n",
            "Epoch: 214 - Train Loss: 0.7540248036384583 - Test Loss: 0.7740877866744995\n",
            "Epoch: 215 - Train Loss: 0.7580617070198059 - Test Loss: 0.7789004445075989\n",
            "Epoch: 216 - Train Loss: 0.7622365355491638 - Test Loss: 0.7838500738143921\n",
            "Epoch: 217 - Train Loss: 0.7665471434593201 - Test Loss: 0.7889347672462463\n",
            "Epoch: 218 - Train Loss: 0.7709918022155762 - Test Loss: 0.7941526770591736\n",
            "Epoch: 219 - Train Loss: 0.7755687236785889 - Test Loss: 0.7995020747184753\n",
            "Epoch: 220 - Train Loss: 0.7802760004997253 - Test Loss: 0.8049810528755188\n",
            "Epoch: 221 - Train Loss: 0.7851120233535767 - Test Loss: 0.8105877637863159\n",
            "Epoch: 222 - Train Loss: 0.7900750637054443 - Test Loss: 0.816320538520813\n",
            "Epoch: 223 - Train Loss: 0.7951633334159851 - Test Loss: 0.8221778869628906\n",
            "Epoch: 224 - Train Loss: 0.8003753423690796 - Test Loss: 0.828157901763916\n",
            "Epoch: 225 - Train Loss: 0.8057095408439636 - Test Loss: 0.8342592120170593\n",
            "Epoch: 226 - Train Loss: 0.811164140701294 - Test Loss: 0.8404800295829773\n",
            "Epoch: 227 - Train Loss: 0.8167377710342407 - Test Loss: 0.8468188643455505\n",
            "Epoch: 228 - Train Loss: 0.822428822517395 - Test Loss: 0.8532741069793701\n",
            "Epoch: 229 - Train Loss: 0.8282361626625061 - Test Loss: 0.8598445653915405\n",
            "Epoch: 230 - Train Loss: 0.8341580033302307 - Test Loss: 0.8665286898612976\n",
            "Epoch: 231 - Train Loss: 0.8401930928230286 - Test Loss: 0.8733248114585876\n",
            "Epoch: 232 - Train Loss: 0.8463400602340698 - Test Loss: 0.8802318572998047\n",
            "Epoch: 233 - Train Loss: 0.8525975942611694 - Test Loss: 0.8872482776641846\n",
            "Epoch: 234 - Train Loss: 0.8589642643928528 - Test Loss: 0.894372820854187\n",
            "Epoch: 235 - Train Loss: 0.8654389977455139 - Test Loss: 0.9016042351722717\n",
            "Epoch: 236 - Train Loss: 0.8720203638076782 - Test Loss: 0.9089412689208984\n",
            "Epoch: 237 - Train Loss: 0.8787071108818054 - Test Loss: 0.9163824915885925\n",
            "Epoch: 238 - Train Loss: 0.8854982256889343 - Test Loss: 0.9239267110824585\n",
            "Epoch: 239 - Train Loss: 0.8923924565315247 - Test Loss: 0.9315729141235352\n",
            "Epoch: 240 - Train Loss: 0.8993886709213257 - Test Loss: 0.9393198490142822\n",
            "Epoch: 241 - Train Loss: 0.9064856767654419 - Test Loss: 0.9471664428710938\n",
            "Epoch: 242 - Train Loss: 0.9136824011802673 - Test Loss: 0.9551116228103638\n",
            "Epoch: 243 - Train Loss: 0.9209778308868408 - Test Loss: 0.9631540179252625\n",
            "Epoch: 244 - Train Loss: 0.9283708930015564 - Test Loss: 0.9712929129600525\n",
            "Epoch: 245 - Train Loss: 0.9358605742454529 - Test Loss: 0.979526937007904\n",
            "Epoch: 246 - Train Loss: 0.9434458017349243 - Test Loss: 0.9878553152084351\n",
            "Epoch: 247 - Train Loss: 0.9511256217956543 - Test Loss: 0.9962769746780396\n",
            "Epoch: 248 - Train Loss: 0.9588992595672607 - Test Loss: 1.0047909021377563\n",
            "Epoch: 249 - Train Loss: 0.9667654633522034 - Test Loss: 1.0133962631225586\n",
            "Epoch: 250 - Train Loss: 0.9747236371040344 - Test Loss: 1.0220919847488403\n",
            "Epoch: 251 - Train Loss: 0.9827725887298584 - Test Loss: 1.0308771133422852\n",
            "Epoch: 252 - Train Loss: 0.9909117221832275 - Test Loss: 1.0397510528564453\n",
            "Epoch: 253 - Train Loss: 0.9991398453712463 - Test Loss: 1.0487124919891357\n",
            "Epoch: 254 - Train Loss: 1.0074564218521118 - Test Loss: 1.0577609539031982\n",
            "Epoch: 255 - Train Loss: 1.015860676765442 - Test Loss: 1.0668954849243164\n",
            "Epoch: 256 - Train Loss: 1.0243514776229858 - Test Loss: 1.0761152505874634\n",
            "Epoch: 257 - Train Loss: 1.0329283475875854 - Test Loss: 1.0854194164276123\n",
            "Epoch: 258 - Train Loss: 1.0415903329849243 - Test Loss: 1.0948071479797363\n",
            "Epoch: 259 - Train Loss: 1.0503368377685547 - Test Loss: 1.1042777299880981\n",
            "Epoch: 260 - Train Loss: 1.0591669082641602 - Test Loss: 1.11383056640625\n",
            "Epoch: 261 - Train Loss: 1.0680800676345825 - Test Loss: 1.1234647035598755\n",
            "Epoch: 262 - Train Loss: 1.077075481414795 - Test Loss: 1.133179783821106\n",
            "Epoch: 263 - Train Loss: 1.0861526727676392 - Test Loss: 1.1429743766784668\n",
            "Epoch: 264 - Train Loss: 1.0953106880187988 - Test Loss: 1.1528488397598267\n",
            "Epoch: 265 - Train Loss: 1.1045490503311157 - Test Loss: 1.1628016233444214\n",
            "Epoch: 266 - Train Loss: 1.113867163658142 - Test Loss: 1.1728323698043823\n",
            "Epoch: 267 - Train Loss: 1.123264193534851 - Test Loss: 1.1829406023025513\n",
            "Epoch: 268 - Train Loss: 1.1327396631240845 - Test Loss: 1.1931254863739014\n",
            "Epoch: 269 - Train Loss: 1.1422929763793945 - Test Loss: 1.2033864259719849\n",
            "Epoch: 270 - Train Loss: 1.151923656463623 - Test Loss: 1.2137229442596436\n",
            "Epoch: 271 - Train Loss: 1.1616308689117432 - Test Loss: 1.2241344451904297\n",
            "Epoch: 272 - Train Loss: 1.1714143753051758 - Test Loss: 1.234620213508606\n",
            "Epoch: 273 - Train Loss: 1.181273341178894 - Test Loss: 1.2451798915863037\n",
            "Epoch: 274 - Train Loss: 1.1912075281143188 - Test Loss: 1.255812644958496\n",
            "Epoch: 275 - Train Loss: 1.2012163400650024 - Test Loss: 1.266518235206604\n",
            "Epoch: 276 - Train Loss: 1.211298942565918 - Test Loss: 1.2772960662841797\n",
            "Epoch: 277 - Train Loss: 1.2214553356170654 - Test Loss: 1.2881455421447754\n",
            "Epoch: 278 - Train Loss: 1.2316848039627075 - Test Loss: 1.2990663051605225\n",
            "Epoch: 279 - Train Loss: 1.2419867515563965 - Test Loss: 1.3100578784942627\n",
            "Epoch: 280 - Train Loss: 1.2523610591888428 - Test Loss: 1.3211194276809692\n",
            "Epoch: 281 - Train Loss: 1.2628068923950195 - Test Loss: 1.3322508335113525\n",
            "Epoch: 282 - Train Loss: 1.2733240127563477 - Test Loss: 1.3434516191482544\n",
            "Epoch: 283 - Train Loss: 1.283912181854248 - Test Loss: 1.3547213077545166\n",
            "Epoch: 284 - Train Loss: 1.2945704460144043 - Test Loss: 1.366059422492981\n",
            "Epoch: 285 - Train Loss: 1.3052990436553955 - Test Loss: 1.3774654865264893\n",
            "Epoch: 286 - Train Loss: 1.3160970211029053 - Test Loss: 1.3889392614364624\n",
            "Epoch: 287 - Train Loss: 1.3269644975662231 - Test Loss: 1.4004801511764526\n",
            "Epoch: 288 - Train Loss: 1.3379006385803223 - Test Loss: 1.4120879173278809\n",
            "Epoch: 289 - Train Loss: 1.348905324935913 - Test Loss: 1.4237622022628784\n",
            "Epoch: 290 - Train Loss: 1.3599783182144165 - Test Loss: 1.435502290725708\n",
            "Epoch: 291 - Train Loss: 1.3711189031600952 - Test Loss: 1.4473081827163696\n",
            "Epoch: 292 - Train Loss: 1.3823269605636597 - Test Loss: 1.4591796398162842\n",
            "Epoch: 293 - Train Loss: 1.3936020135879517 - Test Loss: 1.4711159467697144\n",
            "Epoch: 294 - Train Loss: 1.4049440622329712 - Test Loss: 1.483116865158081\n",
            "Epoch: 295 - Train Loss: 1.4163525104522705 - Test Loss: 1.4951820373535156\n",
            "Epoch: 296 - Train Loss: 1.427827000617981 - Test Loss: 1.507311224937439\n",
            "Epoch: 297 - Train Loss: 1.4393675327301025 - Test Loss: 1.519504189491272\n",
            "Epoch: 298 - Train Loss: 1.4509735107421875 - Test Loss: 1.531760334968567\n",
            "Epoch: 299 - Train Loss: 1.4626449346542358 - Test Loss: 1.5440796613693237\n",
            "Epoch: 300 - Train Loss: 1.4743810892105103 - Test Loss: 1.5564616918563843\n",
            "Epoch: 301 - Train Loss: 1.4861820936203003 - Test Loss: 1.5689061880111694\n",
            "Epoch: 302 - Train Loss: 1.4980475902557373 - Test Loss: 1.5814129114151\n",
            "Epoch: 303 - Train Loss: 1.509977102279663 - Test Loss: 1.5939815044403076\n",
            "Epoch: 304 - Train Loss: 1.5219707489013672 - Test Loss: 1.6066118478775024\n",
            "Epoch: 305 - Train Loss: 1.5340280532836914 - Test Loss: 1.619303584098816\n",
            "Epoch: 306 - Train Loss: 1.5461487770080566 - Test Loss: 1.6320563554763794\n",
            "Epoch: 307 - Train Loss: 1.5583326816558838 - Test Loss: 1.6448700428009033\n",
            "Epoch: 308 - Train Loss: 1.5705797672271729 - Test Loss: 1.657744288444519\n",
            "Epoch: 309 - Train Loss: 1.5828893184661865 - Test Loss: 1.6706792116165161\n",
            "Epoch: 310 - Train Loss: 1.5952616930007935 - Test Loss: 1.6836739778518677\n",
            "Epoch: 311 - Train Loss: 1.607696294784546 - Test Loss: 1.6967289447784424\n",
            "Epoch: 312 - Train Loss: 1.6201932430267334 - Test Loss: 1.7098435163497925\n",
            "Epoch: 313 - Train Loss: 1.6327519416809082 - Test Loss: 1.723017692565918\n",
            "Epoch: 314 - Train Loss: 1.6453725099563599 - Test Loss: 1.7362512350082397\n",
            "Epoch: 315 - Train Loss: 1.6580545902252197 - Test Loss: 1.7495439052581787\n",
            "Epoch: 316 - Train Loss: 1.6707980632781982 - Test Loss: 1.7628953456878662\n",
            "Epoch: 317 - Train Loss: 1.6836026906967163 - Test Loss: 1.7763055562973022\n",
            "Epoch: 318 - Train Loss: 1.696468472480774 - Test Loss: 1.7897745370864868\n",
            "Epoch: 319 - Train Loss: 1.709395170211792 - Test Loss: 1.803301453590393\n",
            "Epoch: 320 - Train Loss: 1.7223825454711914 - Test Loss: 1.8168870210647583\n",
            "Epoch: 321 - Train Loss: 1.735430359840393 - Test Loss: 1.8305304050445557\n",
            "Epoch: 322 - Train Loss: 1.748538613319397 - Test Loss: 1.8442314863204956\n",
            "Epoch: 323 - Train Loss: 1.7617071866989136 - Test Loss: 1.8579905033111572\n",
            "Epoch: 324 - Train Loss: 1.7749359607696533 - Test Loss: 1.8718068599700928\n",
            "Epoch: 325 - Train Loss: 1.7882243394851685 - Test Loss: 1.8856810331344604\n",
            "Epoch: 326 - Train Loss: 1.8015729188919067 - Test Loss: 1.8996118307113647\n",
            "Epoch: 327 - Train Loss: 1.81498122215271 - Test Loss: 1.913599967956543\n",
            "Epoch: 328 - Train Loss: 1.8284486532211304 - Test Loss: 1.9276450872421265\n",
            "Epoch: 329 - Train Loss: 1.8419759273529053 - Test Loss: 1.9417471885681152\n",
            "Epoch: 330 - Train Loss: 1.855562448501587 - Test Loss: 1.955905556678772\n",
            "Epoch: 331 - Train Loss: 1.8692080974578857 - Test Loss: 1.9701205492019653\n",
            "Epoch: 332 - Train Loss: 1.8829129934310913 - Test Loss: 1.9843920469284058\n",
            "Epoch: 333 - Train Loss: 1.8966768980026245 - Test Loss: 1.9987199306488037\n",
            "Epoch: 334 - Train Loss: 1.9104995727539062 - Test Loss: 2.01310396194458\n",
            "Epoch: 335 - Train Loss: 1.924381136894226 - Test Loss: 2.0275440216064453\n",
            "Epoch: 336 - Train Loss: 1.9383212327957153 - Test Loss: 2.0420401096343994\n",
            "Epoch: 337 - Train Loss: 1.9523200988769531 - Test Loss: 2.056591749191284\n",
            "Epoch: 338 - Train Loss: 1.9663774967193604 - Test Loss: 2.071199655532837\n",
            "Epoch: 339 - Train Loss: 1.9804930686950684 - Test Loss: 2.085862636566162\n",
            "Epoch: 340 - Train Loss: 1.9946671724319458 - Test Loss: 2.100581407546997\n",
            "Epoch: 341 - Train Loss: 2.008899688720703 - Test Loss: 2.1153557300567627\n",
            "Epoch: 342 - Train Loss: 2.0231900215148926 - Test Loss: 2.13018536567688\n",
            "Epoch: 343 - Train Loss: 2.037538528442383 - Test Loss: 2.1450700759887695\n",
            "Epoch: 344 - Train Loss: 2.051945209503174 - Test Loss: 2.1600100994110107\n",
            "Epoch: 345 - Train Loss: 2.0664095878601074 - Test Loss: 2.1750054359436035\n",
            "Epoch: 346 - Train Loss: 2.080932140350342 - Test Loss: 2.1900553703308105\n",
            "Epoch: 347 - Train Loss: 2.0955123901367188 - Test Loss: 2.20516037940979\n",
            "Epoch: 348 - Train Loss: 2.1101505756378174 - Test Loss: 2.220320463180542\n",
            "Epoch: 349 - Train Loss: 2.1248459815979004 - Test Loss: 2.235535144805908\n",
            "Epoch: 350 - Train Loss: 2.139599323272705 - Test Loss: 2.2508046627044678\n",
            "Epoch: 351 - Train Loss: 2.1544103622436523 - Test Loss: 2.2661285400390625\n",
            "Epoch: 352 - Train Loss: 2.169278860092163 - Test Loss: 2.2815074920654297\n",
            "Epoch: 353 - Train Loss: 2.1842048168182373 - Test Loss: 2.296940565109253\n",
            "Epoch: 354 - Train Loss: 2.199187994003296 - Test Loss: 2.3124284744262695\n",
            "Epoch: 355 - Train Loss: 2.214229106903076 - Test Loss: 2.327970504760742\n",
            "Epoch: 356 - Train Loss: 2.2293272018432617 - Test Loss: 2.343567132949829\n",
            "Epoch: 357 - Train Loss: 2.2444825172424316 - Test Loss: 2.359218120574951\n",
            "Epoch: 358 - Train Loss: 2.259695291519165 - Test Loss: 2.37492299079895\n",
            "Epoch: 359 - Train Loss: 2.274965286254883 - Test Loss: 2.3906822204589844\n",
            "Epoch: 360 - Train Loss: 2.290292739868164 - Test Loss: 2.406496047973633\n",
            "Epoch: 361 - Train Loss: 2.3056766986846924 - Test Loss: 2.422363519668579\n",
            "Epoch: 362 - Train Loss: 2.3211183547973633 - Test Loss: 2.4382851123809814\n",
            "Epoch: 363 - Train Loss: 2.3366169929504395 - Test Loss: 2.454261064529419\n",
            "Epoch: 364 - Train Loss: 2.352172613143921 - Test Loss: 2.4702906608581543\n",
            "Epoch: 365 - Train Loss: 2.3677854537963867 - Test Loss: 2.4863741397857666\n",
            "Epoch: 366 - Train Loss: 2.383455276489258 - Test Loss: 2.502511978149414\n",
            "Epoch: 367 - Train Loss: 2.399182081222534 - Test Loss: 2.5187034606933594\n",
            "Epoch: 368 - Train Loss: 2.414966106414795 - Test Loss: 2.5349490642547607\n",
            "Epoch: 369 - Train Loss: 2.4308066368103027 - Test Loss: 2.55124831199646\n",
            "Epoch: 370 - Train Loss: 2.446704626083374 - Test Loss: 2.567601442337036\n",
            "Epoch: 371 - Train Loss: 2.4626593589782715 - Test Loss: 2.58400821685791\n",
            "Epoch: 372 - Train Loss: 2.478671073913574 - Test Loss: 2.600468873977661\n",
            "Epoch: 373 - Train Loss: 2.4947397708892822 - Test Loss: 2.616983413696289\n",
            "Epoch: 374 - Train Loss: 2.5108654499053955 - Test Loss: 2.6335513591766357\n",
            "Epoch: 375 - Train Loss: 2.527047872543335 - Test Loss: 2.6501734256744385\n",
            "Epoch: 376 - Train Loss: 2.5432872772216797 - Test Loss: 2.66684889793396\n",
            "Epoch: 377 - Train Loss: 2.5595836639404297 - Test Loss: 2.6835782527923584\n",
            "Epoch: 378 - Train Loss: 2.575936794281006 - Test Loss: 2.7003610134124756\n",
            "Epoch: 379 - Train Loss: 2.5923471450805664 - Test Loss: 2.7171974182128906\n",
            "Epoch: 380 - Train Loss: 2.608814001083374 - Test Loss: 2.7340877056121826\n",
            "Epoch: 381 - Train Loss: 2.625338077545166 - Test Loss: 2.7510316371917725\n",
            "Epoch: 382 - Train Loss: 2.6419191360473633 - Test Loss: 2.76802921295166\n",
            "Epoch: 383 - Train Loss: 2.6585566997528076 - Test Loss: 2.7850804328918457\n",
            "Epoch: 384 - Train Loss: 2.6752512454986572 - Test Loss: 2.802184820175171\n",
            "Epoch: 385 - Train Loss: 2.692003011703491 - Test Loss: 2.8193435668945312\n",
            "Epoch: 386 - Train Loss: 2.7088115215301514 - Test Loss: 2.836555242538452\n",
            "Epoch: 387 - Train Loss: 2.725677251815796 - Test Loss: 2.85382080078125\n",
            "Epoch: 388 - Train Loss: 2.7425994873046875 - Test Loss: 2.8711397647857666\n",
            "Epoch: 389 - Train Loss: 2.7595789432525635 - Test Loss: 2.8885128498077393\n",
            "Epoch: 390 - Train Loss: 2.7766153812408447 - Test Loss: 2.9059391021728516\n",
            "Epoch: 391 - Train Loss: 2.7937088012695312 - Test Loss: 2.9234189987182617\n",
            "Epoch: 392 - Train Loss: 2.810859203338623 - Test Loss: 2.9409525394439697\n",
            "Epoch: 393 - Train Loss: 2.828066825866699 - Test Loss: 2.9585399627685547\n",
            "Epoch: 394 - Train Loss: 2.8453309535980225 - Test Loss: 2.9761807918548584\n",
            "Epoch: 395 - Train Loss: 2.8626527786254883 - Test Loss: 2.99387526512146\n",
            "Epoch: 396 - Train Loss: 2.880031108856201 - Test Loss: 3.0116231441497803\n",
            "Epoch: 397 - Train Loss: 2.8974668979644775 - Test Loss: 3.0294251441955566\n",
            "Epoch: 398 - Train Loss: 2.9149599075317383 - Test Loss: 3.047280788421631\n",
            "Epoch: 399 - Train Loss: 2.932509660720825 - Test Loss: 3.065189838409424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "JXLaWPTzVz29",
        "outputId": "b4814f9d-85c9-493a-dfa6-a30e42226950"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.plot(train_mses)\r\n",
        "plt.plot(test_mses)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f52821f7828>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPB4vn0pV8ay",
        "outputId": "a281325b-7716-42fa-906f-bc3106152fa5"
      },
      "source": [
        "masked_mse(tf.matmul(user_emb, item_emb, transpose_b=True), train_urm, mask,0.,0.,0.,0.)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=2.9325097>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obl-d42vV-II",
        "outputId": "9eca055f-37d3-48c2-b184-5b964887e68a"
      },
      "source": [
        "test_mask = tf.not_equal(test_urm, 0.)\r\n",
        "masked_mse(tf.matmul(user_emb, item_emb, transpose_b=True), test_urm, test_mask, 0.,0.,0.,0.)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=3.0651898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Dxb6QsV_lZ",
        "outputId": "0064564b-7e58-4530-9b7b-e8c7463b0f1f"
      },
      "source": [
        "tf.boolean_mask(tf.matmul(user_emb, item_emb, transpose_b=True), test_mask)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10084,), dtype=float32, numpy=\n",
              "array([2.4166148, 2.732738 , 1.5094749, ..., 1.6962771, 1.8273033,\n",
              "       2.786995 ], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JBaKUueWBgw",
        "outputId": "0b25aa89-219b-4abe-e280-e13976842f92"
      },
      "source": [
        "tf.boolean_mask(test_urm, test_mask)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10084,), dtype=float32, numpy=\n",
              "array([4.462924 , 4.169282 , 3.7919862, ..., 4.0944376, 3.444671 ,\n",
              "       4.076992 ], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bL6murBWCqX"
      },
      "source": [
        "ratings = pd.read_csv(CUR_DIR + '/ml-latest-small/ratings.csv')\r\n",
        "\r\n",
        "C = 3\r\n",
        "total_mean = ratings.rating.mean()\r\n",
        "ratings['normalized_rating'] = ratings.rating - total_mean\r\n",
        "\r\n",
        "b_item = ratings.groupby('movieId').normalized_rating.sum() / (ratings.groupby('movieId').userId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_item, columns=['b_item']), left_on='movieId', right_index=True, how='inner')\r\n",
        "ratings['norm_item_rating'] = ratings.normalized_rating - ratings.b_item\r\n",
        "\r\n",
        "b_user = ratings.groupby('userId').norm_item_rating.sum() / (ratings.groupby('userId').movieId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_user, columns=['b_user']), left_on='userId', right_index=True, how='inner')\r\n",
        "\r\n",
        "ratings['normr_user_item_rating'] = total_mean + ratings.b_item + ratings.b_user"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzFdif_-WE3O"
      },
      "source": [
        "@tf.function\r\n",
        "def masked_mse(y_pred, y_true, mask, weights_1, lamb1, weights_2, lamb2):\r\n",
        "    y_pred_masked = tf.boolean_mask(y_pred, mask)\r\n",
        "    y_true_masked = tf.boolean_mask(y_true, mask)\r\n",
        "    return tf.losses.mean_squared_error(y_true_masked, y_pred_masked) + lamb1 * tf.norm(weights_1) + lamb2 * tf.norm(weights_2)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a90fMP_WGct"
      },
      "source": [
        "mask = tf.not_equal(urm, tf.constant(0., dtype=tf.float32))\r\n",
        "non_zero_rating_ixs = tf.where(mask)\r\n",
        "non_zero_ratings = tf.gather_nd(urm, non_zero_rating_ixs)\r\n",
        "\r\n",
        "split = 0.90\r\n",
        "split_ix = int(split * non_zero_rating_ixs.shape[0])\r\n",
        "\r\n",
        "non_zero_rating_ixs_shuffled = tf.random.shuffle(tf.range(non_zero_ratings.shape))\r\n",
        "\r\n",
        "train_urm_ratings = tf.gather(non_zero_ratings, non_zero_rating_ixs_shuffled[:split_ix])\r\n",
        "train_urm_ratings_ixs = tf.gather(non_zero_rating_ixs, non_zero_rating_ixs_shuffled[:split_ix])\r\n",
        "test_urm_ratings = tf.gather(non_zero_ratings, non_zero_rating_ixs_shuffled[split_ix:])\r\n",
        "test_urm_ratings_ixs = tf.gather(non_zero_rating_ixs, non_zero_rating_ixs_shuffled[split_ix:])\r\n",
        "\r\n",
        "train_urm = tf.scatter_nd(train_urm_ratings_ixs, train_urm_ratings, urm.shape)\r\n",
        "test_urm = tf.scatter_nd(test_urm_ratings_ixs, test_urm_ratings, urm.shape)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll6-U61KWKsE",
        "outputId": "99690fc7-a3ee-4382-a5a4-1482f88d22ee"
      },
      "source": [
        "non_zero_rating_ixs"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100836, 2), dtype=int64, numpy=\n",
              "array([[   0,    0],\n",
              "       [   0,    2],\n",
              "       [   0,    5],\n",
              "       ...,\n",
              "       [ 609, 9444],\n",
              "       [ 609, 9445],\n",
              "       [ 609, 9485]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpY9jYGWWMId"
      },
      "source": [
        "emb_dim = 30\r\n",
        "user_emb = tf.Variable(tf.random.uniform(shape=(urm.shape[0],emb_dim)), trainable=True)\r\n",
        "item_emb = tf.Variable(tf.random.uniform(shape=(urm.shape[1],emb_dim)), trainable=True)\r\n",
        "user_bias = tf.Variable(tf.random.uniform(shape=(urm.shape[0],1)), trainable=True)\r\n",
        "item_bias = tf.Variable(tf.random.uniform(shape=(1, urm.shape[1])), trainable=True)\r\n",
        "mean_rating = tf.Variable(tf.random.uniform(shape=(1,1)), trainable=True)\r\n",
        "\r\n",
        "mask = tf.not_equal(train_urm, tf.constant(0, dtype=tf.float32))\r\n",
        "test_mask = tf.not_equal(test_urm, 0.)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U43pQ57sWPJ8",
        "outputId": "93f877b1-d188-463f-ac83-b3158907164e"
      },
      "source": [
        "epochs = 3000\r\n",
        "opti = tf.optimizers.Adam()\r\n",
        "loss = masked_mse\r\n",
        "train_mses = []\r\n",
        "test_mses = []\r\n",
        "\r\n",
        "for e in range(epochs):      \r\n",
        "\r\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as gt1:\r\n",
        "\r\n",
        "    gt1.watch(item_emb)\r\n",
        "    gt1.watch(user_emb)\r\n",
        "    gt1.watch(item_bias)\r\n",
        "    gt1.watch(user_bias)\r\n",
        "    gt1.watch(mean_rating)\r\n",
        "\r\n",
        "    global_effects = user_bias + item_bias + mean_rating\r\n",
        "    preds = (tf.matmul(user_emb, item_emb, transpose_b=True)) + global_effects\r\n",
        "    preds = tf.clip_by_value(preds, 0., 5.)\r\n",
        "\r\n",
        "    mse = loss(preds, train_urm, mask, user_emb, 0.5, item_emb, 0.6)\r\n",
        "    \r\n",
        "    grads = gt1.gradient([mse], [user_emb, item_emb, item_bias, user_bias, mean_rating])\r\n",
        "    opti.apply_gradients(grads_and_vars=zip(grads, [user_emb, item_emb, item_bias, user_bias, mean_rating])) \r\n",
        "\r\n",
        "    test_mses.append(masked_mse(tf.matmul(user_emb, item_emb, transpose_b=True) + global_effects, test_urm, test_mask, 0.,0.,0.,0.))\r\n",
        "    train_mses.append(masked_mse(tf.matmul(user_emb, item_emb, transpose_b=True) + global_effects, train_urm, mask, 0.,0.,0.,0.))\r\n",
        "    print(f'Epoch: {e} - Train Loss: {train_mses[-1]} - Test Loss: {test_mses[-1]}')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 - Train Loss: 34.83883285522461 - Test Loss: 34.7446403503418\n",
            "Epoch: 1 - Train Loss: 34.482784271240234 - Test Loss: 34.38923645019531\n",
            "Epoch: 2 - Train Loss: 34.129051208496094 - Test Loss: 34.03615188598633\n",
            "Epoch: 3 - Train Loss: 33.778133392333984 - Test Loss: 33.685874938964844\n",
            "Epoch: 4 - Train Loss: 33.42972946166992 - Test Loss: 33.33810806274414\n",
            "Epoch: 5 - Train Loss: 33.08396911621094 - Test Loss: 32.99298858642578\n",
            "Epoch: 6 - Train Loss: 32.74095916748047 - Test Loss: 32.65060806274414\n",
            "Epoch: 7 - Train Loss: 32.40068435668945 - Test Loss: 32.31096649169922\n",
            "Epoch: 8 - Train Loss: 32.06315612792969 - Test Loss: 31.974061965942383\n",
            "Epoch: 9 - Train Loss: 31.72873878479004 - Test Loss: 31.640277862548828\n",
            "Epoch: 10 - Train Loss: 31.396987915039062 - Test Loss: 31.309162139892578\n",
            "Epoch: 11 - Train Loss: 31.067955017089844 - Test Loss: 30.980762481689453\n",
            "Epoch: 12 - Train Loss: 30.741579055786133 - Test Loss: 30.655025482177734\n",
            "Epoch: 13 - Train Loss: 30.417842864990234 - Test Loss: 30.33193016052246\n",
            "Epoch: 14 - Train Loss: 30.09674072265625 - Test Loss: 30.0114688873291\n",
            "Epoch: 15 - Train Loss: 29.778268814086914 - Test Loss: 29.693649291992188\n",
            "Epoch: 16 - Train Loss: 29.462419509887695 - Test Loss: 29.378448486328125\n",
            "Epoch: 17 - Train Loss: 29.149187088012695 - Test Loss: 29.065868377685547\n",
            "Epoch: 18 - Train Loss: 28.838497161865234 - Test Loss: 28.75583267211914\n",
            "Epoch: 19 - Train Loss: 28.530376434326172 - Test Loss: 28.4483585357666\n",
            "Epoch: 20 - Train Loss: 28.22482681274414 - Test Loss: 28.143461227416992\n",
            "Epoch: 21 - Train Loss: 27.92179298400879 - Test Loss: 27.84107208251953\n",
            "Epoch: 22 - Train Loss: 27.621126174926758 - Test Loss: 27.541040420532227\n",
            "Epoch: 23 - Train Loss: 27.322891235351562 - Test Loss: 27.243431091308594\n",
            "Epoch: 24 - Train Loss: 27.027053833007812 - Test Loss: 26.948225021362305\n",
            "Epoch: 25 - Train Loss: 26.7336483001709 - Test Loss: 26.655437469482422\n",
            "Epoch: 26 - Train Loss: 26.442672729492188 - Test Loss: 26.365089416503906\n",
            "Epoch: 27 - Train Loss: 26.154104232788086 - Test Loss: 26.0771484375\n",
            "Epoch: 28 - Train Loss: 25.86789894104004 - Test Loss: 25.791566848754883\n",
            "Epoch: 29 - Train Loss: 25.584075927734375 - Test Loss: 25.50836181640625\n",
            "Epoch: 30 - Train Loss: 25.302608489990234 - Test Loss: 25.227510452270508\n",
            "Epoch: 31 - Train Loss: 25.02349281311035 - Test Loss: 24.948999404907227\n",
            "Epoch: 32 - Train Loss: 24.74671173095703 - Test Loss: 24.67281723022461\n",
            "Epoch: 33 - Train Loss: 24.472211837768555 - Test Loss: 24.398902893066406\n",
            "Epoch: 34 - Train Loss: 24.199962615966797 - Test Loss: 24.127227783203125\n",
            "Epoch: 35 - Train Loss: 23.929868698120117 - Test Loss: 23.857707977294922\n",
            "Epoch: 36 - Train Loss: 23.66194725036621 - Test Loss: 23.590354919433594\n",
            "Epoch: 37 - Train Loss: 23.396135330200195 - Test Loss: 23.325105667114258\n",
            "Epoch: 38 - Train Loss: 23.132442474365234 - Test Loss: 23.061973571777344\n",
            "Epoch: 39 - Train Loss: 22.870885848999023 - Test Loss: 22.80095863342285\n",
            "Epoch: 40 - Train Loss: 22.611417770385742 - Test Loss: 22.54203224182129\n",
            "Epoch: 41 - Train Loss: 22.35405921936035 - Test Loss: 22.28521728515625\n",
            "Epoch: 42 - Train Loss: 22.09877586364746 - Test Loss: 22.030479431152344\n",
            "Epoch: 43 - Train Loss: 21.84543800354004 - Test Loss: 21.777694702148438\n",
            "Epoch: 44 - Train Loss: 21.59404182434082 - Test Loss: 21.526844024658203\n",
            "Epoch: 45 - Train Loss: 21.344558715820312 - Test Loss: 21.277912139892578\n",
            "Epoch: 46 - Train Loss: 21.096973419189453 - Test Loss: 21.030879974365234\n",
            "Epoch: 47 - Train Loss: 20.851242065429688 - Test Loss: 20.785701751708984\n",
            "Epoch: 48 - Train Loss: 20.607406616210938 - Test Loss: 20.54241180419922\n",
            "Epoch: 49 - Train Loss: 20.365434646606445 - Test Loss: 20.30097770690918\n",
            "Epoch: 50 - Train Loss: 20.125242233276367 - Test Loss: 20.06131362915039\n",
            "Epoch: 51 - Train Loss: 19.886533737182617 - Test Loss: 19.823110580444336\n",
            "Epoch: 52 - Train Loss: 19.649471282958984 - Test Loss: 19.5865421295166\n",
            "Epoch: 53 - Train Loss: 19.41400718688965 - Test Loss: 19.351533889770508\n",
            "Epoch: 54 - Train Loss: 19.18025016784668 - Test Loss: 19.11822509765625\n",
            "Epoch: 55 - Train Loss: 18.948223114013672 - Test Loss: 18.886646270751953\n",
            "Epoch: 56 - Train Loss: 18.717899322509766 - Test Loss: 18.65675926208496\n",
            "Epoch: 57 - Train Loss: 18.489334106445312 - Test Loss: 18.42860984802246\n",
            "Epoch: 58 - Train Loss: 18.262479782104492 - Test Loss: 18.20216178894043\n",
            "Epoch: 59 - Train Loss: 18.037275314331055 - Test Loss: 17.977354049682617\n",
            "Epoch: 60 - Train Loss: 17.813703536987305 - Test Loss: 17.754167556762695\n",
            "Epoch: 61 - Train Loss: 17.5917911529541 - Test Loss: 17.532634735107422\n",
            "Epoch: 62 - Train Loss: 17.371490478515625 - Test Loss: 17.312713623046875\n",
            "Epoch: 63 - Train Loss: 17.152807235717773 - Test Loss: 17.094409942626953\n",
            "Epoch: 64 - Train Loss: 16.935762405395508 - Test Loss: 16.87773895263672\n",
            "Epoch: 65 - Train Loss: 16.720258712768555 - Test Loss: 16.6626033782959\n",
            "Epoch: 66 - Train Loss: 16.506258010864258 - Test Loss: 16.4489688873291\n",
            "Epoch: 67 - Train Loss: 16.29368019104004 - Test Loss: 16.236772537231445\n",
            "Epoch: 68 - Train Loss: 16.082489013671875 - Test Loss: 16.025968551635742\n",
            "Epoch: 69 - Train Loss: 15.8726806640625 - Test Loss: 15.816560745239258\n",
            "Epoch: 70 - Train Loss: 15.664304733276367 - Test Loss: 15.60858154296875\n",
            "Epoch: 71 - Train Loss: 15.457350730895996 - Test Loss: 15.402026176452637\n",
            "Epoch: 72 - Train Loss: 15.25180721282959 - Test Loss: 15.196867942810059\n",
            "Epoch: 73 - Train Loss: 15.047616004943848 - Test Loss: 14.993061065673828\n",
            "Epoch: 74 - Train Loss: 14.844844818115234 - Test Loss: 14.790664672851562\n",
            "Epoch: 75 - Train Loss: 14.643462181091309 - Test Loss: 14.589627265930176\n",
            "Epoch: 76 - Train Loss: 14.443520545959473 - Test Loss: 14.389992713928223\n",
            "Epoch: 77 - Train Loss: 14.245052337646484 - Test Loss: 14.191807746887207\n",
            "Epoch: 78 - Train Loss: 14.048056602478027 - Test Loss: 13.995079040527344\n",
            "Epoch: 79 - Train Loss: 13.852505683898926 - Test Loss: 13.79979133605957\n",
            "Epoch: 80 - Train Loss: 13.65842342376709 - Test Loss: 13.605966567993164\n",
            "Epoch: 81 - Train Loss: 13.465733528137207 - Test Loss: 13.413516998291016\n",
            "Epoch: 82 - Train Loss: 13.274415016174316 - Test Loss: 13.222423553466797\n",
            "Epoch: 83 - Train Loss: 13.084474563598633 - Test Loss: 13.03269100189209\n",
            "Epoch: 84 - Train Loss: 12.895873069763184 - Test Loss: 12.844273567199707\n",
            "Epoch: 85 - Train Loss: 12.708586692810059 - Test Loss: 12.65715217590332\n",
            "Epoch: 86 - Train Loss: 12.522622108459473 - Test Loss: 12.471343994140625\n",
            "Epoch: 87 - Train Loss: 12.338013648986816 - Test Loss: 12.28690242767334\n",
            "Epoch: 88 - Train Loss: 12.15476131439209 - Test Loss: 12.103821754455566\n",
            "Epoch: 89 - Train Loss: 11.972796440124512 - Test Loss: 11.92203426361084\n",
            "Epoch: 90 - Train Loss: 11.792044639587402 - Test Loss: 11.741467475891113\n",
            "Epoch: 91 - Train Loss: 11.612505912780762 - Test Loss: 11.56212043762207\n",
            "Epoch: 92 - Train Loss: 11.43424129486084 - Test Loss: 11.384037017822266\n",
            "Epoch: 93 - Train Loss: 11.257231712341309 - Test Loss: 11.20720386505127\n",
            "Epoch: 94 - Train Loss: 11.081482887268066 - Test Loss: 11.03162956237793\n",
            "Epoch: 95 - Train Loss: 10.906988143920898 - Test Loss: 10.857316017150879\n",
            "Epoch: 96 - Train Loss: 10.733778953552246 - Test Loss: 10.684283256530762\n",
            "Epoch: 97 - Train Loss: 10.561860084533691 - Test Loss: 10.512530326843262\n",
            "Epoch: 98 - Train Loss: 10.39124870300293 - Test Loss: 10.342081069946289\n",
            "Epoch: 99 - Train Loss: 10.221906661987305 - Test Loss: 10.172904968261719\n",
            "Epoch: 100 - Train Loss: 10.053873062133789 - Test Loss: 10.005058288574219\n",
            "Epoch: 101 - Train Loss: 9.887190818786621 - Test Loss: 9.838553428649902\n",
            "Epoch: 102 - Train Loss: 9.72179126739502 - Test Loss: 9.67332649230957\n",
            "Epoch: 103 - Train Loss: 9.557703018188477 - Test Loss: 9.509404182434082\n",
            "Epoch: 104 - Train Loss: 9.394968032836914 - Test Loss: 9.346840858459473\n",
            "Epoch: 105 - Train Loss: 9.233577728271484 - Test Loss: 9.185627937316895\n",
            "Epoch: 106 - Train Loss: 9.073528289794922 - Test Loss: 9.02575969696045\n",
            "Epoch: 107 - Train Loss: 8.914813041687012 - Test Loss: 8.86722469329834\n",
            "Epoch: 108 - Train Loss: 8.757428169250488 - Test Loss: 8.710010528564453\n",
            "Epoch: 109 - Train Loss: 8.601370811462402 - Test Loss: 8.554132461547852\n",
            "Epoch: 110 - Train Loss: 8.44668197631836 - Test Loss: 8.399629592895508\n",
            "Epoch: 111 - Train Loss: 8.293363571166992 - Test Loss: 8.246500015258789\n",
            "Epoch: 112 - Train Loss: 8.14144229888916 - Test Loss: 8.094791412353516\n",
            "Epoch: 113 - Train Loss: 7.990915298461914 - Test Loss: 7.944491386413574\n",
            "Epoch: 114 - Train Loss: 7.841794013977051 - Test Loss: 7.795602798461914\n",
            "Epoch: 115 - Train Loss: 7.694004535675049 - Test Loss: 7.648052215576172\n",
            "Epoch: 116 - Train Loss: 7.547634124755859 - Test Loss: 7.5019330978393555\n",
            "Epoch: 117 - Train Loss: 7.402740478515625 - Test Loss: 7.357303142547607\n",
            "Epoch: 118 - Train Loss: 7.259368419647217 - Test Loss: 7.2142014503479\n",
            "Epoch: 119 - Train Loss: 7.117517471313477 - Test Loss: 7.072615146636963\n",
            "Epoch: 120 - Train Loss: 6.9771904945373535 - Test Loss: 6.932554721832275\n",
            "Epoch: 121 - Train Loss: 6.83840799331665 - Test Loss: 6.7940497398376465\n",
            "Epoch: 122 - Train Loss: 6.701157569885254 - Test Loss: 6.65708065032959\n",
            "Epoch: 123 - Train Loss: 6.565371036529541 - Test Loss: 6.5215959548950195\n",
            "Epoch: 124 - Train Loss: 6.43113374710083 - Test Loss: 6.387663841247559\n",
            "Epoch: 125 - Train Loss: 6.298485279083252 - Test Loss: 6.255331039428711\n",
            "Epoch: 126 - Train Loss: 6.167455196380615 - Test Loss: 6.124622344970703\n",
            "Epoch: 127 - Train Loss: 6.0380425453186035 - Test Loss: 5.995543003082275\n",
            "Epoch: 128 - Train Loss: 5.910236358642578 - Test Loss: 5.86807918548584\n",
            "Epoch: 129 - Train Loss: 5.784031391143799 - Test Loss: 5.742227554321289\n",
            "Epoch: 130 - Train Loss: 5.659428596496582 - Test Loss: 5.6179938316345215\n",
            "Epoch: 131 - Train Loss: 5.536467552185059 - Test Loss: 5.495415687561035\n",
            "Epoch: 132 - Train Loss: 5.415167808532715 - Test Loss: 5.374510765075684\n",
            "Epoch: 133 - Train Loss: 5.295534610748291 - Test Loss: 5.255286693572998\n",
            "Epoch: 134 - Train Loss: 5.177567481994629 - Test Loss: 5.137734889984131\n",
            "Epoch: 135 - Train Loss: 5.061297416687012 - Test Loss: 5.021881103515625\n",
            "Epoch: 136 - Train Loss: 4.946706295013428 - Test Loss: 4.907707214355469\n",
            "Epoch: 137 - Train Loss: 4.833795070648193 - Test Loss: 4.795220851898193\n",
            "Epoch: 138 - Train Loss: 4.722551345825195 - Test Loss: 4.684412002563477\n",
            "Epoch: 139 - Train Loss: 4.612974643707275 - Test Loss: 4.57526969909668\n",
            "Epoch: 140 - Train Loss: 4.505091667175293 - Test Loss: 4.467825412750244\n",
            "Epoch: 141 - Train Loss: 4.398919105529785 - Test Loss: 4.362094402313232\n",
            "Epoch: 142 - Train Loss: 4.294460296630859 - Test Loss: 4.258084297180176\n",
            "Epoch: 143 - Train Loss: 4.191731929779053 - Test Loss: 4.155808448791504\n",
            "Epoch: 144 - Train Loss: 4.090743541717529 - Test Loss: 4.055276393890381\n",
            "Epoch: 145 - Train Loss: 3.9914891719818115 - Test Loss: 3.9564831256866455\n",
            "Epoch: 146 - Train Loss: 3.8939671516418457 - Test Loss: 3.859431505203247\n",
            "Epoch: 147 - Train Loss: 3.7981624603271484 - Test Loss: 3.764098644256592\n",
            "Epoch: 148 - Train Loss: 3.7040815353393555 - Test Loss: 3.6704912185668945\n",
            "Epoch: 149 - Train Loss: 3.611713171005249 - Test Loss: 3.578603744506836\n",
            "Epoch: 150 - Train Loss: 3.5210447311401367 - Test Loss: 3.4884214401245117\n",
            "Epoch: 151 - Train Loss: 3.432084560394287 - Test Loss: 3.399949789047241\n",
            "Epoch: 152 - Train Loss: 3.344848394393921 - Test Loss: 3.3132050037384033\n",
            "Epoch: 153 - Train Loss: 3.259345531463623 - Test Loss: 3.2282068729400635\n",
            "Epoch: 154 - Train Loss: 3.1755857467651367 - Test Loss: 3.1449642181396484\n",
            "Epoch: 155 - Train Loss: 3.0935676097869873 - Test Loss: 3.0634727478027344\n",
            "Epoch: 156 - Train Loss: 3.0132858753204346 - Test Loss: 2.983726739883423\n",
            "Epoch: 157 - Train Loss: 2.9347293376922607 - Test Loss: 2.9057130813598633\n",
            "Epoch: 158 - Train Loss: 2.857896089553833 - Test Loss: 2.8294286727905273\n",
            "Epoch: 159 - Train Loss: 2.7827727794647217 - Test Loss: 2.7548584938049316\n",
            "Epoch: 160 - Train Loss: 2.7093472480773926 - Test Loss: 2.6819894313812256\n",
            "Epoch: 161 - Train Loss: 2.6376097202301025 - Test Loss: 2.6108131408691406\n",
            "Epoch: 162 - Train Loss: 2.5675511360168457 - Test Loss: 2.541318655014038\n",
            "Epoch: 163 - Train Loss: 2.4991581439971924 - Test Loss: 2.4734926223754883\n",
            "Epoch: 164 - Train Loss: 2.4324076175689697 - Test Loss: 2.4073104858398438\n",
            "Epoch: 165 - Train Loss: 2.3672757148742676 - Test Loss: 2.3427491188049316\n",
            "Epoch: 166 - Train Loss: 2.3037447929382324 - Test Loss: 2.279789686203003\n",
            "Epoch: 167 - Train Loss: 2.2417969703674316 - Test Loss: 2.218414068222046\n",
            "Epoch: 168 - Train Loss: 2.181417942047119 - Test Loss: 2.158601999282837\n",
            "Epoch: 169 - Train Loss: 2.1225996017456055 - Test Loss: 2.1003458499908447\n",
            "Epoch: 170 - Train Loss: 2.0653228759765625 - Test Loss: 2.043627977371216\n",
            "Epoch: 171 - Train Loss: 2.0095646381378174 - Test Loss: 1.9884257316589355\n",
            "Epoch: 172 - Train Loss: 1.9552973508834839 - Test Loss: 1.9347116947174072\n",
            "Epoch: 173 - Train Loss: 1.9025037288665771 - Test Loss: 1.8824681043624878\n",
            "Epoch: 174 - Train Loss: 1.851158618927002 - Test Loss: 1.8316706418991089\n",
            "Epoch: 175 - Train Loss: 1.8012419939041138 - Test Loss: 1.7823009490966797\n",
            "Epoch: 176 - Train Loss: 1.7527323961257935 - Test Loss: 1.7343316078186035\n",
            "Epoch: 177 - Train Loss: 1.7056056261062622 - Test Loss: 1.687739372253418\n",
            "Epoch: 178 - Train Loss: 1.6598397493362427 - Test Loss: 1.6425014734268188\n",
            "Epoch: 179 - Train Loss: 1.6154134273529053 - Test Loss: 1.5985984802246094\n",
            "Epoch: 180 - Train Loss: 1.5723050832748413 - Test Loss: 1.5560065507888794\n",
            "Epoch: 181 - Train Loss: 1.5304869413375854 - Test Loss: 1.5146980285644531\n",
            "Epoch: 182 - Train Loss: 1.4899306297302246 - Test Loss: 1.4746445417404175\n",
            "Epoch: 183 - Train Loss: 1.4506146907806396 - Test Loss: 1.4358221292495728\n",
            "Epoch: 184 - Train Loss: 1.4125126600265503 - Test Loss: 1.3982067108154297\n",
            "Epoch: 185 - Train Loss: 1.3755967617034912 - Test Loss: 1.361768126487732\n",
            "Epoch: 186 - Train Loss: 1.3398411273956299 - Test Loss: 1.326478362083435\n",
            "Epoch: 187 - Train Loss: 1.3052211999893188 - Test Loss: 1.2923123836517334\n",
            "Epoch: 188 - Train Loss: 1.2717082500457764 - Test Loss: 1.2592437267303467\n",
            "Epoch: 189 - Train Loss: 1.239274501800537 - Test Loss: 1.2272454500198364\n",
            "Epoch: 190 - Train Loss: 1.2078975439071655 - Test Loss: 1.1962950229644775\n",
            "Epoch: 191 - Train Loss: 1.1775511503219604 - Test Loss: 1.1663674116134644\n",
            "Epoch: 192 - Train Loss: 1.1482082605361938 - Test Loss: 1.1374343633651733\n",
            "Epoch: 193 - Train Loss: 1.1198437213897705 - Test Loss: 1.1094708442687988\n",
            "Epoch: 194 - Train Loss: 1.0924338102340698 - Test Loss: 1.0824552774429321\n",
            "Epoch: 195 - Train Loss: 1.06595778465271 - Test Loss: 1.0563676357269287\n",
            "Epoch: 196 - Train Loss: 1.0403943061828613 - Test Loss: 1.0311874151229858\n",
            "Epoch: 197 - Train Loss: 1.0157161951065063 - Test Loss: 1.0068880319595337\n",
            "Epoch: 198 - Train Loss: 0.9918984174728394 - Test Loss: 0.9834432005882263\n",
            "Epoch: 199 - Train Loss: 0.9689154624938965 - Test Loss: 0.9608277082443237\n",
            "Epoch: 200 - Train Loss: 0.9467399716377258 - Test Loss: 0.9390155673027039\n",
            "Epoch: 201 - Train Loss: 0.9253460168838501 - Test Loss: 0.9179820418357849\n",
            "Epoch: 202 - Train Loss: 0.9047109484672546 - Test Loss: 0.8977039456367493\n",
            "Epoch: 203 - Train Loss: 0.8848092555999756 - Test Loss: 0.8781566619873047\n",
            "Epoch: 204 - Train Loss: 0.8656131625175476 - Test Loss: 0.8593130111694336\n",
            "Epoch: 205 - Train Loss: 0.8470973968505859 - Test Loss: 0.8411468863487244\n",
            "Epoch: 206 - Train Loss: 0.8292377591133118 - Test Loss: 0.8236342072486877\n",
            "Epoch: 207 - Train Loss: 0.81201171875 - Test Loss: 0.8067525029182434\n",
            "Epoch: 208 - Train Loss: 0.7953994870185852 - Test Loss: 0.7904821634292603\n",
            "Epoch: 209 - Train Loss: 0.779380738735199 - Test Loss: 0.7748034000396729\n",
            "Epoch: 210 - Train Loss: 0.7639368772506714 - Test Loss: 0.7596969604492188\n",
            "Epoch: 211 - Train Loss: 0.749047040939331 - Test Loss: 0.7451425194740295\n",
            "Epoch: 212 - Train Loss: 0.7346917986869812 - Test Loss: 0.7311195731163025\n",
            "Epoch: 213 - Train Loss: 0.7208513021469116 - Test Loss: 0.7176089286804199\n",
            "Epoch: 214 - Train Loss: 0.7075088620185852 - Test Loss: 0.7045940160751343\n",
            "Epoch: 215 - Train Loss: 0.6946460604667664 - Test Loss: 0.6920567750930786\n",
            "Epoch: 216 - Train Loss: 0.682248055934906 - Test Loss: 0.6799821853637695\n",
            "Epoch: 217 - Train Loss: 0.67029869556427 - Test Loss: 0.6683540940284729\n",
            "Epoch: 218 - Train Loss: 0.6587833166122437 - Test Loss: 0.657157301902771\n",
            "Epoch: 219 - Train Loss: 0.6476873159408569 - Test Loss: 0.6463772654533386\n",
            "Epoch: 220 - Train Loss: 0.6369954943656921 - Test Loss: 0.635998547077179\n",
            "Epoch: 221 - Train Loss: 0.6266928911209106 - Test Loss: 0.6260058879852295\n",
            "Epoch: 222 - Train Loss: 0.6167658567428589 - Test Loss: 0.6163860559463501\n",
            "Epoch: 223 - Train Loss: 0.6072006225585938 - Test Loss: 0.6071252226829529\n",
            "Epoch: 224 - Train Loss: 0.5979833602905273 - Test Loss: 0.5982099175453186\n",
            "Epoch: 225 - Train Loss: 0.5891024470329285 - Test Loss: 0.5896286368370056\n",
            "Epoch: 226 - Train Loss: 0.5805466175079346 - Test Loss: 0.581369936466217\n",
            "Epoch: 227 - Train Loss: 0.5723053812980652 - Test Loss: 0.573422908782959\n",
            "Epoch: 228 - Train Loss: 0.5643686056137085 - Test Loss: 0.5657769441604614\n",
            "Epoch: 229 - Train Loss: 0.5567264556884766 - Test Loss: 0.5584220290184021\n",
            "Epoch: 230 - Train Loss: 0.5493675470352173 - Test Loss: 0.5513473749160767\n",
            "Epoch: 231 - Train Loss: 0.542281448841095 - Test Loss: 0.5445427298545837\n",
            "Epoch: 232 - Train Loss: 0.5354578495025635 - Test Loss: 0.5379975438117981\n",
            "Epoch: 233 - Train Loss: 0.5288878679275513 - Test Loss: 0.5317029356956482\n",
            "Epoch: 234 - Train Loss: 0.522563099861145 - Test Loss: 0.5256505012512207\n",
            "Epoch: 235 - Train Loss: 0.5164749026298523 - Test Loss: 0.5198314785957336\n",
            "Epoch: 236 - Train Loss: 0.510615348815918 - Test Loss: 0.5142379403114319\n",
            "Epoch: 237 - Train Loss: 0.5049759745597839 - Test Loss: 0.5088616013526917\n",
            "Epoch: 238 - Train Loss: 0.4995501935482025 - Test Loss: 0.5036954879760742\n",
            "Epoch: 239 - Train Loss: 0.49433010816574097 - Test Loss: 0.498731791973114\n",
            "Epoch: 240 - Train Loss: 0.48930826783180237 - Test Loss: 0.49396368861198425\n",
            "Epoch: 241 - Train Loss: 0.48447856307029724 - Test Loss: 0.48938438296318054\n",
            "Epoch: 242 - Train Loss: 0.4798341691493988 - Test Loss: 0.4849872291088104\n",
            "Epoch: 243 - Train Loss: 0.4753687381744385 - Test Loss: 0.4807661175727844\n",
            "Epoch: 244 - Train Loss: 0.47107627987861633 - Test Loss: 0.47671470046043396\n",
            "Epoch: 245 - Train Loss: 0.4669512212276459 - Test Loss: 0.47282716631889343\n",
            "Epoch: 246 - Train Loss: 0.46298837661743164 - Test Loss: 0.46909794211387634\n",
            "Epoch: 247 - Train Loss: 0.4591822326183319 - Test Loss: 0.46552154421806335\n",
            "Epoch: 248 - Train Loss: 0.45552748441696167 - Test Loss: 0.46209245920181274\n",
            "Epoch: 249 - Train Loss: 0.4520191252231598 - Test Loss: 0.45880571007728577\n",
            "Epoch: 250 - Train Loss: 0.4486522972583771 - Test Loss: 0.45565590262413025\n",
            "Epoch: 251 - Train Loss: 0.44542235136032104 - Test Loss: 0.4526383578777313\n",
            "Epoch: 252 - Train Loss: 0.4423249661922455 - Test Loss: 0.44974857568740845\n",
            "Epoch: 253 - Train Loss: 0.43935534358024597 - Test Loss: 0.44698166847229004\n",
            "Epoch: 254 - Train Loss: 0.43650922179222107 - Test Loss: 0.4443334639072418\n",
            "Epoch: 255 - Train Loss: 0.433782696723938 - Test Loss: 0.441800057888031\n",
            "Epoch: 256 - Train Loss: 0.431171715259552 - Test Loss: 0.43937748670578003\n",
            "Epoch: 257 - Train Loss: 0.42867231369018555 - Test Loss: 0.4370621144771576\n",
            "Epoch: 258 - Train Loss: 0.4262809753417969 - Test Loss: 0.4348503351211548\n",
            "Epoch: 259 - Train Loss: 0.42399415373802185 - Test Loss: 0.4327385425567627\n",
            "Epoch: 260 - Train Loss: 0.42180854082107544 - Test Loss: 0.43072372674942017\n",
            "Epoch: 261 - Train Loss: 0.41972100734710693 - Test Loss: 0.4288027584552765\n",
            "Epoch: 262 - Train Loss: 0.4177282154560089 - Test Loss: 0.42697256803512573\n",
            "Epoch: 263 - Train Loss: 0.41582727432250977 - Test Loss: 0.42523011565208435\n",
            "Epoch: 264 - Train Loss: 0.414014995098114 - Test Loss: 0.4235726594924927\n",
            "Epoch: 265 - Train Loss: 0.41228869557380676 - Test Loss: 0.42199739813804626\n",
            "Epoch: 266 - Train Loss: 0.41064557433128357 - Test Loss: 0.4205019176006317\n",
            "Epoch: 267 - Train Loss: 0.4090830981731415 - Test Loss: 0.41908350586891174\n",
            "Epoch: 268 - Train Loss: 0.4075985550880432 - Test Loss: 0.41773971915245056\n",
            "Epoch: 269 - Train Loss: 0.4061894714832306 - Test Loss: 0.41646808385849\n",
            "Epoch: 270 - Train Loss: 0.40485358238220215 - Test Loss: 0.4152663052082062\n",
            "Epoch: 271 - Train Loss: 0.40358880162239075 - Test Loss: 0.41413232684135437\n",
            "Epoch: 272 - Train Loss: 0.40239307284355164 - Test Loss: 0.413064181804657\n",
            "Epoch: 273 - Train Loss: 0.4012643098831177 - Test Loss: 0.41205963492393494\n",
            "Epoch: 274 - Train Loss: 0.4002005159854889 - Test Loss: 0.4111168384552002\n",
            "Epoch: 275 - Train Loss: 0.3991999328136444 - Test Loss: 0.41023412346839905\n",
            "Epoch: 276 - Train Loss: 0.39826059341430664 - Test Loss: 0.40940871834754944\n",
            "Epoch: 277 - Train Loss: 0.3973808288574219 - Test Loss: 0.4086395502090454\n",
            "Epoch: 278 - Train Loss: 0.39655885100364685 - Test Loss: 0.4079252779483795\n",
            "Epoch: 279 - Train Loss: 0.39579302072525024 - Test Loss: 0.40726444125175476\n",
            "Epoch: 280 - Train Loss: 0.39508193731307983 - Test Loss: 0.4066556692123413\n",
            "Epoch: 281 - Train Loss: 0.39442408084869385 - Test Loss: 0.4060976207256317\n",
            "Epoch: 282 - Train Loss: 0.3938180208206177 - Test Loss: 0.4055885672569275\n",
            "Epoch: 283 - Train Loss: 0.3932623267173767 - Test Loss: 0.40512731671333313\n",
            "Epoch: 284 - Train Loss: 0.39275550842285156 - Test Loss: 0.4047127366065979\n",
            "Epoch: 285 - Train Loss: 0.3922964632511139 - Test Loss: 0.4043436050415039\n",
            "Epoch: 286 - Train Loss: 0.3918839991092682 - Test Loss: 0.4040188193321228\n",
            "Epoch: 287 - Train Loss: 0.39151695370674133 - Test Loss: 0.40373730659484863\n",
            "Epoch: 288 - Train Loss: 0.3911941647529602 - Test Loss: 0.4034980535507202\n",
            "Epoch: 289 - Train Loss: 0.39091458916664124 - Test Loss: 0.403300017118454\n",
            "Epoch: 290 - Train Loss: 0.39067721366882324 - Test Loss: 0.40314221382141113\n",
            "Epoch: 291 - Train Loss: 0.3904811441898346 - Test Loss: 0.4030238389968872\n",
            "Epoch: 292 - Train Loss: 0.39032524824142456 - Test Loss: 0.40294376015663147\n",
            "Epoch: 293 - Train Loss: 0.3902086019515991 - Test Loss: 0.40290117263793945\n",
            "Epoch: 294 - Train Loss: 0.39013031125068665 - Test Loss: 0.4028952717781067\n",
            "Epoch: 295 - Train Loss: 0.3900894522666931 - Test Loss: 0.4029251039028168\n",
            "Epoch: 296 - Train Loss: 0.3900851905345917 - Test Loss: 0.40298992395401\n",
            "Epoch: 297 - Train Loss: 0.39011669158935547 - Test Loss: 0.4030890166759491\n",
            "Epoch: 298 - Train Loss: 0.3901832401752472 - Test Loss: 0.4032215476036072\n",
            "Epoch: 299 - Train Loss: 0.3902839124202728 - Test Loss: 0.40338629484176636\n",
            "Epoch: 300 - Train Loss: 0.39041799306869507 - Test Loss: 0.40358278155326843\n",
            "Epoch: 301 - Train Loss: 0.3905846178531647 - Test Loss: 0.4038103222846985\n",
            "Epoch: 302 - Train Loss: 0.39078304171562195 - Test Loss: 0.40406835079193115\n",
            "Epoch: 303 - Train Loss: 0.39101260900497437 - Test Loss: 0.4043561518192291\n",
            "Epoch: 304 - Train Loss: 0.391272634267807 - Test Loss: 0.4046730697154999\n",
            "Epoch: 305 - Train Loss: 0.39156240224838257 - Test Loss: 0.4050186276435852\n",
            "Epoch: 306 - Train Loss: 0.3918813467025757 - Test Loss: 0.4053920805454254\n",
            "Epoch: 307 - Train Loss: 0.3922288417816162 - Test Loss: 0.40579286217689514\n",
            "Epoch: 308 - Train Loss: 0.39260420203208923 - Test Loss: 0.40622052550315857\n",
            "Epoch: 309 - Train Loss: 0.3930068910121918 - Test Loss: 0.40667441487312317\n",
            "Epoch: 310 - Train Loss: 0.3934363126754761 - Test Loss: 0.40715402364730835\n",
            "Epoch: 311 - Train Loss: 0.3938919007778168 - Test Loss: 0.40765878558158875\n",
            "Epoch: 312 - Train Loss: 0.3943731188774109 - Test Loss: 0.40818822383880615\n",
            "Epoch: 313 - Train Loss: 0.39487937092781067 - Test Loss: 0.40874171257019043\n",
            "Epoch: 314 - Train Loss: 0.3954101800918579 - Test Loss: 0.4093188941478729\n",
            "Epoch: 315 - Train Loss: 0.39596500992774963 - Test Loss: 0.4099191725254059\n",
            "Epoch: 316 - Train Loss: 0.3965432941913605 - Test Loss: 0.4105420708656311\n",
            "Epoch: 317 - Train Loss: 0.3971446454524994 - Test Loss: 0.41118723154067993\n",
            "Epoch: 318 - Train Loss: 0.3977685272693634 - Test Loss: 0.411854088306427\n",
            "Epoch: 319 - Train Loss: 0.39841461181640625 - Test Loss: 0.4125422537326813\n",
            "Epoch: 320 - Train Loss: 0.3990822732448578 - Test Loss: 0.4132513403892517\n",
            "Epoch: 321 - Train Loss: 0.39977115392684937 - Test Loss: 0.4139808714389801\n",
            "Epoch: 322 - Train Loss: 0.40048083662986755 - Test Loss: 0.4147305190563202\n",
            "Epoch: 323 - Train Loss: 0.4012109339237213 - Test Loss: 0.41549983620643616\n",
            "Epoch: 324 - Train Loss: 0.40196099877357483 - Test Loss: 0.41628843545913696\n",
            "Epoch: 325 - Train Loss: 0.4027306139469147 - Test Loss: 0.4170958995819092\n",
            "Epoch: 326 - Train Loss: 0.4035194218158722 - Test Loss: 0.41792187094688416\n",
            "Epoch: 327 - Train Loss: 0.4043269157409668 - Test Loss: 0.41876596212387085\n",
            "Epoch: 328 - Train Loss: 0.405152827501297 - Test Loss: 0.41962775588035583\n",
            "Epoch: 329 - Train Loss: 0.40599676966667175 - Test Loss: 0.42050689458847046\n",
            "Epoch: 330 - Train Loss: 0.4068582355976105 - Test Loss: 0.42140305042266846\n",
            "Epoch: 331 - Train Loss: 0.40773701667785645 - Test Loss: 0.422315776348114\n",
            "Epoch: 332 - Train Loss: 0.4086326062679291 - Test Loss: 0.42324480414390564\n",
            "Epoch: 333 - Train Loss: 0.40954476594924927 - Test Loss: 0.4241897761821747\n",
            "Epoch: 334 - Train Loss: 0.41047313809394836 - Test Loss: 0.4251503646373749\n",
            "Epoch: 335 - Train Loss: 0.4114173352718353 - Test Loss: 0.4261262118816376\n",
            "Epoch: 336 - Train Loss: 0.4123770296573639 - Test Loss: 0.4271170496940613\n",
            "Epoch: 337 - Train Loss: 0.41335195302963257 - Test Loss: 0.42812252044677734\n",
            "Epoch: 338 - Train Loss: 0.4143417775630951 - Test Loss: 0.42914238572120667\n",
            "Epoch: 339 - Train Loss: 0.4153461456298828 - Test Loss: 0.4301762580871582\n",
            "Epoch: 340 - Train Loss: 0.41636478900909424 - Test Loss: 0.43122389912605286\n",
            "Epoch: 341 - Train Loss: 0.4173974096775055 - Test Loss: 0.43228501081466675\n",
            "Epoch: 342 - Train Loss: 0.4184436798095703 - Test Loss: 0.43335938453674316\n",
            "Epoch: 343 - Train Loss: 0.41950342059135437 - Test Loss: 0.4344465434551239\n",
            "Epoch: 344 - Train Loss: 0.42057618498802185 - Test Loss: 0.4355463981628418\n",
            "Epoch: 345 - Train Loss: 0.42166173458099365 - Test Loss: 0.4366585314273834\n",
            "Epoch: 346 - Train Loss: 0.4227598309516907 - Test Loss: 0.43778273463249207\n",
            "Epoch: 347 - Train Loss: 0.4238702356815338 - Test Loss: 0.43891870975494385\n",
            "Epoch: 348 - Train Loss: 0.42499253153800964 - Test Loss: 0.44006624817848206\n",
            "Epoch: 349 - Train Loss: 0.42612653970718384 - Test Loss: 0.4412250518798828\n",
            "Epoch: 350 - Train Loss: 0.4272719919681549 - Test Loss: 0.442394882440567\n",
            "Epoch: 351 - Train Loss: 0.42842864990234375 - Test Loss: 0.4435754418373108\n",
            "Epoch: 352 - Train Loss: 0.4295961558818817 - Test Loss: 0.44476646184921265\n",
            "Epoch: 353 - Train Loss: 0.4307743310928345 - Test Loss: 0.44596773386001587\n",
            "Epoch: 354 - Train Loss: 0.4319629669189453 - Test Loss: 0.44717904925346375\n",
            "Epoch: 355 - Train Loss: 0.43316176533699036 - Test Loss: 0.4484001398086548\n",
            "Epoch: 356 - Train Loss: 0.43437036871910095 - Test Loss: 0.4496307075023651\n",
            "Epoch: 357 - Train Loss: 0.4355887174606323 - Test Loss: 0.4508706033229828\n",
            "Epoch: 358 - Train Loss: 0.43681639432907104 - Test Loss: 0.45211952924728394\n",
            "Epoch: 359 - Train Loss: 0.43805330991744995 - Test Loss: 0.45337721705436707\n",
            "Epoch: 360 - Train Loss: 0.43929919600486755 - Test Loss: 0.454643577337265\n",
            "Epoch: 361 - Train Loss: 0.44055378437042236 - Test Loss: 0.4559182822704315\n",
            "Epoch: 362 - Train Loss: 0.44181689620018005 - Test Loss: 0.4572010636329651\n",
            "Epoch: 363 - Train Loss: 0.4430882930755615 - Test Loss: 0.45849183201789856\n",
            "Epoch: 364 - Train Loss: 0.44436773657798767 - Test Loss: 0.4597904086112976\n",
            "Epoch: 365 - Train Loss: 0.445654958486557 - Test Loss: 0.46109631657600403\n",
            "Epoch: 366 - Train Loss: 0.4469498097896576 - Test Loss: 0.46240949630737305\n",
            "Epoch: 367 - Train Loss: 0.4482520818710327 - Test Loss: 0.4637298285961151\n",
            "Epoch: 368 - Train Loss: 0.44956153631210327 - Test Loss: 0.4650568962097168\n",
            "Epoch: 369 - Train Loss: 0.45087796449661255 - Test Loss: 0.4663906693458557\n",
            "Epoch: 370 - Train Loss: 0.45220112800598145 - Test Loss: 0.4677308201789856\n",
            "Epoch: 371 - Train Loss: 0.45353081822395325 - Test Loss: 0.4690772294998169\n",
            "Epoch: 372 - Train Loss: 0.45486685633659363 - Test Loss: 0.47042953968048096\n",
            "Epoch: 373 - Train Loss: 0.4562089443206787 - Test Loss: 0.4717877507209778\n",
            "Epoch: 374 - Train Loss: 0.4575570523738861 - Test Loss: 0.4731515049934387\n",
            "Epoch: 375 - Train Loss: 0.4589109420776367 - Test Loss: 0.4745207726955414\n",
            "Epoch: 376 - Train Loss: 0.4602702558040619 - Test Loss: 0.4758951663970947\n",
            "Epoch: 377 - Train Loss: 0.46163487434387207 - Test Loss: 0.4772745370864868\n",
            "Epoch: 378 - Train Loss: 0.4630047380924225 - Test Loss: 0.47865885496139526\n",
            "Epoch: 379 - Train Loss: 0.46437951922416687 - Test Loss: 0.4800477623939514\n",
            "Epoch: 380 - Train Loss: 0.4657590985298157 - Test Loss: 0.4814411401748657\n",
            "Epoch: 381 - Train Loss: 0.4671432375907898 - Test Loss: 0.48283877968788147\n",
            "Epoch: 382 - Train Loss: 0.4685317277908325 - Test Loss: 0.48424047231674194\n",
            "Epoch: 383 - Train Loss: 0.4699244797229767 - Test Loss: 0.4856460988521576\n",
            "Epoch: 384 - Train Loss: 0.4713212251663208 - Test Loss: 0.4870555102825165\n",
            "Epoch: 385 - Train Loss: 0.4727218449115753 - Test Loss: 0.4884684085845947\n",
            "Epoch: 386 - Train Loss: 0.47412610054016113 - Test Loss: 0.4898846745491028\n",
            "Epoch: 387 - Train Loss: 0.47553393244743347 - Test Loss: 0.4913041591644287\n",
            "Epoch: 388 - Train Loss: 0.4769449830055237 - Test Loss: 0.4927266240119934\n",
            "Epoch: 389 - Train Loss: 0.47835925221443176 - Test Loss: 0.4941519498825073\n",
            "Epoch: 390 - Train Loss: 0.47977644205093384 - Test Loss: 0.4955799877643585\n",
            "Epoch: 391 - Train Loss: 0.48119643330574036 - Test Loss: 0.4970104694366455\n",
            "Epoch: 392 - Train Loss: 0.4826190173625946 - Test Loss: 0.4984433054924011\n",
            "Epoch: 393 - Train Loss: 0.4840441644191742 - Test Loss: 0.4998783767223358\n",
            "Epoch: 394 - Train Loss: 0.48547157645225525 - Test Loss: 0.5013154149055481\n",
            "Epoch: 395 - Train Loss: 0.48690110445022583 - Test Loss: 0.5027543306350708\n",
            "Epoch: 396 - Train Loss: 0.4883325695991516 - Test Loss: 0.5041949152946472\n",
            "Epoch: 397 - Train Loss: 0.48976588249206543 - Test Loss: 0.5056370496749878\n",
            "Epoch: 398 - Train Loss: 0.4912007749080658 - Test Loss: 0.5070804357528687\n",
            "Epoch: 399 - Train Loss: 0.4926372170448303 - Test Loss: 0.508525013923645\n",
            "Epoch: 400 - Train Loss: 0.4940749406814575 - Test Loss: 0.5099706649780273\n",
            "Epoch: 401 - Train Loss: 0.4955137372016907 - Test Loss: 0.5114171504974365\n",
            "Epoch: 402 - Train Loss: 0.4969536364078522 - Test Loss: 0.5128644108772278\n",
            "Epoch: 403 - Train Loss: 0.49839428067207336 - Test Loss: 0.5143120884895325\n",
            "Epoch: 404 - Train Loss: 0.4998355805873871 - Test Loss: 0.5157603025436401\n",
            "Epoch: 405 - Train Loss: 0.5012774467468262 - Test Loss: 0.5172085762023926\n",
            "Epoch: 406 - Train Loss: 0.5027195811271667 - Test Loss: 0.5186570286750793\n",
            "Epoch: 407 - Train Loss: 0.5041619539260864 - Test Loss: 0.5201054215431213\n",
            "Epoch: 408 - Train Loss: 0.5056043863296509 - Test Loss: 0.5215535759925842\n",
            "Epoch: 409 - Train Loss: 0.5070466995239258 - Test Loss: 0.5230013728141785\n",
            "Epoch: 410 - Train Loss: 0.5084887742996216 - Test Loss: 0.5244485139846802\n",
            "Epoch: 411 - Train Loss: 0.5099304914474487 - Test Loss: 0.5258951783180237\n",
            "Epoch: 412 - Train Loss: 0.5113715529441833 - Test Loss: 0.5273408889770508\n",
            "Epoch: 413 - Train Loss: 0.5128119587898254 - Test Loss: 0.5287857055664062\n",
            "Epoch: 414 - Train Loss: 0.5142515897750854 - Test Loss: 0.530229389667511\n",
            "Epoch: 415 - Train Loss: 0.5156901478767395 - Test Loss: 0.5316718220710754\n",
            "Epoch: 416 - Train Loss: 0.5171276330947876 - Test Loss: 0.5331128835678101\n",
            "Epoch: 417 - Train Loss: 0.5185638070106506 - Test Loss: 0.5345523357391357\n",
            "Epoch: 418 - Train Loss: 0.5199986696243286 - Test Loss: 0.5359901785850525\n",
            "Epoch: 419 - Train Loss: 0.5214318633079529 - Test Loss: 0.5374261736869812\n",
            "Epoch: 420 - Train Loss: 0.5228633880615234 - Test Loss: 0.5388602018356323\n",
            "Epoch: 421 - Train Loss: 0.5242931246757507 - Test Loss: 0.5402920842170715\n",
            "Epoch: 422 - Train Loss: 0.5257207751274109 - Test Loss: 0.5417217016220093\n",
            "Epoch: 423 - Train Loss: 0.5271463394165039 - Test Loss: 0.5431490540504456\n",
            "Epoch: 424 - Train Loss: 0.5285696983337402 - Test Loss: 0.5445738434791565\n",
            "Epoch: 425 - Train Loss: 0.5299906730651855 - Test Loss: 0.5459959506988525\n",
            "Epoch: 426 - Train Loss: 0.5314090251922607 - Test Loss: 0.5474152565002441\n",
            "Epoch: 427 - Train Loss: 0.5328247547149658 - Test Loss: 0.548831582069397\n",
            "Epoch: 428 - Train Loss: 0.5342375636100769 - Test Loss: 0.5502448081970215\n",
            "Epoch: 429 - Train Loss: 0.5356475710868835 - Test Loss: 0.5516549944877625\n",
            "Epoch: 430 - Train Loss: 0.5370543599128723 - Test Loss: 0.5530617237091064\n",
            "Epoch: 431 - Train Loss: 0.5384581089019775 - Test Loss: 0.5544649362564087\n",
            "Epoch: 432 - Train Loss: 0.539858341217041 - Test Loss: 0.5558646321296692\n",
            "Epoch: 433 - Train Loss: 0.5412551164627075 - Test Loss: 0.5572605133056641\n",
            "Epoch: 434 - Train Loss: 0.5426483154296875 - Test Loss: 0.5586525201797485\n",
            "Epoch: 435 - Train Loss: 0.5440376996994019 - Test Loss: 0.5600404739379883\n",
            "Epoch: 436 - Train Loss: 0.5454232692718506 - Test Loss: 0.5614243745803833\n",
            "Epoch: 437 - Train Loss: 0.5468047261238098 - Test Loss: 0.5628039836883545\n",
            "Epoch: 438 - Train Loss: 0.5481821298599243 - Test Loss: 0.5641792416572571\n",
            "Epoch: 439 - Train Loss: 0.5495551824569702 - Test Loss: 0.5655498504638672\n",
            "Epoch: 440 - Train Loss: 0.5509238243103027 - Test Loss: 0.5669159293174744\n",
            "Epoch: 441 - Train Loss: 0.5522879958152771 - Test Loss: 0.5682771801948547\n",
            "Epoch: 442 - Train Loss: 0.5536474585533142 - Test Loss: 0.5696335434913635\n",
            "Epoch: 443 - Train Loss: 0.5550021529197693 - Test Loss: 0.5709847807884216\n",
            "Epoch: 444 - Train Loss: 0.5563518404960632 - Test Loss: 0.572330892086029\n",
            "Epoch: 445 - Train Loss: 0.5576965808868408 - Test Loss: 0.5736717581748962\n",
            "Epoch: 446 - Train Loss: 0.5590360760688782 - Test Loss: 0.5750071406364441\n",
            "Epoch: 447 - Train Loss: 0.5603703260421753 - Test Loss: 0.5763370394706726\n",
            "Epoch: 448 - Train Loss: 0.5616991519927979 - Test Loss: 0.5776612162590027\n",
            "Epoch: 449 - Train Loss: 0.5630224347114563 - Test Loss: 0.5789797306060791\n",
            "Epoch: 450 - Train Loss: 0.5643399953842163 - Test Loss: 0.5802921652793884\n",
            "Epoch: 451 - Train Loss: 0.5656517744064331 - Test Loss: 0.5815986394882202\n",
            "Epoch: 452 - Train Loss: 0.5669576525688171 - Test Loss: 0.5828989744186401\n",
            "Epoch: 453 - Train Loss: 0.5682574510574341 - Test Loss: 0.5841930508613586\n",
            "Epoch: 454 - Train Loss: 0.5695511698722839 - Test Loss: 0.5854806900024414\n",
            "Epoch: 455 - Train Loss: 0.5708385109901428 - Test Loss: 0.5867618322372437\n",
            "Epoch: 456 - Train Loss: 0.572119414806366 - Test Loss: 0.5880363583564758\n",
            "Epoch: 457 - Train Loss: 0.5733938813209534 - Test Loss: 0.5893041491508484\n",
            "Epoch: 458 - Train Loss: 0.5746616125106812 - Test Loss: 0.5905649662017822\n",
            "Epoch: 459 - Train Loss: 0.5759226679801941 - Test Loss: 0.5918188691139221\n",
            "Epoch: 460 - Train Loss: 0.5771767497062683 - Test Loss: 0.593065619468689\n",
            "Epoch: 461 - Train Loss: 0.578423798084259 - Test Loss: 0.5943050980567932\n",
            "Epoch: 462 - Train Loss: 0.5796636343002319 - Test Loss: 0.5955373048782349\n",
            "Epoch: 463 - Train Loss: 0.5808963179588318 - Test Loss: 0.5967618823051453\n",
            "Epoch: 464 - Train Loss: 0.5821216106414795 - Test Loss: 0.5979790091514587\n",
            "Epoch: 465 - Train Loss: 0.5833393931388855 - Test Loss: 0.5991883277893066\n",
            "Epoch: 466 - Train Loss: 0.5845495462417603 - Test Loss: 0.6003899574279785\n",
            "Epoch: 467 - Train Loss: 0.585752010345459 - Test Loss: 0.6015835404396057\n",
            "Epoch: 468 - Train Loss: 0.5869466662406921 - Test Loss: 0.602769136428833\n",
            "Epoch: 469 - Train Loss: 0.5881332159042358 - Test Loss: 0.6039465665817261\n",
            "Epoch: 470 - Train Loss: 0.5893118381500244 - Test Loss: 0.6051155924797058\n",
            "Epoch: 471 - Train Loss: 0.5904820561408997 - Test Loss: 0.6062763333320618\n",
            "Epoch: 472 - Train Loss: 0.5916440486907959 - Test Loss: 0.6074285507202148\n",
            "Epoch: 473 - Train Loss: 0.5927976965904236 - Test Loss: 0.6085720658302307\n",
            "Epoch: 474 - Train Loss: 0.5939426422119141 - Test Loss: 0.6097068786621094\n",
            "Epoch: 475 - Train Loss: 0.5950790643692017 - Test Loss: 0.6108328700065613\n",
            "Epoch: 476 - Train Loss: 0.5962067246437073 - Test Loss: 0.6119499802589417\n",
            "Epoch: 477 - Train Loss: 0.5973255038261414 - Test Loss: 0.6130579113960266\n",
            "Epoch: 478 - Train Loss: 0.5984352231025696 - Test Loss: 0.6141567826271057\n",
            "Epoch: 479 - Train Loss: 0.5995358824729919 - Test Loss: 0.6152462959289551\n",
            "Epoch: 480 - Train Loss: 0.6006271839141846 - Test Loss: 0.6163262724876404\n",
            "Epoch: 481 - Train Loss: 0.6017091274261475 - Test Loss: 0.6173967123031616\n",
            "Epoch: 482 - Train Loss: 0.6027815937995911 - Test Loss: 0.6184576153755188\n",
            "Epoch: 483 - Train Loss: 0.6038444638252258 - Test Loss: 0.6195086240768433\n",
            "Epoch: 484 - Train Loss: 0.604897677898407 - Test Loss: 0.6205497980117798\n",
            "Epoch: 485 - Train Loss: 0.605941116809845 - Test Loss: 0.6215810775756836\n",
            "Epoch: 486 - Train Loss: 0.6069745421409607 - Test Loss: 0.6226022243499756\n",
            "Epoch: 487 - Train Loss: 0.6079980731010437 - Test Loss: 0.6236132979393005\n",
            "Epoch: 488 - Train Loss: 0.6090114116668701 - Test Loss: 0.6246139407157898\n",
            "Epoch: 489 - Train Loss: 0.6100145578384399 - Test Loss: 0.6256042718887329\n",
            "Epoch: 490 - Train Loss: 0.6110072731971741 - Test Loss: 0.6265840530395508\n",
            "Epoch: 491 - Train Loss: 0.6119896769523621 - Test Loss: 0.6275533437728882\n",
            "Epoch: 492 - Train Loss: 0.6129614114761353 - Test Loss: 0.628511905670166\n",
            "Epoch: 493 - Train Loss: 0.613922655582428 - Test Loss: 0.6294597387313843\n",
            "Epoch: 494 - Train Loss: 0.6148729920387268 - Test Loss: 0.6303965449333191\n",
            "Epoch: 495 - Train Loss: 0.6158124804496765 - Test Loss: 0.63132244348526\n",
            "Epoch: 496 - Train Loss: 0.6167410612106323 - Test Loss: 0.6322372555732727\n",
            "Epoch: 497 - Train Loss: 0.6176585555076599 - Test Loss: 0.6331408023834229\n",
            "Epoch: 498 - Train Loss: 0.6185649037361145 - Test Loss: 0.6340330839157104\n",
            "Epoch: 499 - Train Loss: 0.6194599270820618 - Test Loss: 0.6349140405654907\n",
            "Epoch: 500 - Train Loss: 0.6203435659408569 - Test Loss: 0.6357833743095398\n",
            "Epoch: 501 - Train Loss: 0.6212158203125 - Test Loss: 0.6366412043571472\n",
            "Epoch: 502 - Train Loss: 0.6220763921737671 - Test Loss: 0.6374872922897339\n",
            "Epoch: 503 - Train Loss: 0.622925341129303 - Test Loss: 0.638321578502655\n",
            "Epoch: 504 - Train Loss: 0.6237624883651733 - Test Loss: 0.6391440629959106\n",
            "Epoch: 505 - Train Loss: 0.6245877146720886 - Test Loss: 0.6399545669555664\n",
            "Epoch: 506 - Train Loss: 0.625400960445404 - Test Loss: 0.6407528519630432\n",
            "Epoch: 507 - Train Loss: 0.6262021660804749 - Test Loss: 0.6415389776229858\n",
            "Epoch: 508 - Train Loss: 0.6269911527633667 - Test Loss: 0.6423128247261047\n",
            "Epoch: 509 - Train Loss: 0.6277678608894348 - Test Loss: 0.6430743932723999\n",
            "Epoch: 510 - Train Loss: 0.6285321712493896 - Test Loss: 0.6438234448432922\n",
            "Epoch: 511 - Train Loss: 0.6292840838432312 - Test Loss: 0.6445600390434265\n",
            "Epoch: 512 - Train Loss: 0.6300234198570251 - Test Loss: 0.6452838182449341\n",
            "Epoch: 513 - Train Loss: 0.6307499408721924 - Test Loss: 0.6459948420524597\n",
            "Epoch: 514 - Train Loss: 0.6314637660980225 - Test Loss: 0.6466931700706482\n",
            "Epoch: 515 - Train Loss: 0.6321648955345154 - Test Loss: 0.6473785042762756\n",
            "Epoch: 516 - Train Loss: 0.6328528523445129 - Test Loss: 0.6480507850646973\n",
            "Epoch: 517 - Train Loss: 0.6335276961326599 - Test Loss: 0.6487098932266235\n",
            "Epoch: 518 - Train Loss: 0.6341895461082458 - Test Loss: 0.649355947971344\n",
            "Epoch: 519 - Train Loss: 0.6348381638526917 - Test Loss: 0.6499886512756348\n",
            "Epoch: 520 - Train Loss: 0.6354734301567078 - Test Loss: 0.6506080031394958\n",
            "Epoch: 521 - Train Loss: 0.6360952258110046 - Test Loss: 0.6512137651443481\n",
            "Epoch: 522 - Train Loss: 0.6367034316062927 - Test Loss: 0.6518060564994812\n",
            "Epoch: 523 - Train Loss: 0.6372982263565063 - Test Loss: 0.6523848176002502\n",
            "Epoch: 524 - Train Loss: 0.6378791928291321 - Test Loss: 0.6529496312141418\n",
            "Epoch: 525 - Train Loss: 0.6384463310241699 - Test Loss: 0.6535006761550903\n",
            "Epoch: 526 - Train Loss: 0.6389997005462646 - Test Loss: 0.6540378928184509\n",
            "Epoch: 527 - Train Loss: 0.6395391225814819 - Test Loss: 0.6545612215995789\n",
            "Epoch: 528 - Train Loss: 0.6400644183158875 - Test Loss: 0.6550703048706055\n",
            "Epoch: 529 - Train Loss: 0.6405755281448364 - Test Loss: 0.6555652618408203\n",
            "Epoch: 530 - Train Loss: 0.6410725712776184 - Test Loss: 0.6560461521148682\n",
            "Epoch: 531 - Train Loss: 0.6415552496910095 - Test Loss: 0.6565125584602356\n",
            "Epoch: 532 - Train Loss: 0.6420235633850098 - Test Loss: 0.6569647789001465\n",
            "Epoch: 533 - Train Loss: 0.6424773931503296 - Test Loss: 0.6574023365974426\n",
            "Epoch: 534 - Train Loss: 0.6429166197776794 - Test Loss: 0.6578254103660583\n",
            "Epoch: 535 - Train Loss: 0.6433413028717041 - Test Loss: 0.6582339406013489\n",
            "Epoch: 536 - Train Loss: 0.6437512040138245 - Test Loss: 0.6586276888847351\n",
            "Epoch: 537 - Train Loss: 0.6441463828086853 - Test Loss: 0.6590067148208618\n",
            "Epoch: 538 - Train Loss: 0.6445266008377075 - Test Loss: 0.6593708992004395\n",
            "Epoch: 539 - Train Loss: 0.6448920369148254 - Test Loss: 0.659720242023468\n",
            "Epoch: 540 - Train Loss: 0.6452422738075256 - Test Loss: 0.6600544452667236\n",
            "Epoch: 541 - Train Loss: 0.645577609539032 - Test Loss: 0.6603737473487854\n",
            "Epoch: 542 - Train Loss: 0.645897626876831 - Test Loss: 0.6606778502464294\n",
            "Epoch: 543 - Train Loss: 0.6462023854255676 - Test Loss: 0.6609667539596558\n",
            "Epoch: 544 - Train Loss: 0.6464919447898865 - Test Loss: 0.6612404584884644\n",
            "Epoch: 545 - Train Loss: 0.6467660665512085 - Test Loss: 0.6614987850189209\n",
            "Epoch: 546 - Train Loss: 0.6470247507095337 - Test Loss: 0.6617416739463806\n",
            "Epoch: 547 - Train Loss: 0.647267758846283 - Test Loss: 0.6619690656661987\n",
            "Epoch: 548 - Train Loss: 0.6474952697753906 - Test Loss: 0.66218101978302\n",
            "Epoch: 549 - Train Loss: 0.6477071642875671 - Test Loss: 0.6623773574829102\n",
            "Epoch: 550 - Train Loss: 0.6479034423828125 - Test Loss: 0.6625580191612244\n",
            "Epoch: 551 - Train Loss: 0.6480836868286133 - Test Loss: 0.6627229452133179\n",
            "Epoch: 552 - Train Loss: 0.6482481360435486 - Test Loss: 0.6628721356391907\n",
            "Epoch: 553 - Train Loss: 0.6483966708183289 - Test Loss: 0.663005530834198\n",
            "Epoch: 554 - Train Loss: 0.6485292315483093 - Test Loss: 0.6631229519844055\n",
            "Epoch: 555 - Train Loss: 0.6486457586288452 - Test Loss: 0.6632245182991028\n",
            "Epoch: 556 - Train Loss: 0.648746132850647 - Test Loss: 0.6633099317550659\n",
            "Epoch: 557 - Train Loss: 0.6488304138183594 - Test Loss: 0.6633794903755188\n",
            "Epoch: 558 - Train Loss: 0.6488984227180481 - Test Loss: 0.6634327173233032\n",
            "Epoch: 559 - Train Loss: 0.6489501595497131 - Test Loss: 0.6634699702262878\n",
            "Epoch: 560 - Train Loss: 0.6489856243133545 - Test Loss: 0.6634909510612488\n",
            "Epoch: 561 - Train Loss: 0.6490046977996826 - Test Loss: 0.663495659828186\n",
            "Epoch: 562 - Train Loss: 0.6490073204040527 - Test Loss: 0.6634841561317444\n",
            "Epoch: 563 - Train Loss: 0.6489933729171753 - Test Loss: 0.6634562015533447\n",
            "Epoch: 564 - Train Loss: 0.6489629149436951 - Test Loss: 0.6634117960929871\n",
            "Epoch: 565 - Train Loss: 0.6489158868789673 - Test Loss: 0.6633509993553162\n",
            "Epoch: 566 - Train Loss: 0.6488522887229919 - Test Loss: 0.6632736921310425\n",
            "Epoch: 567 - Train Loss: 0.6487718224525452 - Test Loss: 0.6631798148155212\n",
            "Epoch: 568 - Train Loss: 0.6486746668815613 - Test Loss: 0.6630693674087524\n",
            "Epoch: 569 - Train Loss: 0.6485608220100403 - Test Loss: 0.6629423499107361\n",
            "Epoch: 570 - Train Loss: 0.6484301686286926 - Test Loss: 0.6627987027168274\n",
            "Epoch: 571 - Train Loss: 0.648282527923584 - Test Loss: 0.6626383066177368\n",
            "Epoch: 572 - Train Loss: 0.6481181383132935 - Test Loss: 0.6624612212181091\n",
            "Epoch: 573 - Train Loss: 0.6479367613792419 - Test Loss: 0.6622673273086548\n",
            "Epoch: 574 - Train Loss: 0.6477383971214294 - Test Loss: 0.6620566844940186\n",
            "Epoch: 575 - Train Loss: 0.6475231051445007 - Test Loss: 0.6618292331695557\n",
            "Epoch: 576 - Train Loss: 0.6472907066345215 - Test Loss: 0.6615848541259766\n",
            "Epoch: 577 - Train Loss: 0.6470412611961365 - Test Loss: 0.6613236665725708\n",
            "Epoch: 578 - Train Loss: 0.6467747092247009 - Test Loss: 0.6610456109046936\n",
            "Epoch: 579 - Train Loss: 0.6464909911155701 - Test Loss: 0.6607505679130554\n",
            "Epoch: 580 - Train Loss: 0.6461901068687439 - Test Loss: 0.660438597202301\n",
            "Epoch: 581 - Train Loss: 0.6458721160888672 - Test Loss: 0.6601096391677856\n",
            "Epoch: 582 - Train Loss: 0.6455368995666504 - Test Loss: 0.6597636342048645\n",
            "Epoch: 583 - Train Loss: 0.6451844573020935 - Test Loss: 0.6594007611274719\n",
            "Epoch: 584 - Train Loss: 0.6448147892951965 - Test Loss: 0.6590207815170288\n",
            "Epoch: 585 - Train Loss: 0.6444277763366699 - Test Loss: 0.6586236953735352\n",
            "Epoch: 586 - Train Loss: 0.6440234780311584 - Test Loss: 0.6582095623016357\n",
            "Epoch: 587 - Train Loss: 0.6436018347740173 - Test Loss: 0.6577783823013306\n",
            "Epoch: 588 - Train Loss: 0.6431629657745361 - Test Loss: 0.6573300957679749\n",
            "Epoch: 589 - Train Loss: 0.6427067518234253 - Test Loss: 0.6568647623062134\n",
            "Epoch: 590 - Train Loss: 0.64223313331604 - Test Loss: 0.6563822627067566\n",
            "Epoch: 591 - Train Loss: 0.6417422890663147 - Test Loss: 0.6558827757835388\n",
            "Epoch: 592 - Train Loss: 0.6412338614463806 - Test Loss: 0.6553659439086914\n",
            "Epoch: 593 - Train Loss: 0.640708327293396 - Test Loss: 0.6548321843147278\n",
            "Epoch: 594 - Train Loss: 0.6401652693748474 - Test Loss: 0.6542811989784241\n",
            "Epoch: 595 - Train Loss: 0.6396048069000244 - Test Loss: 0.6537132263183594\n",
            "Epoch: 596 - Train Loss: 0.6390270590782166 - Test Loss: 0.6531280875205994\n",
            "Epoch: 597 - Train Loss: 0.6384318470954895 - Test Loss: 0.6525257229804993\n",
            "Epoch: 598 - Train Loss: 0.6378193497657776 - Test Loss: 0.6519063115119934\n",
            "Epoch: 599 - Train Loss: 0.6371894478797913 - Test Loss: 0.6512698531150818\n",
            "Epoch: 600 - Train Loss: 0.6365422010421753 - Test Loss: 0.6506162881851196\n",
            "Epoch: 601 - Train Loss: 0.6358775496482849 - Test Loss: 0.6499456167221069\n",
            "Epoch: 602 - Train Loss: 0.6351956129074097 - Test Loss: 0.6492579579353333\n",
            "Epoch: 603 - Train Loss: 0.6344963312149048 - Test Loss: 0.6485531330108643\n",
            "Epoch: 604 - Train Loss: 0.633779764175415 - Test Loss: 0.647831380367279\n",
            "Epoch: 605 - Train Loss: 0.6330457925796509 - Test Loss: 0.6470925211906433\n",
            "Epoch: 606 - Train Loss: 0.6322946548461914 - Test Loss: 0.6463367938995361\n",
            "Epoch: 607 - Train Loss: 0.6315261721611023 - Test Loss: 0.6455639600753784\n",
            "Epoch: 608 - Train Loss: 0.6307405829429626 - Test Loss: 0.6447743773460388\n",
            "Epoch: 609 - Train Loss: 0.6299376487731934 - Test Loss: 0.6439676880836487\n",
            "Epoch: 610 - Train Loss: 0.6291176080703735 - Test Loss: 0.6431441903114319\n",
            "Epoch: 611 - Train Loss: 0.6282803416252136 - Test Loss: 0.6423038840293884\n",
            "Epoch: 612 - Train Loss: 0.6274259686470032 - Test Loss: 0.6414466500282288\n",
            "Epoch: 613 - Train Loss: 0.626554548740387 - Test Loss: 0.640572726726532\n",
            "Epoch: 614 - Train Loss: 0.6256659626960754 - Test Loss: 0.639681875705719\n",
            "Epoch: 615 - Train Loss: 0.6247604489326477 - Test Loss: 0.6387743949890137\n",
            "Epoch: 616 - Train Loss: 0.623837947845459 - Test Loss: 0.637850284576416\n",
            "Epoch: 617 - Train Loss: 0.6228983998298645 - Test Loss: 0.6369094848632812\n",
            "Epoch: 618 - Train Loss: 0.6219419836997986 - Test Loss: 0.6359520554542542\n",
            "Epoch: 619 - Train Loss: 0.620968759059906 - Test Loss: 0.634978175163269\n",
            "Epoch: 620 - Train Loss: 0.6199786067008972 - Test Loss: 0.6339877247810364\n",
            "Epoch: 621 - Train Loss: 0.6189718246459961 - Test Loss: 0.6329808831214905\n",
            "Epoch: 622 - Train Loss: 0.6179481744766235 - Test Loss: 0.6319575309753418\n",
            "Epoch: 623 - Train Loss: 0.6169079542160034 - Test Loss: 0.6309179067611694\n",
            "Epoch: 624 - Train Loss: 0.6158509850502014 - Test Loss: 0.6298620104789734\n",
            "Epoch: 625 - Train Loss: 0.6147776246070862 - Test Loss: 0.6287898421287537\n",
            "Epoch: 626 - Train Loss: 0.6136876344680786 - Test Loss: 0.6277014017105103\n",
            "Epoch: 627 - Train Loss: 0.6125813126564026 - Test Loss: 0.6265970468521118\n",
            "Epoch: 628 - Train Loss: 0.6114585995674133 - Test Loss: 0.6254765391349792\n",
            "Epoch: 629 - Train Loss: 0.6103195548057556 - Test Loss: 0.6243399977684021\n",
            "Epoch: 630 - Train Loss: 0.6091642379760742 - Test Loss: 0.6231876015663147\n",
            "Epoch: 631 - Train Loss: 0.6079928278923035 - Test Loss: 0.622019350528717\n",
            "Epoch: 632 - Train Loss: 0.6068053245544434 - Test Loss: 0.6208354830741882\n",
            "Epoch: 633 - Train Loss: 0.6056017279624939 - Test Loss: 0.6196357607841492\n",
            "Epoch: 634 - Train Loss: 0.6043822169303894 - Test Loss: 0.6184203624725342\n",
            "Epoch: 635 - Train Loss: 0.6031468510627747 - Test Loss: 0.6171894073486328\n",
            "Epoch: 636 - Train Loss: 0.6018956303596497 - Test Loss: 0.6159431338310242\n",
            "Epoch: 637 - Train Loss: 0.6006286144256592 - Test Loss: 0.6146813035011292\n",
            "Epoch: 638 - Train Loss: 0.5993461012840271 - Test Loss: 0.6134042143821716\n",
            "Epoch: 639 - Train Loss: 0.5980479717254639 - Test Loss: 0.6121119856834412\n",
            "Epoch: 640 - Train Loss: 0.596734344959259 - Test Loss: 0.6108046174049377\n",
            "Epoch: 641 - Train Loss: 0.5954053997993469 - Test Loss: 0.6094821095466614\n",
            "Epoch: 642 - Train Loss: 0.5940611958503723 - Test Loss: 0.6081447005271912\n",
            "Epoch: 643 - Train Loss: 0.5927017331123352 - Test Loss: 0.6067923903465271\n",
            "Epoch: 644 - Train Loss: 0.5913271903991699 - Test Loss: 0.6054252982139587\n",
            "Epoch: 645 - Train Loss: 0.589937686920166 - Test Loss: 0.6040435433387756\n",
            "Epoch: 646 - Train Loss: 0.5885332822799683 - Test Loss: 0.6026473045349121\n",
            "Epoch: 647 - Train Loss: 0.587114155292511 - Test Loss: 0.6012365818023682\n",
            "Epoch: 648 - Train Loss: 0.5856802463531494 - Test Loss: 0.5998113751411438\n",
            "Epoch: 649 - Train Loss: 0.5842316746711731 - Test Loss: 0.5983719825744629\n",
            "Epoch: 650 - Train Loss: 0.5827688574790955 - Test Loss: 0.596918523311615\n",
            "Epoch: 651 - Train Loss: 0.5812915563583374 - Test Loss: 0.5954509377479553\n",
            "Epoch: 652 - Train Loss: 0.5798001289367676 - Test Loss: 0.5939695239067078\n",
            "Epoch: 653 - Train Loss: 0.5782945156097412 - Test Loss: 0.5924742221832275\n",
            "Epoch: 654 - Train Loss: 0.5767748355865479 - Test Loss: 0.590965211391449\n",
            "Epoch: 655 - Train Loss: 0.5752412676811218 - Test Loss: 0.5894425511360168\n",
            "Epoch: 656 - Train Loss: 0.5736938714981079 - Test Loss: 0.5879064798355103\n",
            "Epoch: 657 - Train Loss: 0.57213294506073 - Test Loss: 0.5863571166992188\n",
            "Epoch: 658 - Train Loss: 0.5705584287643433 - Test Loss: 0.5847944617271423\n",
            "Epoch: 659 - Train Loss: 0.5689705014228821 - Test Loss: 0.5832188129425049\n",
            "Epoch: 660 - Train Loss: 0.5673693418502808 - Test Loss: 0.5816300511360168\n",
            "Epoch: 661 - Train Loss: 0.5657549500465393 - Test Loss: 0.5800284743309021\n",
            "Epoch: 662 - Train Loss: 0.5641275644302368 - Test Loss: 0.5784142017364502\n",
            "Epoch: 663 - Train Loss: 0.5624872446060181 - Test Loss: 0.5767871737480164\n",
            "Epoch: 664 - Train Loss: 0.5608342289924622 - Test Loss: 0.5751478672027588\n",
            "Epoch: 665 - Train Loss: 0.5591685771942139 - Test Loss: 0.5734961032867432\n",
            "Epoch: 666 - Train Loss: 0.5574905276298523 - Test Loss: 0.5718322396278381\n",
            "Epoch: 667 - Train Loss: 0.5558000206947327 - Test Loss: 0.5701562762260437\n",
            "Epoch: 668 - Train Loss: 0.5540973544120789 - Test Loss: 0.5684685111045837\n",
            "Epoch: 669 - Train Loss: 0.5523827075958252 - Test Loss: 0.5667688250541687\n",
            "Epoch: 670 - Train Loss: 0.5506560802459717 - Test Loss: 0.5650575160980225\n",
            "Epoch: 671 - Train Loss: 0.5489176511764526 - Test Loss: 0.5633347034454346\n",
            "Epoch: 672 - Train Loss: 0.5471675992012024 - Test Loss: 0.5616005659103394\n",
            "Epoch: 673 - Train Loss: 0.5454062223434448 - Test Loss: 0.5598552227020264\n",
            "Epoch: 674 - Train Loss: 0.5436333417892456 - Test Loss: 0.5580987930297852\n",
            "Epoch: 675 - Train Loss: 0.5418494343757629 - Test Loss: 0.5563315749168396\n",
            "Epoch: 676 - Train Loss: 0.5400546193122864 - Test Loss: 0.5545536279678345\n",
            "Epoch: 677 - Train Loss: 0.5382488369941711 - Test Loss: 0.5527649521827698\n",
            "Epoch: 678 - Train Loss: 0.5364323258399963 - Test Loss: 0.5509659051895142\n",
            "Epoch: 679 - Train Loss: 0.5346053838729858 - Test Loss: 0.5491566061973572\n",
            "Epoch: 680 - Train Loss: 0.5327679514884949 - Test Loss: 0.547336995601654\n",
            "Epoch: 681 - Train Loss: 0.5309203863143921 - Test Loss: 0.545507550239563\n",
            "Epoch: 682 - Train Loss: 0.529062807559967 - Test Loss: 0.543668270111084\n",
            "Epoch: 683 - Train Loss: 0.5271952748298645 - Test Loss: 0.5418193340301514\n",
            "Epoch: 684 - Train Loss: 0.5253179669380188 - Test Loss: 0.5399608016014099\n",
            "Epoch: 685 - Train Loss: 0.5234312415122986 - Test Loss: 0.5380930304527283\n",
            "Epoch: 686 - Train Loss: 0.5215350985527039 - Test Loss: 0.5362160801887512\n",
            "Epoch: 687 - Train Loss: 0.519629716873169 - Test Loss: 0.5343301892280579\n",
            "Epoch: 688 - Train Loss: 0.5177152752876282 - Test Loss: 0.5324352979660034\n",
            "Epoch: 689 - Train Loss: 0.5157918930053711 - Test Loss: 0.5305318236351013\n",
            "Epoch: 690 - Train Loss: 0.5138598680496216 - Test Loss: 0.5286197662353516\n",
            "Epoch: 691 - Train Loss: 0.511919379234314 - Test Loss: 0.5266994833946228\n",
            "Epoch: 692 - Train Loss: 0.509970486164093 - Test Loss: 0.5247709155082703\n",
            "Epoch: 693 - Train Loss: 0.5080134868621826 - Test Loss: 0.5228345394134521\n",
            "Epoch: 694 - Train Loss: 0.5060484409332275 - Test Loss: 0.5208902359008789\n",
            "Epoch: 695 - Train Loss: 0.5040756464004517 - Test Loss: 0.5189383029937744\n",
            "Epoch: 696 - Train Loss: 0.502095103263855 - Test Loss: 0.5169788002967834\n",
            "Epoch: 697 - Train Loss: 0.5001071095466614 - Test Loss: 0.5150120258331299\n",
            "Epoch: 698 - Train Loss: 0.49811187386512756 - Test Loss: 0.5130382180213928\n",
            "Epoch: 699 - Train Loss: 0.4961094260215759 - Test Loss: 0.5110573172569275\n",
            "Epoch: 700 - Train Loss: 0.49410003423690796 - Test Loss: 0.5090696215629578\n",
            "Epoch: 701 - Train Loss: 0.49208393692970276 - Test Loss: 0.507075309753418\n",
            "Epoch: 702 - Train Loss: 0.4900612533092499 - Test Loss: 0.505074679851532\n",
            "Epoch: 703 - Train Loss: 0.48803219199180603 - Test Loss: 0.5030676126480103\n",
            "Epoch: 704 - Train Loss: 0.48599687218666077 - Test Loss: 0.501054584980011\n",
            "Epoch: 705 - Train Loss: 0.4839556813240051 - Test Loss: 0.4990357458591461\n",
            "Epoch: 706 - Train Loss: 0.4819086790084839 - Test Loss: 0.4970111548900604\n",
            "Epoch: 707 - Train Loss: 0.47985589504241943 - Test Loss: 0.49498093128204346\n",
            "Epoch: 708 - Train Loss: 0.4777977466583252 - Test Loss: 0.49294552206993103\n",
            "Epoch: 709 - Train Loss: 0.47573426365852356 - Test Loss: 0.4909048080444336\n",
            "Epoch: 710 - Train Loss: 0.47366568446159363 - Test Loss: 0.48885905742645264\n",
            "Epoch: 711 - Train Loss: 0.4715922176837921 - Test Loss: 0.48680856823921204\n",
            "Epoch: 712 - Train Loss: 0.4695141017436981 - Test Loss: 0.4847535192966461\n",
            "Epoch: 713 - Train Loss: 0.46743133664131165 - Test Loss: 0.4826938211917877\n",
            "Epoch: 714 - Train Loss: 0.46534425020217896 - Test Loss: 0.48062998056411743\n",
            "Epoch: 715 - Train Loss: 0.46325305104255676 - Test Loss: 0.47856205701828003\n",
            "Epoch: 716 - Train Loss: 0.46115797758102417 - Test Loss: 0.4764902591705322\n",
            "Epoch: 717 - Train Loss: 0.45905888080596924 - Test Loss: 0.4744146168231964\n",
            "Epoch: 718 - Train Loss: 0.45695623755455017 - Test Loss: 0.4723353981971741\n",
            "Epoch: 719 - Train Loss: 0.45485013723373413 - Test Loss: 0.4702528119087219\n",
            "Epoch: 720 - Train Loss: 0.4527408480644226 - Test Loss: 0.4681670069694519\n",
            "Epoch: 721 - Train Loss: 0.45062848925590515 - Test Loss: 0.4660781919956207\n",
            "Epoch: 722 - Train Loss: 0.44851335883140564 - Test Loss: 0.4639867842197418\n",
            "Epoch: 723 - Train Loss: 0.44639554619789124 - Test Loss: 0.46189260482788086\n",
            "Epoch: 724 - Train Loss: 0.44427525997161865 - Test Loss: 0.45979592204093933\n",
            "Epoch: 725 - Train Loss: 0.44215261936187744 - Test Loss: 0.45769694447517395\n",
            "Epoch: 726 - Train Loss: 0.4400279223918915 - Test Loss: 0.4555959403514862\n",
            "Epoch: 727 - Train Loss: 0.43790119886398315 - Test Loss: 0.4534929394721985\n",
            "Epoch: 728 - Train Loss: 0.43577277660369873 - Test Loss: 0.4513881802558899\n",
            "Epoch: 729 - Train Loss: 0.43364277482032776 - Test Loss: 0.44928184151649475\n",
            "Epoch: 730 - Train Loss: 0.4315113127231598 - Test Loss: 0.4471741020679474\n",
            "Epoch: 731 - Train Loss: 0.4293787479400635 - Test Loss: 0.4450652003288269\n",
            "Epoch: 732 - Train Loss: 0.42724522948265076 - Test Loss: 0.44295534491539\n",
            "Epoch: 733 - Train Loss: 0.42511072754859924 - Test Loss: 0.44084447622299194\n",
            "Epoch: 734 - Train Loss: 0.4229755997657776 - Test Loss: 0.4387330114841461\n",
            "Epoch: 735 - Train Loss: 0.42084014415740967 - Test Loss: 0.4366210699081421\n",
            "Epoch: 736 - Train Loss: 0.41870418190956116 - Test Loss: 0.43450862169265747\n",
            "Epoch: 737 - Train Loss: 0.4165681302547455 - Test Loss: 0.4323960542678833\n",
            "Epoch: 738 - Train Loss: 0.41443219780921936 - Test Loss: 0.4302836060523987\n",
            "Epoch: 739 - Train Loss: 0.41229644417762756 - Test Loss: 0.4281712472438812\n",
            "Epoch: 740 - Train Loss: 0.41016116738319397 - Test Loss: 0.4260592460632324\n",
            "Epoch: 741 - Train Loss: 0.40802642703056335 - Test Loss: 0.423947811126709\n",
            "Epoch: 742 - Train Loss: 0.40589240193367004 - Test Loss: 0.4218370318412781\n",
            "Epoch: 743 - Train Loss: 0.4037593901157379 - Test Loss: 0.4197271168231964\n",
            "Epoch: 744 - Train Loss: 0.40162742137908936 - Test Loss: 0.4176182746887207\n",
            "Epoch: 745 - Train Loss: 0.39949673414230347 - Test Loss: 0.41551053524017334\n",
            "Epoch: 746 - Train Loss: 0.3973674774169922 - Test Loss: 0.4134041666984558\n",
            "Epoch: 747 - Train Loss: 0.39523983001708984 - Test Loss: 0.41129928827285767\n",
            "Epoch: 748 - Train Loss: 0.39311400055885315 - Test Loss: 0.4091961979866028\n",
            "Epoch: 749 - Train Loss: 0.3909900486469269 - Test Loss: 0.4070948660373688\n",
            "Epoch: 750 - Train Loss: 0.3888682425022125 - Test Loss: 0.4049955904483795\n",
            "Epoch: 751 - Train Loss: 0.38674861192703247 - Test Loss: 0.4028984308242798\n",
            "Epoch: 752 - Train Loss: 0.38463157415390015 - Test Loss: 0.40080365538597107\n",
            "Epoch: 753 - Train Loss: 0.3825170397758484 - Test Loss: 0.39871129393577576\n",
            "Epoch: 754 - Train Loss: 0.3804052174091339 - Test Loss: 0.39662161469459534\n",
            "Epoch: 755 - Train Loss: 0.3782964050769806 - Test Loss: 0.39453473687171936\n",
            "Epoch: 756 - Train Loss: 0.37619057297706604 - Test Loss: 0.39245080947875977\n",
            "Epoch: 757 - Train Loss: 0.3740880787372589 - Test Loss: 0.3903699815273285\n",
            "Epoch: 758 - Train Loss: 0.37198886275291443 - Test Loss: 0.3882923126220703\n",
            "Epoch: 759 - Train Loss: 0.36989328265190125 - Test Loss: 0.3862181603908539\n",
            "Epoch: 760 - Train Loss: 0.367801308631897 - Test Loss: 0.3841475248336792\n",
            "Epoch: 761 - Train Loss: 0.3657131791114807 - Test Loss: 0.3820805847644806\n",
            "Epoch: 762 - Train Loss: 0.36362898349761963 - Test Loss: 0.3800174295902252\n",
            "Epoch: 763 - Train Loss: 0.3615490198135376 - Test Loss: 0.3779582977294922\n",
            "Epoch: 764 - Train Loss: 0.35947319865226746 - Test Loss: 0.3759032189846039\n",
            "Epoch: 765 - Train Loss: 0.35740190744400024 - Test Loss: 0.37385252118110657\n",
            "Epoch: 766 - Train Loss: 0.3553350269794464 - Test Loss: 0.3718060255050659\n",
            "Epoch: 767 - Train Loss: 0.3532729148864746 - Test Loss: 0.36976414918899536\n",
            "Epoch: 768 - Train Loss: 0.3512157201766968 - Test Loss: 0.3677270710468292\n",
            "Epoch: 769 - Train Loss: 0.3491634130477905 - Test Loss: 0.36569464206695557\n",
            "Epoch: 770 - Train Loss: 0.34711623191833496 - Test Loss: 0.3636672794818878\n",
            "Epoch: 771 - Train Loss: 0.34507429599761963 - Test Loss: 0.3616449236869812\n",
            "Epoch: 772 - Train Loss: 0.3430376946926117 - Test Loss: 0.35962772369384766\n",
            "Epoch: 773 - Train Loss: 0.34100663661956787 - Test Loss: 0.3576158881187439\n",
            "Epoch: 774 - Train Loss: 0.3389812409877777 - Test Loss: 0.35560959577560425\n",
            "Epoch: 775 - Train Loss: 0.3369615375995636 - Test Loss: 0.35360872745513916\n",
            "Epoch: 776 - Train Loss: 0.33494776487350464 - Test Loss: 0.3516136407852173\n",
            "Epoch: 777 - Train Loss: 0.3329399824142456 - Test Loss: 0.3496243953704834\n",
            "Epoch: 778 - Train Loss: 0.33093830943107605 - Test Loss: 0.3476410210132599\n",
            "Epoch: 779 - Train Loss: 0.3289428949356079 - Test Loss: 0.34566372632980347\n",
            "Epoch: 780 - Train Loss: 0.32695382833480835 - Test Loss: 0.3436926305294037\n",
            "Epoch: 781 - Train Loss: 0.3249712288379669 - Test Loss: 0.34172776341438293\n",
            "Epoch: 782 - Train Loss: 0.32299521565437317 - Test Loss: 0.33976930379867554\n",
            "Epoch: 783 - Train Loss: 0.32102593779563904 - Test Loss: 0.33781740069389343\n",
            "Epoch: 784 - Train Loss: 0.3190634548664093 - Test Loss: 0.33587202429771423\n",
            "Epoch: 785 - Train Loss: 0.3171078860759735 - Test Loss: 0.3339334726333618\n",
            "Epoch: 786 - Train Loss: 0.315159410238266 - Test Loss: 0.3320017457008362\n",
            "Epoch: 787 - Train Loss: 0.31321796774864197 - Test Loss: 0.3300769031047821\n",
            "Epoch: 788 - Train Loss: 0.31128382682800293 - Test Loss: 0.32815903425216675\n",
            "Epoch: 789 - Train Loss: 0.30935704708099365 - Test Loss: 0.3262484669685364\n",
            "Epoch: 790 - Train Loss: 0.30743759870529175 - Test Loss: 0.3243449330329895\n",
            "Epoch: 791 - Train Loss: 0.3055257797241211 - Test Loss: 0.3224489092826843\n",
            "Epoch: 792 - Train Loss: 0.30362164974212646 - Test Loss: 0.3205602467060089\n",
            "Epoch: 793 - Train Loss: 0.3017251193523407 - Test Loss: 0.3186790645122528\n",
            "Epoch: 794 - Train Loss: 0.29983648657798767 - Test Loss: 0.31680554151535034\n",
            "Epoch: 795 - Train Loss: 0.2979556620121002 - Test Loss: 0.31493961811065674\n",
            "Epoch: 796 - Train Loss: 0.29608291387557983 - Test Loss: 0.3130815327167511\n",
            "Epoch: 797 - Train Loss: 0.2942183017730713 - Test Loss: 0.311231404542923\n",
            "Epoch: 798 - Train Loss: 0.2923617660999298 - Test Loss: 0.3093891739845276\n",
            "Epoch: 799 - Train Loss: 0.2905135452747345 - Test Loss: 0.3075549900531769\n",
            "Epoch: 800 - Train Loss: 0.2886735498905182 - Test Loss: 0.30572885274887085\n",
            "Epoch: 801 - Train Loss: 0.2868420481681824 - Test Loss: 0.30391091108322144\n",
            "Epoch: 802 - Train Loss: 0.28501901030540466 - Test Loss: 0.3021013140678406\n",
            "Epoch: 803 - Train Loss: 0.28320443630218506 - Test Loss: 0.3003000020980835\n",
            "Epoch: 804 - Train Loss: 0.28139856457710266 - Test Loss: 0.2985071539878845\n",
            "Epoch: 805 - Train Loss: 0.2796013653278351 - Test Loss: 0.2967226207256317\n",
            "Epoch: 806 - Train Loss: 0.2778129279613495 - Test Loss: 0.29494670033454895\n",
            "Epoch: 807 - Train Loss: 0.27603334188461304 - Test Loss: 0.29317954182624817\n",
            "Epoch: 808 - Train Loss: 0.2742626368999481 - Test Loss: 0.2914210557937622\n",
            "Epoch: 809 - Train Loss: 0.2725008726119995 - Test Loss: 0.2896711528301239\n",
            "Epoch: 810 - Train Loss: 0.27074819803237915 - Test Loss: 0.28793013095855713\n",
            "Epoch: 811 - Train Loss: 0.26900461316108704 - Test Loss: 0.28619810938835144\n",
            "Epoch: 812 - Train Loss: 0.26727011799812317 - Test Loss: 0.2844749689102173\n",
            "Epoch: 813 - Train Loss: 0.2655448615550995 - Test Loss: 0.28276076912879944\n",
            "Epoch: 814 - Train Loss: 0.2638288140296936 - Test Loss: 0.28105559945106506\n",
            "Epoch: 815 - Train Loss: 0.2621220648288727 - Test Loss: 0.2793594300746918\n",
            "Epoch: 816 - Train Loss: 0.26042473316192627 - Test Loss: 0.2776726186275482\n",
            "Epoch: 817 - Train Loss: 0.2587367594242096 - Test Loss: 0.2759949266910553\n",
            "Epoch: 818 - Train Loss: 0.2570582926273346 - Test Loss: 0.2743264436721802\n",
            "Epoch: 819 - Train Loss: 0.2553893029689789 - Test Loss: 0.2726672291755676\n",
            "Epoch: 820 - Train Loss: 0.25372985005378723 - Test Loss: 0.27101749181747437\n",
            "Epoch: 821 - Train Loss: 0.2520800232887268 - Test Loss: 0.26937711238861084\n",
            "Epoch: 822 - Train Loss: 0.2504397928714752 - Test Loss: 0.26774609088897705\n",
            "Epoch: 823 - Train Loss: 0.24880924820899963 - Test Loss: 0.26612451672554016\n",
            "Epoch: 824 - Train Loss: 0.24718840420246124 - Test Loss: 0.2645125091075897\n",
            "Epoch: 825 - Train Loss: 0.24557726085186005 - Test Loss: 0.26291000843048096\n",
            "Epoch: 826 - Train Loss: 0.24397598206996918 - Test Loss: 0.2613171637058258\n",
            "Epoch: 827 - Train Loss: 0.24238437414169312 - Test Loss: 0.25973382592201233\n",
            "Epoch: 828 - Train Loss: 0.24080276489257812 - Test Loss: 0.25816020369529724\n",
            "Epoch: 829 - Train Loss: 0.2392309606075287 - Test Loss: 0.25659623742103577\n",
            "Epoch: 830 - Train Loss: 0.23766911029815674 - Test Loss: 0.25504207611083984\n",
            "Epoch: 831 - Train Loss: 0.2361171543598175 - Test Loss: 0.25349757075309753\n",
            "Epoch: 832 - Train Loss: 0.23457515239715576 - Test Loss: 0.251962810754776\n",
            "Epoch: 833 - Train Loss: 0.23304308950901031 - Test Loss: 0.25043785572052\n",
            "Epoch: 834 - Train Loss: 0.2315211147069931 - Test Loss: 0.24892276525497437\n",
            "Epoch: 835 - Train Loss: 0.23000913858413696 - Test Loss: 0.24741753935813904\n",
            "Epoch: 836 - Train Loss: 0.22850728034973145 - Test Loss: 0.24592219293117523\n",
            "Epoch: 837 - Train Loss: 0.22701546549797058 - Test Loss: 0.24443672597408295\n",
            "Epoch: 838 - Train Loss: 0.22553370893001556 - Test Loss: 0.242961123585701\n",
            "Epoch: 839 - Train Loss: 0.2240619957447052 - Test Loss: 0.24149543046951294\n",
            "Epoch: 840 - Train Loss: 0.22260044515132904 - Test Loss: 0.24003972113132477\n",
            "Epoch: 841 - Train Loss: 0.22114905714988708 - Test Loss: 0.2385939061641693\n",
            "Epoch: 842 - Train Loss: 0.21970775723457336 - Test Loss: 0.23715811967849731\n",
            "Epoch: 843 - Train Loss: 0.21827664971351624 - Test Loss: 0.2357323169708252\n",
            "Epoch: 844 - Train Loss: 0.2168557345867157 - Test Loss: 0.23431655764579773\n",
            "Epoch: 845 - Train Loss: 0.21544502675533295 - Test Loss: 0.23291082680225372\n",
            "Epoch: 846 - Train Loss: 0.21404443681240082 - Test Loss: 0.23151499032974243\n",
            "Epoch: 847 - Train Loss: 0.21265409886837006 - Test Loss: 0.23012931644916534\n",
            "Epoch: 848 - Train Loss: 0.21127401292324066 - Test Loss: 0.2287537306547165\n",
            "Epoch: 849 - Train Loss: 0.20990407466888428 - Test Loss: 0.22738811373710632\n",
            "Epoch: 850 - Train Loss: 0.20854443311691284 - Test Loss: 0.22603259980678558\n",
            "Epoch: 851 - Train Loss: 0.20719492435455322 - Test Loss: 0.22468708455562592\n",
            "Epoch: 852 - Train Loss: 0.20585574209690094 - Test Loss: 0.22335180640220642\n",
            "Epoch: 853 - Train Loss: 0.20452673733234406 - Test Loss: 0.22202648222446442\n",
            "Epoch: 854 - Train Loss: 0.20320796966552734 - Test Loss: 0.22071126103401184\n",
            "Epoch: 855 - Train Loss: 0.20189915597438812 - Test Loss: 0.2194058746099472\n",
            "Epoch: 856 - Train Loss: 0.20060096681118011 - Test Loss: 0.21811078488826752\n",
            "Epoch: 857 - Train Loss: 0.1993122398853302 - Test Loss: 0.21682536602020264\n",
            "Epoch: 858 - Train Loss: 0.19803394377231598 - Test Loss: 0.2155500203371048\n",
            "Epoch: 859 - Train Loss: 0.19676701724529266 - Test Loss: 0.2142856866121292\n",
            "Epoch: 860 - Train Loss: 0.195510134100914 - Test Loss: 0.21303144097328186\n",
            "Epoch: 861 - Train Loss: 0.19426491856575012 - Test Loss: 0.21178855001926422\n",
            "Epoch: 862 - Train Loss: 0.19303052127361298 - Test Loss: 0.21055656671524048\n",
            "Epoch: 863 - Train Loss: 0.1918075680732727 - Test Loss: 0.20933577418327332\n",
            "Epoch: 864 - Train Loss: 0.19059565663337708 - Test Loss: 0.20812588930130005\n",
            "Epoch: 865 - Train Loss: 0.18939493596553802 - Test Loss: 0.20692704617977142\n",
            "Epoch: 866 - Train Loss: 0.18820519745349884 - Test Loss: 0.2057390809059143\n",
            "Epoch: 867 - Train Loss: 0.1870262622833252 - Test Loss: 0.20456181466579437\n",
            "Epoch: 868 - Train Loss: 0.1858581006526947 - Test Loss: 0.20339523255825043\n",
            "Epoch: 869 - Train Loss: 0.18470045924186707 - Test Loss: 0.20223906636238098\n",
            "Epoch: 870 - Train Loss: 0.18355320394039154 - Test Loss: 0.20109319686889648\n",
            "Epoch: 871 - Train Loss: 0.182416170835495 - Test Loss: 0.199957475066185\n",
            "Epoch: 872 - Train Loss: 0.18128927052021027 - Test Loss: 0.1988317370414734\n",
            "Epoch: 873 - Train Loss: 0.18017227947711945 - Test Loss: 0.19771592319011688\n",
            "Epoch: 874 - Train Loss: 0.17906509339809418 - Test Loss: 0.19660978019237518\n",
            "Epoch: 875 - Train Loss: 0.17796772718429565 - Test Loss: 0.19551332294940948\n",
            "Epoch: 876 - Train Loss: 0.17687994241714478 - Test Loss: 0.19442638754844666\n",
            "Epoch: 877 - Train Loss: 0.17580178380012512 - Test Loss: 0.1933489441871643\n",
            "Epoch: 878 - Train Loss: 0.1747330278158188 - Test Loss: 0.1922808438539505\n",
            "Epoch: 879 - Train Loss: 0.17367370426654816 - Test Loss: 0.19122202694416046\n",
            "Epoch: 880 - Train Loss: 0.1726236343383789 - Test Loss: 0.19017238914966583\n",
            "Epoch: 881 - Train Loss: 0.17158280313014984 - Test Loss: 0.18913191556930542\n",
            "Epoch: 882 - Train Loss: 0.17055098712444305 - Test Loss: 0.1881004124879837\n",
            "Epoch: 883 - Train Loss: 0.16952820122241974 - Test Loss: 0.1870778203010559\n",
            "Epoch: 884 - Train Loss: 0.16851429641246796 - Test Loss: 0.18606403470039368\n",
            "Epoch: 885 - Train Loss: 0.16750922799110413 - Test Loss: 0.18505901098251343\n",
            "Epoch: 886 - Train Loss: 0.16651281714439392 - Test Loss: 0.18406257033348083\n",
            "Epoch: 887 - Train Loss: 0.16552500426769257 - Test Loss: 0.18307463824748993\n",
            "Epoch: 888 - Train Loss: 0.1645456999540329 - Test Loss: 0.18209517002105713\n",
            "Epoch: 889 - Train Loss: 0.16357485949993134 - Test Loss: 0.18112404644489288\n",
            "Epoch: 890 - Train Loss: 0.16261228919029236 - Test Loss: 0.18016113340854645\n",
            "Epoch: 891 - Train Loss: 0.16165798902511597 - Test Loss: 0.17920641601085663\n",
            "Epoch: 892 - Train Loss: 0.16071180999279022 - Test Loss: 0.1782597452402115\n",
            "Epoch: 893 - Train Loss: 0.15977375209331512 - Test Loss: 0.1773211508989334\n",
            "Epoch: 894 - Train Loss: 0.15884365141391754 - Test Loss: 0.17639043927192688\n",
            "Epoch: 895 - Train Loss: 0.1579214632511139 - Test Loss: 0.17546755075454712\n",
            "Epoch: 896 - Train Loss: 0.15700706839561462 - Test Loss: 0.17455241084098816\n",
            "Epoch: 897 - Train Loss: 0.15610043704509735 - Test Loss: 0.17364494502544403\n",
            "Epoch: 898 - Train Loss: 0.15520144999027252 - Test Loss: 0.17274507880210876\n",
            "Epoch: 899 - Train Loss: 0.15431000292301178 - Test Loss: 0.1718526929616928\n",
            "Epoch: 900 - Train Loss: 0.15342606604099274 - Test Loss: 0.17096774280071259\n",
            "Epoch: 901 - Train Loss: 0.15254950523376465 - Test Loss: 0.17009010910987854\n",
            "Epoch: 902 - Train Loss: 0.15168027579784393 - Test Loss: 0.1692197471857071\n",
            "Epoch: 903 - Train Loss: 0.15081825852394104 - Test Loss: 0.16835655272006989\n",
            "Epoch: 904 - Train Loss: 0.14996343851089478 - Test Loss: 0.16750049591064453\n",
            "Epoch: 905 - Train Loss: 0.14911572635173798 - Test Loss: 0.16665145754814148\n",
            "Epoch: 906 - Train Loss: 0.14827503263950348 - Test Loss: 0.16580939292907715\n",
            "Epoch: 907 - Train Loss: 0.1474413275718689 - Test Loss: 0.16497430205345154\n",
            "Epoch: 908 - Train Loss: 0.1466144323348999 - Test Loss: 0.16414594650268555\n",
            "Epoch: 909 - Train Loss: 0.14579439163208008 - Test Loss: 0.16332437098026276\n",
            "Epoch: 910 - Train Loss: 0.14498107135295868 - Test Loss: 0.162509486079216\n",
            "Epoch: 911 - Train Loss: 0.14417444169521332 - Test Loss: 0.1617012470960617\n",
            "Epoch: 912 - Train Loss: 0.14337439835071564 - Test Loss: 0.16089951992034912\n",
            "Epoch: 913 - Train Loss: 0.14258088171482086 - Test Loss: 0.16010427474975586\n",
            "Epoch: 914 - Train Loss: 0.14179378747940063 - Test Loss: 0.15931545197963715\n",
            "Epoch: 915 - Train Loss: 0.14101311564445496 - Test Loss: 0.15853296220302582\n",
            "Epoch: 916 - Train Loss: 0.14023873209953308 - Test Loss: 0.15775670111179352\n",
            "Epoch: 917 - Train Loss: 0.13947060704231262 - Test Loss: 0.15698666870594025\n",
            "Epoch: 918 - Train Loss: 0.13870865106582642 - Test Loss: 0.15622279047966003\n",
            "Epoch: 919 - Train Loss: 0.13795283436775208 - Test Loss: 0.15546496212482452\n",
            "Epoch: 920 - Train Loss: 0.13720309734344482 - Test Loss: 0.15471318364143372\n",
            "Epoch: 921 - Train Loss: 0.1364593356847763 - Test Loss: 0.15396733582019806\n",
            "Epoch: 922 - Train Loss: 0.13572145998477936 - Test Loss: 0.15322737395763397\n",
            "Epoch: 923 - Train Loss: 0.13498952984809875 - Test Loss: 0.15249326825141907\n",
            "Epoch: 924 - Train Loss: 0.13426338136196136 - Test Loss: 0.1517648994922638\n",
            "Epoch: 925 - Train Loss: 0.1335429847240448 - Test Loss: 0.15104226768016815\n",
            "Epoch: 926 - Train Loss: 0.13282832503318787 - Test Loss: 0.15032534301280975\n",
            "Epoch: 927 - Train Loss: 0.13211922347545624 - Test Loss: 0.14961394667625427\n",
            "Epoch: 928 - Train Loss: 0.13141576945781708 - Test Loss: 0.1489081084728241\n",
            "Epoch: 929 - Train Loss: 0.13071778416633606 - Test Loss: 0.14820776879787445\n",
            "Epoch: 930 - Train Loss: 0.13002526760101318 - Test Loss: 0.14751280844211578\n",
            "Epoch: 931 - Train Loss: 0.12933816015720367 - Test Loss: 0.14682325720787048\n",
            "Epoch: 932 - Train Loss: 0.12865643203258514 - Test Loss: 0.14613904058933258\n",
            "Epoch: 933 - Train Loss: 0.12797997891902924 - Test Loss: 0.14546003937721252\n",
            "Epoch: 934 - Train Loss: 0.12730878591537476 - Test Loss: 0.1447863131761551\n",
            "Epoch: 935 - Train Loss: 0.12664273381233215 - Test Loss: 0.14411768317222595\n",
            "Epoch: 936 - Train Loss: 0.12598183751106262 - Test Loss: 0.1434541493654251\n",
            "Epoch: 937 - Train Loss: 0.1253260374069214 - Test Loss: 0.14279568195343018\n",
            "Epoch: 938 - Train Loss: 0.1246752068400383 - Test Loss: 0.142142191529274\n",
            "Epoch: 939 - Train Loss: 0.12402942031621933 - Test Loss: 0.14149369299411774\n",
            "Epoch: 940 - Train Loss: 0.12338852882385254 - Test Loss: 0.14085006713867188\n",
            "Epoch: 941 - Train Loss: 0.12275252491235733 - Test Loss: 0.140211284160614\n",
            "Epoch: 942 - Train Loss: 0.12212136387825012 - Test Loss: 0.13957732915878296\n",
            "Epoch: 943 - Train Loss: 0.12149496376514435 - Test Loss: 0.13894811272621155\n",
            "Epoch: 944 - Train Loss: 0.12087330222129822 - Test Loss: 0.1383236050605774\n",
            "Epoch: 945 - Train Loss: 0.12025630474090576 - Test Loss: 0.13770373165607452\n",
            "Epoch: 946 - Train Loss: 0.1196439266204834 - Test Loss: 0.13708847761154175\n",
            "Epoch: 947 - Train Loss: 0.11903614550828934 - Test Loss: 0.1364777535200119\n",
            "Epoch: 948 - Train Loss: 0.11843295395374298 - Test Loss: 0.13587161898612976\n",
            "Epoch: 949 - Train Loss: 0.11783420294523239 - Test Loss: 0.1352698802947998\n",
            "Epoch: 950 - Train Loss: 0.11723994463682175 - Test Loss: 0.1346726268529892\n",
            "Epoch: 951 - Train Loss: 0.11665008217096329 - Test Loss: 0.13407975435256958\n",
            "Epoch: 952 - Train Loss: 0.11606454849243164 - Test Loss: 0.1334911584854126\n",
            "Epoch: 953 - Train Loss: 0.11548332870006561 - Test Loss: 0.13290686905384064\n",
            "Epoch: 954 - Train Loss: 0.11490638554096222 - Test Loss: 0.1323268562555313\n",
            "Epoch: 955 - Train Loss: 0.11433367431163788 - Test Loss: 0.13175104558467865\n",
            "Epoch: 956 - Train Loss: 0.11376514285802841 - Test Loss: 0.13117936253547668\n",
            "Epoch: 957 - Train Loss: 0.11320076882839203 - Test Loss: 0.130611851811409\n",
            "Epoch: 958 - Train Loss: 0.11264052987098694 - Test Loss: 0.13004843890666962\n",
            "Epoch: 959 - Train Loss: 0.11208437383174896 - Test Loss: 0.12948909401893616\n",
            "Epoch: 960 - Train Loss: 0.11153217405080795 - Test Loss: 0.12893369793891907\n",
            "Epoch: 961 - Train Loss: 0.11098401993513107 - Test Loss: 0.12838232517242432\n",
            "Epoch: 962 - Train Loss: 0.11043976992368698 - Test Loss: 0.12783482670783997\n",
            "Epoch: 963 - Train Loss: 0.10989944636821747 - Test Loss: 0.1272912472486496\n",
            "Epoch: 964 - Train Loss: 0.10936300456523895 - Test Loss: 0.12675152719020844\n",
            "Epoch: 965 - Train Loss: 0.10883037745952606 - Test Loss: 0.1262156069278717\n",
            "Epoch: 966 - Train Loss: 0.1083015501499176 - Test Loss: 0.1256834715604782\n",
            "Epoch: 967 - Train Loss: 0.1077764630317688 - Test Loss: 0.12515504658222198\n",
            "Epoch: 968 - Train Loss: 0.10725510120391846 - Test Loss: 0.12463035434484482\n",
            "Epoch: 969 - Train Loss: 0.1067374050617218 - Test Loss: 0.12410929054021835\n",
            "Epoch: 970 - Train Loss: 0.10622338205575943 - Test Loss: 0.12359190732240677\n",
            "Epoch: 971 - Train Loss: 0.10571293532848358 - Test Loss: 0.12307805567979813\n",
            "Epoch: 972 - Train Loss: 0.10520608723163605 - Test Loss: 0.12256781756877899\n",
            "Epoch: 973 - Train Loss: 0.10470277816057205 - Test Loss: 0.12206108868122101\n",
            "Epoch: 974 - Train Loss: 0.10420295596122742 - Test Loss: 0.12155784666538239\n",
            "Epoch: 975 - Train Loss: 0.10370665043592453 - Test Loss: 0.12105809897184372\n",
            "Epoch: 976 - Train Loss: 0.10321374237537384 - Test Loss: 0.12056174129247665\n",
            "Epoch: 977 - Train Loss: 0.10272426903247833 - Test Loss: 0.12006878107786179\n",
            "Epoch: 978 - Train Loss: 0.10223811864852905 - Test Loss: 0.11957915127277374\n",
            "Epoch: 979 - Train Loss: 0.10175532102584839 - Test Loss: 0.11909285932779312\n",
            "Epoch: 980 - Train Loss: 0.10127582401037216 - Test Loss: 0.11860985308885574\n",
            "Epoch: 981 - Train Loss: 0.10079962760210037 - Test Loss: 0.1181301400065422\n",
            "Epoch: 982 - Train Loss: 0.10032663494348526 - Test Loss: 0.11765363812446594\n",
            "Epoch: 983 - Train Loss: 0.099856898188591 - Test Loss: 0.11718035489320755\n",
            "Epoch: 984 - Train Loss: 0.09939032793045044 - Test Loss: 0.11671023815870285\n",
            "Epoch: 985 - Train Loss: 0.09892692416906357 - Test Loss: 0.11624325811862946\n",
            "Epoch: 986 - Train Loss: 0.09846661984920502 - Test Loss: 0.11577940732240677\n",
            "Epoch: 987 - Train Loss: 0.09800942242145538 - Test Loss: 0.11531861871480942\n",
            "Epoch: 988 - Train Loss: 0.09755527973175049 - Test Loss: 0.1148608922958374\n",
            "Epoch: 989 - Train Loss: 0.09710415452718735 - Test Loss: 0.11440616101026535\n",
            "Epoch: 990 - Train Loss: 0.09665605425834656 - Test Loss: 0.11395446211099625\n",
            "Epoch: 991 - Train Loss: 0.09621092677116394 - Test Loss: 0.11350570619106293\n",
            "Epoch: 992 - Train Loss: 0.09576874226331711 - Test Loss: 0.1130598932504654\n",
            "Epoch: 993 - Train Loss: 0.09532947838306427 - Test Loss: 0.11261700838804245\n",
            "Epoch: 994 - Train Loss: 0.09489309787750244 - Test Loss: 0.11217697709798813\n",
            "Epoch: 995 - Train Loss: 0.09445958584547043 - Test Loss: 0.11173982918262482\n",
            "Epoch: 996 - Train Loss: 0.09402891993522644 - Test Loss: 0.11130549013614655\n",
            "Epoch: 997 - Train Loss: 0.0936010479927063 - Test Loss: 0.11087395995855331\n",
            "Epoch: 998 - Train Loss: 0.09317599982023239 - Test Loss: 0.11044523119926453\n",
            "Epoch: 999 - Train Loss: 0.09275365620851517 - Test Loss: 0.11001922190189362\n",
            "Epoch: 1000 - Train Loss: 0.09233409911394119 - Test Loss: 0.10959596931934357\n",
            "Epoch: 1001 - Train Loss: 0.0919172465801239 - Test Loss: 0.10917540639638901\n",
            "Epoch: 1002 - Train Loss: 0.09150303900241852 - Test Loss: 0.10875753313302994\n",
            "Epoch: 1003 - Train Loss: 0.09109149128198624 - Test Loss: 0.10834226757287979\n",
            "Epoch: 1004 - Train Loss: 0.09068258851766586 - Test Loss: 0.10792965441942215\n",
            "Epoch: 1005 - Train Loss: 0.09027627855539322 - Test Loss: 0.10751963406801224\n",
            "Epoch: 1006 - Train Loss: 0.0898725762963295 - Test Loss: 0.10711219906806946\n",
            "Epoch: 1007 - Train Loss: 0.08947142958641052 - Test Loss: 0.10670731216669083\n",
            "Epoch: 1008 - Train Loss: 0.0890728011727333 - Test Loss: 0.10630496591329575\n",
            "Epoch: 1009 - Train Loss: 0.08867674320936203 - Test Loss: 0.10590516030788422\n",
            "Epoch: 1010 - Train Loss: 0.08828313648700714 - Test Loss: 0.10550781339406967\n",
            "Epoch: 1011 - Train Loss: 0.08789204061031342 - Test Loss: 0.1051129475235939\n",
            "Epoch: 1012 - Train Loss: 0.08750332146883011 - Test Loss: 0.10472049564123154\n",
            "Epoch: 1013 - Train Loss: 0.08711704611778259 - Test Loss: 0.10433045029640198\n",
            "Epoch: 1014 - Train Loss: 0.08673319220542908 - Test Loss: 0.10394283384084702\n",
            "Epoch: 1015 - Train Loss: 0.08635172247886658 - Test Loss: 0.10355760902166367\n",
            "Epoch: 1016 - Train Loss: 0.08597259968519211 - Test Loss: 0.10317470133304596\n",
            "Epoch: 1017 - Train Loss: 0.08559582382440567 - Test Loss: 0.10279414802789688\n",
            "Epoch: 1018 - Train Loss: 0.08522133529186249 - Test Loss: 0.10241588205099106\n",
            "Epoch: 1019 - Train Loss: 0.08484915643930435 - Test Loss: 0.10203992575407028\n",
            "Epoch: 1020 - Train Loss: 0.08447927981615067 - Test Loss: 0.10166626423597336\n",
            "Epoch: 1021 - Train Loss: 0.08411160856485367 - Test Loss: 0.10129479318857193\n",
            "Epoch: 1022 - Train Loss: 0.08374617248773575 - Test Loss: 0.10092557966709137\n",
            "Epoch: 1023 - Train Loss: 0.0833829939365387 - Test Loss: 0.10055858641862869\n",
            "Epoch: 1024 - Train Loss: 0.08302199840545654 - Test Loss: 0.1001937985420227\n",
            "Epoch: 1025 - Train Loss: 0.0826631560921669 - Test Loss: 0.09983114898204803\n",
            "Epoch: 1026 - Train Loss: 0.08230649679899216 - Test Loss: 0.09947068244218826\n",
            "Epoch: 1027 - Train Loss: 0.08195196092128754 - Test Loss: 0.09911234676837921\n",
            "Epoch: 1028 - Train Loss: 0.08159954100847244 - Test Loss: 0.09875611960887909\n",
            "Epoch: 1029 - Train Loss: 0.08124922215938568 - Test Loss: 0.0984019786119461\n",
            "Epoch: 1030 - Train Loss: 0.08090096712112427 - Test Loss: 0.09804990887641907\n",
            "Epoch: 1031 - Train Loss: 0.0805547833442688 - Test Loss: 0.09769990295171738\n",
            "Epoch: 1032 - Train Loss: 0.08021067827939987 - Test Loss: 0.09735198318958282\n",
            "Epoch: 1033 - Train Loss: 0.07986856997013092 - Test Loss: 0.09700603783130646\n",
            "Epoch: 1034 - Train Loss: 0.07952846586704254 - Test Loss: 0.09666211903095245\n",
            "Epoch: 1035 - Train Loss: 0.07919035851955414 - Test Loss: 0.09632018208503723\n",
            "Epoch: 1036 - Train Loss: 0.07885423302650452 - Test Loss: 0.09598023444414139\n",
            "Epoch: 1037 - Train Loss: 0.0785200372338295 - Test Loss: 0.09564220905303955\n",
            "Epoch: 1038 - Train Loss: 0.07818779349327087 - Test Loss: 0.09530612826347351\n",
            "Epoch: 1039 - Train Loss: 0.07785750925540924 - Test Loss: 0.09497200697660446\n",
            "Epoch: 1040 - Train Loss: 0.07752909511327744 - Test Loss: 0.09463976323604584\n",
            "Epoch: 1041 - Train Loss: 0.07720258831977844 - Test Loss: 0.09430942684412003\n",
            "Epoch: 1042 - Train Loss: 0.07687793672084808 - Test Loss: 0.09398093074560165\n",
            "Epoch: 1043 - Train Loss: 0.07655516266822815 - Test Loss: 0.0936543196439743\n",
            "Epoch: 1044 - Train Loss: 0.07623421400785446 - Test Loss: 0.093329519033432\n",
            "Epoch: 1045 - Train Loss: 0.07591510564088821 - Test Loss: 0.09300658851861954\n",
            "Epoch: 1046 - Train Loss: 0.07559780031442642 - Test Loss: 0.09268543124198914\n",
            "Epoch: 1047 - Train Loss: 0.07528230547904968 - Test Loss: 0.09236610680818558\n",
            "Epoch: 1048 - Train Loss: 0.07496859133243561 - Test Loss: 0.0920485407114029\n",
            "Epoch: 1049 - Train Loss: 0.0746566504240036 - Test Loss: 0.09173274785280228\n",
            "Epoch: 1050 - Train Loss: 0.07434644550085068 - Test Loss: 0.09141869843006134\n",
            "Epoch: 1051 - Train Loss: 0.07403796911239624 - Test Loss: 0.09110637754201889\n",
            "Epoch: 1052 - Train Loss: 0.07373122125864029 - Test Loss: 0.09079579263925552\n",
            "Epoch: 1053 - Train Loss: 0.07342617958784103 - Test Loss: 0.09048691391944885\n",
            "Epoch: 1054 - Train Loss: 0.07312285155057907 - Test Loss: 0.09017972648143768\n",
            "Epoch: 1055 - Train Loss: 0.07282119244337082 - Test Loss: 0.08987422287464142\n",
            "Epoch: 1056 - Train Loss: 0.07252119481563568 - Test Loss: 0.08957036584615707\n",
            "Epoch: 1057 - Train Loss: 0.07222286611795425 - Test Loss: 0.08926820755004883\n",
            "Epoch: 1058 - Train Loss: 0.07192617654800415 - Test Loss: 0.08896767348051071\n",
            "Epoch: 1059 - Train Loss: 0.07163108885288239 - Test Loss: 0.08866873383522034\n",
            "Epoch: 1060 - Train Loss: 0.07133761793375015 - Test Loss: 0.0883714109659195\n",
            "Epoch: 1061 - Train Loss: 0.07104572653770447 - Test Loss: 0.0880756825208664\n",
            "Epoch: 1062 - Train Loss: 0.07075544446706772 - Test Loss: 0.08778154104948044\n",
            "Epoch: 1063 - Train Loss: 0.07046676427125931 - Test Loss: 0.08748901635408401\n",
            "Epoch: 1064 - Train Loss: 0.07017961889505386 - Test Loss: 0.08719802647829056\n",
            "Epoch: 1065 - Train Loss: 0.06989400088787079 - Test Loss: 0.08690855652093887\n",
            "Epoch: 1066 - Train Loss: 0.06960994005203247 - Test Loss: 0.08662064373493195\n",
            "Epoch: 1067 - Train Loss: 0.06932739913463593 - Test Loss: 0.08633426576852798\n",
            "Epoch: 1068 - Train Loss: 0.06904637813568115 - Test Loss: 0.08604937791824341\n",
            "Epoch: 1069 - Train Loss: 0.06876685470342636 - Test Loss: 0.0857660248875618\n",
            "Epoch: 1070 - Train Loss: 0.06848882138729095 - Test Loss: 0.08548413962125778\n",
            "Epoch: 1071 - Train Loss: 0.06821225583553314 - Test Loss: 0.08520373702049255\n",
            "Epoch: 1072 - Train Loss: 0.06793714314699173 - Test Loss: 0.08492478728294373\n",
            "Epoch: 1073 - Train Loss: 0.06766347587108612 - Test Loss: 0.08464726060628891\n",
            "Epoch: 1074 - Train Loss: 0.0673912763595581 - Test Loss: 0.08437123149633408\n",
            "Epoch: 1075 - Train Loss: 0.06712047010660172 - Test Loss: 0.08409658074378967\n",
            "Epoch: 1076 - Train Loss: 0.06685110926628113 - Test Loss: 0.08382337540388107\n",
            "Epoch: 1077 - Train Loss: 0.06658315658569336 - Test Loss: 0.0835515707731247\n",
            "Epoch: 1078 - Train Loss: 0.06631660461425781 - Test Loss: 0.08328118920326233\n",
            "Epoch: 1079 - Train Loss: 0.0660514235496521 - Test Loss: 0.0830121710896492\n",
            "Epoch: 1080 - Train Loss: 0.06578762084245682 - Test Loss: 0.0827445238828659\n",
            "Epoch: 1081 - Train Loss: 0.06552516669034958 - Test Loss: 0.08247823268175125\n",
            "Epoch: 1082 - Train Loss: 0.06526407599449158 - Test Loss: 0.08221330493688583\n",
            "Epoch: 1083 - Train Loss: 0.06500432640314102 - Test Loss: 0.08194972574710846\n",
            "Epoch: 1084 - Train Loss: 0.06474592536687851 - Test Loss: 0.08168748766183853\n",
            "Epoch: 1085 - Train Loss: 0.06448885053396225 - Test Loss: 0.08142657577991486\n",
            "Epoch: 1086 - Train Loss: 0.06423305720090866 - Test Loss: 0.08116695284843445\n",
            "Epoch: 1087 - Train Loss: 0.06397858262062073 - Test Loss: 0.0809086412191391\n",
            "Epoch: 1088 - Train Loss: 0.06372540444135666 - Test Loss: 0.08065163344144821\n",
            "Epoch: 1089 - Train Loss: 0.06347351521253586 - Test Loss: 0.08039591461420059\n",
            "Epoch: 1090 - Train Loss: 0.06322287768125534 - Test Loss: 0.08014144003391266\n",
            "Epoch: 1091 - Train Loss: 0.0629734992980957 - Test Loss: 0.0798882395029068\n",
            "Epoch: 1092 - Train Loss: 0.06272538751363754 - Test Loss: 0.07963629066944122\n",
            "Epoch: 1093 - Train Loss: 0.06247851625084877 - Test Loss: 0.07938560098409653\n",
            "Epoch: 1094 - Train Loss: 0.062232863157987595 - Test Loss: 0.07913611084222794\n",
            "Epoch: 1095 - Train Loss: 0.06198844313621521 - Test Loss: 0.07888787984848022\n",
            "Epoch: 1096 - Train Loss: 0.061745259910821915 - Test Loss: 0.078640878200531\n",
            "Epoch: 1097 - Train Loss: 0.06150326505303383 - Test Loss: 0.0783950611948967\n",
            "Epoch: 1098 - Train Loss: 0.06126248463988304 - Test Loss: 0.07815045118331909\n",
            "Epoch: 1099 - Train Loss: 0.06102287396788597 - Test Loss: 0.0779070258140564\n",
            "Epoch: 1100 - Train Loss: 0.060784440487623215 - Test Loss: 0.07766477018594742\n",
            "Epoch: 1101 - Train Loss: 0.060547199100255966 - Test Loss: 0.07742370665073395\n",
            "Epoch: 1102 - Train Loss: 0.06031111255288124 - Test Loss: 0.0771838128566742\n",
            "Epoch: 1103 - Train Loss: 0.06007619947195053 - Test Loss: 0.07694508880376816\n",
            "Epoch: 1104 - Train Loss: 0.05984240397810936 - Test Loss: 0.07670747488737106\n",
            "Epoch: 1105 - Train Loss: 0.05960976704955101 - Test Loss: 0.07647103071212769\n",
            "Epoch: 1106 - Train Loss: 0.0593782439827919 - Test Loss: 0.07623569667339325\n",
            "Epoch: 1107 - Train Loss: 0.05914784595370293 - Test Loss: 0.07600148022174835\n",
            "Epoch: 1108 - Train Loss: 0.05891856551170349 - Test Loss: 0.07576839625835419\n",
            "Epoch: 1109 - Train Loss: 0.0586903914809227 - Test Loss: 0.07553642243146896\n",
            "Epoch: 1110 - Train Loss: 0.05846332758665085 - Test Loss: 0.07530555874109268\n",
            "Epoch: 1111 - Train Loss: 0.05823735147714615 - Test Loss: 0.07507577538490295\n",
            "Epoch: 1112 - Train Loss: 0.0580124594271183 - Test Loss: 0.07484707981348038\n",
            "Epoch: 1113 - Train Loss: 0.057788629084825516 - Test Loss: 0.07461944967508316\n",
            "Epoch: 1114 - Train Loss: 0.05756586790084839 - Test Loss: 0.0743928775191307\n",
            "Epoch: 1115 - Train Loss: 0.05734415352344513 - Test Loss: 0.07416737824678421\n",
            "Epoch: 1116 - Train Loss: 0.057123519480228424 - Test Loss: 0.07394294440746307\n",
            "Epoch: 1117 - Train Loss: 0.05690392106771469 - Test Loss: 0.07371953874826431\n",
            "Epoch: 1118 - Train Loss: 0.05668536201119423 - Test Loss: 0.07349719852209091\n",
            "Epoch: 1119 - Train Loss: 0.056467827409505844 - Test Loss: 0.0732758641242981\n",
            "Epoch: 1120 - Train Loss: 0.05625132843852043 - Test Loss: 0.07305557280778885\n",
            "Epoch: 1121 - Train Loss: 0.0560358464717865 - Test Loss: 0.07283630222082138\n",
            "Epoch: 1122 - Train Loss: 0.05582134798169136 - Test Loss: 0.0726180151104927\n",
            "Epoch: 1123 - Train Loss: 0.055607881397008896 - Test Loss: 0.072400763630867\n",
            "Epoch: 1124 - Train Loss: 0.05539538711309433 - Test Loss: 0.0721844807267189\n",
            "Epoch: 1125 - Train Loss: 0.055183906108140945 - Test Loss: 0.07196922600269318\n",
            "Epoch: 1126 - Train Loss: 0.05497339367866516 - Test Loss: 0.07175492495298386\n",
            "Epoch: 1127 - Train Loss: 0.054763857275247574 - Test Loss: 0.07154161483049393\n",
            "Epoch: 1128 - Train Loss: 0.054555293172597885 - Test Loss: 0.0713292732834816\n",
            "Epoch: 1129 - Train Loss: 0.05434770509600639 - Test Loss: 0.07111790776252747\n",
            "Epoch: 1130 - Train Loss: 0.054141055792570114 - Test Loss: 0.07090747356414795\n",
            "Epoch: 1131 - Train Loss: 0.053935352712869644 - Test Loss: 0.07069800794124603\n",
            "Epoch: 1132 - Train Loss: 0.05373060703277588 - Test Loss: 0.07048948109149933\n",
            "Epoch: 1133 - Train Loss: 0.053526800125837326 - Test Loss: 0.07028190046548843\n",
            "Epoch: 1134 - Train Loss: 0.05332392454147339 - Test Loss: 0.07007525861263275\n",
            "Epoch: 1135 - Train Loss: 0.05312196910381317 - Test Loss: 0.0698695257306099\n",
            "Epoch: 1136 - Train Loss: 0.052920930087566376 - Test Loss: 0.06966472417116165\n",
            "Epoch: 1137 - Train Loss: 0.052720796316862106 - Test Loss: 0.06946083158254623\n",
            "Epoch: 1138 - Train Loss: 0.05252157896757126 - Test Loss: 0.06925784796476364\n",
            "Epoch: 1139 - Train Loss: 0.052323248237371445 - Test Loss: 0.06905575096607208\n",
            "Epoch: 1140 - Train Loss: 0.052125826478004456 - Test Loss: 0.06885457038879395\n",
            "Epoch: 1141 - Train Loss: 0.0519292838871479 - Test Loss: 0.06865426152944565\n",
            "Epoch: 1142 - Train Loss: 0.051733631640672684 - Test Loss: 0.06845486164093018\n",
            "Epoch: 1143 - Train Loss: 0.0515388585627079 - Test Loss: 0.06825631856918335\n",
            "Epoch: 1144 - Train Loss: 0.051344964653253555 - Test Loss: 0.06805867701768875\n",
            "Epoch: 1145 - Train Loss: 0.051151927560567856 - Test Loss: 0.0678618848323822\n",
            "Epoch: 1146 - Train Loss: 0.050959765911102295 - Test Loss: 0.06766597181558609\n",
            "Epoch: 1147 - Train Loss: 0.050768449902534485 - Test Loss: 0.06747091561555862\n",
            "Epoch: 1148 - Train Loss: 0.05057799071073532 - Test Loss: 0.06727669388055801\n",
            "Epoch: 1149 - Train Loss: 0.050388362258672714 - Test Loss: 0.06708332896232605\n",
            "Epoch: 1150 - Train Loss: 0.050199586898088455 - Test Loss: 0.06689080595970154\n",
            "Epoch: 1151 - Train Loss: 0.050011638551950455 - Test Loss: 0.06669911742210388\n",
            "Epoch: 1152 - Train Loss: 0.04982452094554901 - Test Loss: 0.06650825589895248\n",
            "Epoch: 1153 - Train Loss: 0.049638234078884125 - Test Loss: 0.06631822884082794\n",
            "Epoch: 1154 - Train Loss: 0.049452755600214005 - Test Loss: 0.06612902134656906\n",
            "Epoch: 1155 - Train Loss: 0.04926808923482895 - Test Loss: 0.06594061106443405\n",
            "Epoch: 1156 - Train Loss: 0.049084242433309555 - Test Loss: 0.0657530277967453\n",
            "Epoch: 1157 - Train Loss: 0.048901185393333435 - Test Loss: 0.06556624174118042\n",
            "Epoch: 1158 - Train Loss: 0.04871894791722298 - Test Loss: 0.0653802677989006\n",
            "Epoch: 1159 - Train Loss: 0.048537496477365494 - Test Loss: 0.06519508361816406\n",
            "Epoch: 1160 - Train Loss: 0.04835681989789009 - Test Loss: 0.0650106817483902\n",
            "Epoch: 1161 - Train Loss: 0.048176925629377365 - Test Loss: 0.06482706218957901\n",
            "Epoch: 1162 - Train Loss: 0.04799782484769821 - Test Loss: 0.06464424729347229\n",
            "Epoch: 1163 - Train Loss: 0.04781949520111084 - Test Loss: 0.06446219235658646\n",
            "Epoch: 1164 - Train Loss: 0.047641921788454056 - Test Loss: 0.06428088992834091\n",
            "Epoch: 1165 - Train Loss: 0.04746513068675995 - Test Loss: 0.06410037726163864\n",
            "Epoch: 1166 - Train Loss: 0.047289103269577026 - Test Loss: 0.06392063200473785\n",
            "Epoch: 1167 - Train Loss: 0.047113820910453796 - Test Loss: 0.06374162435531616\n",
            "Epoch: 1168 - Train Loss: 0.046939294785261154 - Test Loss: 0.06356339156627655\n",
            "Epoch: 1169 - Train Loss: 0.046765509992837906 - Test Loss: 0.06338588893413544\n",
            "Epoch: 1170 - Train Loss: 0.046592459082603455 - Test Loss: 0.06320912390947342\n",
            "Epoch: 1171 - Train Loss: 0.04642016813158989 - Test Loss: 0.06303312629461288\n",
            "Epoch: 1172 - Train Loss: 0.046248603612184525 - Test Loss: 0.06285785883665085\n",
            "Epoch: 1173 - Train Loss: 0.04607776552438736 - Test Loss: 0.06268331408500671\n",
            "Epoch: 1174 - Train Loss: 0.04590766131877899 - Test Loss: 0.06250949949026108\n",
            "Epoch: 1175 - Train Loss: 0.04573826491832733 - Test Loss: 0.062336403876543045\n",
            "Epoch: 1176 - Train Loss: 0.04556959867477417 - Test Loss: 0.062164027243852615\n",
            "Epoch: 1177 - Train Loss: 0.04540165141224861 - Test Loss: 0.061992380768060684\n",
            "Epoch: 1178 - Train Loss: 0.04523438960313797 - Test Loss: 0.06182142347097397\n",
            "Epoch: 1179 - Train Loss: 0.04506782442331314 - Test Loss: 0.061651162803173065\n",
            "Epoch: 1180 - Train Loss: 0.04490198194980621 - Test Loss: 0.06148161366581917\n",
            "Epoch: 1181 - Train Loss: 0.0447368286550045 - Test Loss: 0.06131276860833168\n",
            "Epoch: 1182 - Train Loss: 0.0445723719894886 - Test Loss: 0.06114461272954941\n",
            "Epoch: 1183 - Train Loss: 0.04440861940383911 - Test Loss: 0.06097717583179474\n",
            "Epoch: 1184 - Train Loss: 0.04424551501870155 - Test Loss: 0.060810383409261703\n",
            "Epoch: 1185 - Train Loss: 0.044083114713430405 - Test Loss: 0.060644298791885376\n",
            "Epoch: 1186 - Train Loss: 0.043921396136283875 - Test Loss: 0.06047889217734337\n",
            "Epoch: 1187 - Train Loss: 0.04376034066081047 - Test Loss: 0.06031416356563568\n",
            "Epoch: 1188 - Train Loss: 0.043599966913461685 - Test Loss: 0.06015009805560112\n",
            "Epoch: 1189 - Train Loss: 0.04344024881720543 - Test Loss: 0.05998671054840088\n",
            "Epoch: 1190 - Train Loss: 0.04328121244907379 - Test Loss: 0.05982399359345436\n",
            "Epoch: 1191 - Train Loss: 0.043122805655002594 - Test Loss: 0.05966191366314888\n",
            "Epoch: 1192 - Train Loss: 0.04296506941318512 - Test Loss: 0.059500500559806824\n",
            "Epoch: 1193 - Train Loss: 0.04280797764658928 - Test Loss: 0.0593397431075573\n",
            "Epoch: 1194 - Train Loss: 0.042651545256376266 - Test Loss: 0.05917963758111\n",
            "Epoch: 1195 - Train Loss: 0.0424957349896431 - Test Loss: 0.059020161628723145\n",
            "Epoch: 1196 - Train Loss: 0.04234058037400246 - Test Loss: 0.058861326426267624\n",
            "Epoch: 1197 - Train Loss: 0.042186055332422256 - Test Loss: 0.058703139424324036\n",
            "Epoch: 1198 - Train Loss: 0.04203217849135399 - Test Loss: 0.05854560807347298\n",
            "Epoch: 1199 - Train Loss: 0.04187891259789467 - Test Loss: 0.05838867276906967\n",
            "Epoch: 1200 - Train Loss: 0.04172626882791519 - Test Loss: 0.058232370764017105\n",
            "Epoch: 1201 - Train Loss: 0.04157426208257675 - Test Loss: 0.058076705783605576\n",
            "Epoch: 1202 - Train Loss: 0.04142286628484726 - Test Loss: 0.057921648025512695\n",
            "Epoch: 1203 - Train Loss: 0.041272085160017014 - Test Loss: 0.05776720866560936\n",
            "Epoch: 1204 - Train Loss: 0.04112192988395691 - Test Loss: 0.05761340633034706\n",
            "Epoch: 1205 - Train Loss: 0.040972381830215454 - Test Loss: 0.057460203766822815\n",
            "Epoch: 1206 - Train Loss: 0.040823422372341156 - Test Loss: 0.05730760097503662\n",
            "Epoch: 1207 - Train Loss: 0.04067506641149521 - Test Loss: 0.05715559422969818\n",
            "Epoch: 1208 - Train Loss: 0.0405273400247097 - Test Loss: 0.05700422078371048\n",
            "Epoch: 1209 - Train Loss: 0.04038017988204956 - Test Loss: 0.056853413581848145\n",
            "Epoch: 1210 - Train Loss: 0.040233634412288666 - Test Loss: 0.05670322850346565\n",
            "Epoch: 1211 - Train Loss: 0.040087658911943436 - Test Loss: 0.056553613394498825\n",
            "Epoch: 1212 - Train Loss: 0.039942286908626556 - Test Loss: 0.05640460178256035\n",
            "Epoch: 1213 - Train Loss: 0.039797477424144745 - Test Loss: 0.05625614523887634\n",
            "Epoch: 1214 - Train Loss: 0.039653267711400986 - Test Loss: 0.05610830336809158\n",
            "Epoch: 1215 - Train Loss: 0.039509616792201996 - Test Loss: 0.055961012840270996\n",
            "Epoch: 1216 - Train Loss: 0.03936655819416046 - Test Loss: 0.05581432580947876\n",
            "Epoch: 1217 - Train Loss: 0.0392240546643734 - Test Loss: 0.0556681826710701\n",
            "Epoch: 1218 - Train Loss: 0.03908213973045349 - Test Loss: 0.05552263557910919\n",
            "Epoch: 1219 - Train Loss: 0.03894076496362686 - Test Loss: 0.05537763610482216\n",
            "Epoch: 1220 - Train Loss: 0.03879997879266739 - Test Loss: 0.05523322895169258\n",
            "Epoch: 1221 - Train Loss: 0.03865973651409149 - Test Loss: 0.055089354515075684\n",
            "Epoch: 1222 - Train Loss: 0.03852006793022156 - Test Loss: 0.05494605749845505\n",
            "Epoch: 1223 - Train Loss: 0.038380932062864304 - Test Loss: 0.054803311824798584\n",
            "Epoch: 1224 - Train Loss: 0.03824237361550331 - Test Loss: 0.054661136120557785\n",
            "Epoch: 1225 - Train Loss: 0.038104347884655 - Test Loss: 0.05451948940753937\n",
            "Epoch: 1226 - Train Loss: 0.03796686977148056 - Test Loss: 0.05437839776277542\n",
            "Epoch: 1227 - Train Loss: 0.037829939275979996 - Test Loss: 0.05423784628510475\n",
            "Epoch: 1228 - Train Loss: 0.03769353777170181 - Test Loss: 0.05409783497452736\n",
            "Epoch: 1229 - Train Loss: 0.037557680159807205 - Test Loss: 0.05395837128162384\n",
            "Epoch: 1230 - Train Loss: 0.03742237016558647 - Test Loss: 0.0538194514811039\n",
            "Epoch: 1231 - Train Loss: 0.037287577986717224 - Test Loss: 0.05368104949593544\n",
            "Epoch: 1232 - Train Loss: 0.03715331479907036 - Test Loss: 0.05354318767786026\n",
            "Epoch: 1233 - Train Loss: 0.03701959177851677 - Test Loss: 0.05340585857629776\n",
            "Epoch: 1234 - Train Loss: 0.03688637539744377 - Test Loss: 0.05326903238892555\n",
            "Epoch: 1235 - Train Loss: 0.03675369173288345 - Test Loss: 0.05313275754451752\n",
            "Epoch: 1236 - Train Loss: 0.03662153333425522 - Test Loss: 0.05299700051546097\n",
            "Epoch: 1237 - Train Loss: 0.03648987039923668 - Test Loss: 0.05286174640059471\n",
            "Epoch: 1238 - Train Loss: 0.036358751356601715 - Test Loss: 0.052727021276950836\n",
            "Epoch: 1239 - Train Loss: 0.03622811660170555 - Test Loss: 0.052592795342206955\n",
            "Epoch: 1240 - Train Loss: 0.036097992211580276 - Test Loss: 0.05245908722281456\n",
            "Epoch: 1241 - Train Loss: 0.035968393087387085 - Test Loss: 0.05232589691877365\n",
            "Epoch: 1242 - Train Loss: 0.03583928942680359 - Test Loss: 0.052193205803632736\n",
            "Epoch: 1243 - Train Loss: 0.035710692405700684 - Test Loss: 0.05206102877855301\n",
            "Epoch: 1244 - Train Loss: 0.03558257967233658 - Test Loss: 0.05192933231592178\n",
            "Epoch: 1245 - Train Loss: 0.035454973578453064 - Test Loss: 0.05179814621806145\n",
            "Epoch: 1246 - Train Loss: 0.03532785177230835 - Test Loss: 0.05166744440793991\n",
            "Epoch: 1247 - Train Loss: 0.035201236605644226 - Test Loss: 0.05153724551200867\n",
            "Epoch: 1248 - Train Loss: 0.0350751094520092 - Test Loss: 0.051407549530267715\n",
            "Epoch: 1249 - Train Loss: 0.034949466586112976 - Test Loss: 0.05127833038568497\n",
            "Epoch: 1250 - Train Loss: 0.03482430800795555 - Test Loss: 0.051149602979421616\n",
            "Epoch: 1251 - Train Loss: 0.03469962626695633 - Test Loss: 0.05102135241031647\n",
            "Epoch: 1252 - Train Loss: 0.03457542508840561 - Test Loss: 0.05089358240365982\n",
            "Epoch: 1253 - Train Loss: 0.034451715648174286 - Test Loss: 0.05076630041003227\n",
            "Epoch: 1254 - Train Loss: 0.03432847559452057 - Test Loss: 0.050639502704143524\n",
            "Epoch: 1255 - Train Loss: 0.03420570120215416 - Test Loss: 0.050513170659542084\n",
            "Epoch: 1256 - Train Loss: 0.03408340737223625 - Test Loss: 0.05038731172680855\n",
            "Epoch: 1257 - Train Loss: 0.03396157547831535 - Test Loss: 0.05026192590594292\n",
            "Epoch: 1258 - Train Loss: 0.03384021669626236 - Test Loss: 0.05013700947165489\n",
            "Epoch: 1259 - Train Loss: 0.03371931239962578 - Test Loss: 0.050012554973363876\n",
            "Epoch: 1260 - Train Loss: 0.033598873764276505 - Test Loss: 0.04988855868577957\n",
            "Epoch: 1261 - Train Loss: 0.03347890079021454 - Test Loss: 0.04976503551006317\n",
            "Epoch: 1262 - Train Loss: 0.03335938602685928 - Test Loss: 0.049641985446214676\n",
            "Epoch: 1263 - Train Loss: 0.03324032947421074 - Test Loss: 0.0495193749666214\n",
            "Epoch: 1264 - Train Loss: 0.033121731132268906 - Test Loss: 0.04939722642302513\n",
            "Epoch: 1265 - Train Loss: 0.03300357237458229 - Test Loss: 0.049275532364845276\n",
            "Epoch: 1266 - Train Loss: 0.032885853201150894 - Test Loss: 0.04915427789092064\n",
            "Epoch: 1267 - Train Loss: 0.032768599689006805 - Test Loss: 0.04903348162770271\n",
            "Epoch: 1268 - Train Loss: 0.03265180066227913 - Test Loss: 0.048913151025772095\n",
            "Epoch: 1269 - Train Loss: 0.032535430043935776 - Test Loss: 0.0487932451069355\n",
            "Epoch: 1270 - Train Loss: 0.032419510185718536 - Test Loss: 0.04867379367351532\n",
            "Epoch: 1271 - Train Loss: 0.03230402618646622 - Test Loss: 0.048554785549640656\n",
            "Epoch: 1272 - Train Loss: 0.03218897059559822 - Test Loss: 0.04843619838356972\n",
            "Epoch: 1273 - Train Loss: 0.03207436203956604 - Test Loss: 0.04831806570291519\n",
            "Epoch: 1274 - Train Loss: 0.03196018561720848 - Test Loss: 0.04820036515593529\n",
            "Epoch: 1275 - Train Loss: 0.031846433877944946 - Test Loss: 0.048083096742630005\n",
            "Epoch: 1276 - Train Loss: 0.03173312172293663 - Test Loss: 0.04796626791357994\n",
            "Epoch: 1277 - Train Loss: 0.03162023052573204 - Test Loss: 0.0478498600423336\n",
            "Epoch: 1278 - Train Loss: 0.031507767736911774 - Test Loss: 0.04773388430476189\n",
            "Epoch: 1279 - Train Loss: 0.03139573335647583 - Test Loss: 0.04761833697557449\n",
            "Epoch: 1280 - Train Loss: 0.03128410503268242 - Test Loss: 0.04750319942831993\n",
            "Epoch: 1281 - Train Loss: 0.031172899529337883 - Test Loss: 0.047388482838869095\n",
            "Epoch: 1282 - Train Loss: 0.031062118709087372 - Test Loss: 0.04727419465780258\n",
            "Epoch: 1283 - Train Loss: 0.030951743945479393 - Test Loss: 0.0471603199839592\n",
            "Epoch: 1284 - Train Loss: 0.030841801315546036 - Test Loss: 0.04704686999320984\n",
            "Epoch: 1285 - Train Loss: 0.030732261016964912 - Test Loss: 0.04693383350968361\n",
            "Epoch: 1286 - Train Loss: 0.03062313050031662 - Test Loss: 0.04682121053338051\n",
            "Epoch: 1287 - Train Loss: 0.030514415353536606 - Test Loss: 0.04670899361371994\n",
            "Epoch: 1288 - Train Loss: 0.030406096950173378 - Test Loss: 0.046597182750701904\n",
            "Epoch: 1289 - Train Loss: 0.030298184603452682 - Test Loss: 0.046485785394907\n",
            "Epoch: 1290 - Train Loss: 0.030190693214535713 - Test Loss: 0.04637480154633522\n",
            "Epoch: 1291 - Train Loss: 0.030083591118454933 - Test Loss: 0.04626421257853508\n",
            "Epoch: 1292 - Train Loss: 0.029976891353726387 - Test Loss: 0.04615402966737747\n",
            "Epoch: 1293 - Train Loss: 0.02987057715654373 - Test Loss: 0.0460442379117012\n",
            "Epoch: 1294 - Train Loss: 0.029764678329229355 - Test Loss: 0.04593485966324806\n",
            "Epoch: 1295 - Train Loss: 0.029659157618880272 - Test Loss: 0.045825861394405365\n",
            "Epoch: 1296 - Train Loss: 0.02955404669046402 - Test Loss: 0.0457172766327858\n",
            "Epoch: 1297 - Train Loss: 0.02944931574165821 - Test Loss: 0.04560907185077667\n",
            "Epoch: 1298 - Train Loss: 0.02934497594833374 - Test Loss: 0.045501261949539185\n",
            "Epoch: 1299 - Train Loss: 0.029241016134619713 - Test Loss: 0.04539383947849274\n",
            "Epoch: 1300 - Train Loss: 0.02913745865225792 - Test Loss: 0.045286815613508224\n",
            "Epoch: 1301 - Train Loss: 0.029034268110990524 - Test Loss: 0.04518016427755356\n",
            "Epoch: 1302 - Train Loss: 0.02893148548901081 - Test Loss: 0.045073915272951126\n",
            "Epoch: 1303 - Train Loss: 0.028829067945480347 - Test Loss: 0.04496804624795914\n",
            "Epoch: 1304 - Train Loss: 0.028727032244205475 - Test Loss: 0.04486255347728729\n",
            "Epoch: 1305 - Train Loss: 0.02862536907196045 - Test Loss: 0.04475743696093559\n",
            "Epoch: 1306 - Train Loss: 0.028524085879325867 - Test Loss: 0.044652700424194336\n",
            "Epoch: 1307 - Train Loss: 0.028423182666301727 - Test Loss: 0.04454834759235382\n",
            "Epoch: 1308 - Train Loss: 0.028322642669081688 - Test Loss: 0.044444359838962555\n",
            "Epoch: 1309 - Train Loss: 0.028222480788826942 - Test Loss: 0.04434075951576233\n",
            "Epoch: 1310 - Train Loss: 0.028122693300247192 - Test Loss: 0.04423752799630165\n",
            "Epoch: 1311 - Train Loss: 0.028023267164826393 - Test Loss: 0.04413466528058052\n",
            "Epoch: 1312 - Train Loss: 0.027924207970499992 - Test Loss: 0.04403216764330864\n",
            "Epoch: 1313 - Train Loss: 0.027825510129332542 - Test Loss: 0.043930042535066605\n",
            "Epoch: 1314 - Train Loss: 0.027727177366614342 - Test Loss: 0.04382827877998352\n",
            "Epoch: 1315 - Train Loss: 0.027629217132925987 - Test Loss: 0.04372689127922058\n",
            "Epoch: 1316 - Train Loss: 0.027531607076525688 - Test Loss: 0.0436258539557457\n",
            "Epoch: 1317 - Train Loss: 0.027434365823864937 - Test Loss: 0.04352518916130066\n",
            "Epoch: 1318 - Train Loss: 0.027337484061717987 - Test Loss: 0.04342488572001457\n",
            "Epoch: 1319 - Train Loss: 0.027240952476859093 - Test Loss: 0.04332493245601654\n",
            "Epoch: 1320 - Train Loss: 0.027144769206643105 - Test Loss: 0.04322533309459686\n",
            "Epoch: 1321 - Train Loss: 0.027048947289586067 - Test Loss: 0.043126098811626434\n",
            "Epoch: 1322 - Train Loss: 0.026953475549817085 - Test Loss: 0.04302721843123436\n",
            "Epoch: 1323 - Train Loss: 0.026858346536755562 - Test Loss: 0.042928680777549744\n",
            "Epoch: 1324 - Train Loss: 0.026763571426272392 - Test Loss: 0.04283049702644348\n",
            "Epoch: 1325 - Train Loss: 0.026669148355722427 - Test Loss: 0.04273267835378647\n",
            "Epoch: 1326 - Train Loss: 0.026575064286589622 - Test Loss: 0.04263519123196602\n",
            "Epoch: 1327 - Train Loss: 0.026481330394744873 - Test Loss: 0.04253805801272392\n",
            "Epoch: 1328 - Train Loss: 0.026387939229607582 - Test Loss: 0.04244127497076988\n",
            "Epoch: 1329 - Train Loss: 0.0262948889285326 - Test Loss: 0.0423448272049427\n",
            "Epoch: 1330 - Train Loss: 0.02620217576622963 - Test Loss: 0.04224872961640358\n",
            "Epoch: 1331 - Train Loss: 0.02610979974269867 - Test Loss: 0.04215296730399132\n",
            "Epoch: 1332 - Train Loss: 0.026017773896455765 - Test Loss: 0.04205755516886711\n",
            "Epoch: 1333 - Train Loss: 0.025926070287823677 - Test Loss: 0.04196247085928917\n",
            "Epoch: 1334 - Train Loss: 0.02583470568060875 - Test Loss: 0.04186772555112839\n",
            "Epoch: 1335 - Train Loss: 0.025743680074810982 - Test Loss: 0.041773322969675064\n",
            "Epoch: 1336 - Train Loss: 0.02565298043191433 - Test Loss: 0.0416792556643486\n",
            "Epoch: 1337 - Train Loss: 0.02556261233985424 - Test Loss: 0.041585516184568405\n",
            "Epoch: 1338 - Train Loss: 0.025472570210695267 - Test Loss: 0.04149210825562477\n",
            "Epoch: 1339 - Train Loss: 0.025382855907082558 - Test Loss: 0.0413990244269371\n",
            "Epoch: 1340 - Train Loss: 0.02529347874224186 - Test Loss: 0.04130628705024719\n",
            "Epoch: 1341 - Train Loss: 0.025204414501786232 - Test Loss: 0.04121386632323265\n",
            "Epoch: 1342 - Train Loss: 0.025115683674812317 - Test Loss: 0.04112177714705467\n",
            "Epoch: 1343 - Train Loss: 0.025027265772223473 - Test Loss: 0.04103000462055206\n",
            "Epoch: 1344 - Train Loss: 0.024939175695180893 - Test Loss: 0.04093856364488602\n",
            "Epoch: 1345 - Train Loss: 0.024851396679878235 - Test Loss: 0.04084743186831474\n",
            "Epoch: 1346 - Train Loss: 0.024763941764831543 - Test Loss: 0.04075663164258003\n",
            "Epoch: 1347 - Train Loss: 0.02467680536210537 - Test Loss: 0.040666159242391586\n",
            "Epoch: 1348 - Train Loss: 0.024589980021119118 - Test Loss: 0.040575988590717316\n",
            "Epoch: 1349 - Train Loss: 0.024503473192453384 - Test Loss: 0.04048614948987961\n",
            "Epoch: 1350 - Train Loss: 0.024417290464043617 - Test Loss: 0.04039663448929787\n",
            "Epoch: 1351 - Train Loss: 0.024331409484148026 - Test Loss: 0.0403074212372303\n",
            "Epoch: 1352 - Train Loss: 0.024245833978056908 - Test Loss: 0.04021851345896721\n",
            "Epoch: 1353 - Train Loss: 0.02416057139635086 - Test Loss: 0.04012992978096008\n",
            "Epoch: 1354 - Train Loss: 0.024075627326965332 - Test Loss: 0.040041666477918625\n",
            "Epoch: 1355 - Train Loss: 0.02399098128080368 - Test Loss: 0.039953697472810745\n",
            "Epoch: 1356 - Train Loss: 0.023906633257865906 - Test Loss: 0.03986603021621704\n",
            "Epoch: 1357 - Train Loss: 0.023822598159313202 - Test Loss: 0.039778683334589005\n",
            "Epoch: 1358 - Train Loss: 0.023738877847790718 - Test Loss: 0.03969164937734604\n",
            "Epoch: 1359 - Train Loss: 0.023655448108911514 - Test Loss: 0.03960491344332695\n",
            "Epoch: 1360 - Train Loss: 0.02357231080532074 - Test Loss: 0.03951847180724144\n",
            "Epoch: 1361 - Train Loss: 0.023489482700824738 - Test Loss: 0.039432343095541\n",
            "Epoch: 1362 - Train Loss: 0.02340696007013321 - Test Loss: 0.039346519857645035\n",
            "Epoch: 1363 - Train Loss: 0.02332473173737526 - Test Loss: 0.039260998368263245\n",
            "Epoch: 1364 - Train Loss: 0.023242786526679993 - Test Loss: 0.03917576000094414\n",
            "Epoch: 1365 - Train Loss: 0.0231611505150795 - Test Loss: 0.0390908308327198\n",
            "Epoch: 1366 - Train Loss: 0.02307981252670288 - Test Loss: 0.03900621086359024\n",
            "Epoch: 1367 - Train Loss: 0.022998761385679245 - Test Loss: 0.03892187401652336\n",
            "Epoch: 1368 - Train Loss: 0.022917991504073143 - Test Loss: 0.03883783146739006\n",
            "Epoch: 1369 - Train Loss: 0.022837525233626366 - Test Loss: 0.03875407949090004\n",
            "Epoch: 1370 - Train Loss: 0.022757353261113167 - Test Loss: 0.03867064416408539\n",
            "Epoch: 1371 - Train Loss: 0.022677462548017502 - Test Loss: 0.038587480783462524\n",
            "Epoch: 1372 - Train Loss: 0.02259785309433937 - Test Loss: 0.038504600524902344\n",
            "Epoch: 1373 - Train Loss: 0.02251853421330452 - Test Loss: 0.03842201456427574\n",
            "Epoch: 1374 - Train Loss: 0.022439494729042053 - Test Loss: 0.03833971545100212\n",
            "Epoch: 1375 - Train Loss: 0.022360745817422867 - Test Loss: 0.03825771063566208\n",
            "Epoch: 1376 - Train Loss: 0.022282276302576065 - Test Loss: 0.03817598894238472\n",
            "Epoch: 1377 - Train Loss: 0.022204093635082245 - Test Loss: 0.038094550371170044\n",
            "Epoch: 1378 - Train Loss: 0.02212618663907051 - Test Loss: 0.03801339492201805\n",
            "Epoch: 1379 - Train Loss: 0.022048555314540863 - Test Loss: 0.03793251886963844\n",
            "Epoch: 1380 - Train Loss: 0.0219712071120739 - Test Loss: 0.03785192593932152\n",
            "Epoch: 1381 - Train Loss: 0.02189413458108902 - Test Loss: 0.037771616131067276\n",
            "Epoch: 1382 - Train Loss: 0.021817339584231377 - Test Loss: 0.03769158944487572\n",
            "Epoch: 1383 - Train Loss: 0.02174081839621067 - Test Loss: 0.037611834704875946\n",
            "Epoch: 1384 - Train Loss: 0.021664578467607498 - Test Loss: 0.037532366812229156\n",
            "Epoch: 1385 - Train Loss: 0.021588603034615517 - Test Loss: 0.037453167140483856\n",
            "Epoch: 1386 - Train Loss: 0.021512899547815323 - Test Loss: 0.037374235689640045\n",
            "Epoch: 1387 - Train Loss: 0.021437471732497215 - Test Loss: 0.037295594811439514\n",
            "Epoch: 1388 - Train Loss: 0.021362310275435448 - Test Loss: 0.037217214703559875\n",
            "Epoch: 1389 - Train Loss: 0.02128741517663002 - Test Loss: 0.037139106541872025\n",
            "Epoch: 1390 - Train Loss: 0.021212788298726082 - Test Loss: 0.03706126660108566\n",
            "Epoch: 1391 - Train Loss: 0.021138429641723633 - Test Loss: 0.036983709782361984\n",
            "Epoch: 1392 - Train Loss: 0.021064339205622673 - Test Loss: 0.036906417459249496\n",
            "Epoch: 1393 - Train Loss: 0.020990503951907158 - Test Loss: 0.0368293859064579\n",
            "Epoch: 1394 - Train Loss: 0.02091694250702858 - Test Loss: 0.036752622574567795\n",
            "Epoch: 1395 - Train Loss: 0.020843636244535446 - Test Loss: 0.03667612373828888\n",
            "Epoch: 1396 - Train Loss: 0.0207706019282341 - Test Loss: 0.03659990802407265\n",
            "Epoch: 1397 - Train Loss: 0.0206978190690279 - Test Loss: 0.036523934453725815\n",
            "Epoch: 1398 - Train Loss: 0.020625300705432892 - Test Loss: 0.03644823655486107\n",
            "Epoch: 1399 - Train Loss: 0.020553037524223328 - Test Loss: 0.036372795701026917\n",
            "Epoch: 1400 - Train Loss: 0.02048102766275406 - Test Loss: 0.03629760816693306\n",
            "Epoch: 1401 - Train Loss: 0.02040928229689598 - Test Loss: 0.03622269630432129\n",
            "Epoch: 1402 - Train Loss: 0.0203377865254879 - Test Loss: 0.03614803031086922\n",
            "Epoch: 1403 - Train Loss: 0.020266545936465263 - Test Loss: 0.03607362508773804\n",
            "Epoch: 1404 - Train Loss: 0.020195569843053818 - Test Loss: 0.03599948808550835\n",
            "Epoch: 1405 - Train Loss: 0.02012483775615692 - Test Loss: 0.03592560067772865\n",
            "Epoch: 1406 - Train Loss: 0.020054353401064873 - Test Loss: 0.03585195913910866\n",
            "Epoch: 1407 - Train Loss: 0.019984126091003418 - Test Loss: 0.03577858582139015\n",
            "Epoch: 1408 - Train Loss: 0.019914142787456512 - Test Loss: 0.03570545092225075\n",
            "Epoch: 1409 - Train Loss: 0.019844401627779007 - Test Loss: 0.03563256934285164\n",
            "Epoch: 1410 - Train Loss: 0.019774919375777245 - Test Loss: 0.035559944808483124\n",
            "Epoch: 1411 - Train Loss: 0.01970568485558033 - Test Loss: 0.035487569868564606\n",
            "Epoch: 1412 - Train Loss: 0.019636688753962517 - Test Loss: 0.035415444523096085\n",
            "Epoch: 1413 - Train Loss: 0.019567938521504402 - Test Loss: 0.035343561321496964\n",
            "Epoch: 1414 - Train Loss: 0.019499443471431732 - Test Loss: 0.03527194261550903\n",
            "Epoch: 1415 - Train Loss: 0.019431184977293015 - Test Loss: 0.035200562328100204\n",
            "Epoch: 1416 - Train Loss: 0.01936316303908825 - Test Loss: 0.035129424184560776\n",
            "Epoch: 1417 - Train Loss: 0.01929539255797863 - Test Loss: 0.035058535635471344\n",
            "Epoch: 1418 - Train Loss: 0.019227853044867516 - Test Loss: 0.034987885504961014\n",
            "Epoch: 1419 - Train Loss: 0.019160550087690353 - Test Loss: 0.034917473793029785\n",
            "Epoch: 1420 - Train Loss: 0.01909349299967289 - Test Loss: 0.03484731167554855\n",
            "Epoch: 1421 - Train Loss: 0.01902666874229908 - Test Loss: 0.03477738797664642\n",
            "Epoch: 1422 - Train Loss: 0.01896008849143982 - Test Loss: 0.03470771387219429\n",
            "Epoch: 1423 - Train Loss: 0.018893741071224213 - Test Loss: 0.03463826701045036\n",
            "Epoch: 1424 - Train Loss: 0.01882762461900711 - Test Loss: 0.034569062292575836\n",
            "Epoch: 1425 - Train Loss: 0.01876174844801426 - Test Loss: 0.03450010344386101\n",
            "Epoch: 1426 - Train Loss: 0.018696099519729614 - Test Loss: 0.034431375563144684\n",
            "Epoch: 1427 - Train Loss: 0.01863069459795952 - Test Loss: 0.03436289355158806\n",
            "Epoch: 1428 - Train Loss: 0.01856551319360733 - Test Loss: 0.03429463878273964\n",
            "Epoch: 1429 - Train Loss: 0.018500560894608498 - Test Loss: 0.034226614981889725\n",
            "Epoch: 1430 - Train Loss: 0.018435833975672722 - Test Loss: 0.034158822149038315\n",
            "Epoch: 1431 - Train Loss: 0.0183713398873806 - Test Loss: 0.03409126400947571\n",
            "Epoch: 1432 - Train Loss: 0.018307078629732132 - Test Loss: 0.034023940563201904\n",
            "Epoch: 1433 - Train Loss: 0.018243037164211273 - Test Loss: 0.033956848084926605\n",
            "Epoch: 1434 - Train Loss: 0.01817922294139862 - Test Loss: 0.033889979124069214\n",
            "Epoch: 1435 - Train Loss: 0.018115635961294174 - Test Loss: 0.033823344856500626\n",
            "Epoch: 1436 - Train Loss: 0.018052278086543083 - Test Loss: 0.03375694155693054\n",
            "Epoch: 1437 - Train Loss: 0.017989138141274452 - Test Loss: 0.033690765500068665\n",
            "Epoch: 1438 - Train Loss: 0.01792621985077858 - Test Loss: 0.033624809235334396\n",
            "Epoch: 1439 - Train Loss: 0.017863530665636063 - Test Loss: 0.03355908766388893\n",
            "Epoch: 1440 - Train Loss: 0.017801063135266304 - Test Loss: 0.03349359333515167\n",
            "Epoch: 1441 - Train Loss: 0.01773880049586296 - Test Loss: 0.03342830389738083\n",
            "Epoch: 1442 - Train Loss: 0.01767677068710327 - Test Loss: 0.03336325287818909\n",
            "Epoch: 1443 - Train Loss: 0.01761496253311634 - Test Loss: 0.03329842910170555\n",
            "Epoch: 1444 - Train Loss: 0.017553364858031273 - Test Loss: 0.033233821392059326\n",
            "Epoch: 1445 - Train Loss: 0.017491985112428665 - Test Loss: 0.03316942974925041\n",
            "Epoch: 1446 - Train Loss: 0.017430834472179413 - Test Loss: 0.0331052765250206\n",
            "Epoch: 1447 - Train Loss: 0.017369884997606277 - Test Loss: 0.0330413319170475\n",
            "Epoch: 1448 - Train Loss: 0.017309153452515602 - Test Loss: 0.032977595925331116\n",
            "Epoch: 1449 - Train Loss: 0.017248637974262238 - Test Loss: 0.03291408717632294\n",
            "Epoch: 1450 - Train Loss: 0.017188336700201035 - Test Loss: 0.03285079821944237\n",
            "Epoch: 1451 - Train Loss: 0.017128244042396545 - Test Loss: 0.03278771787881851\n",
            "Epoch: 1452 - Train Loss: 0.01706835813820362 - Test Loss: 0.03272485360503197\n",
            "Epoch: 1453 - Train Loss: 0.017008692026138306 - Test Loss: 0.03266220912337303\n",
            "Epoch: 1454 - Train Loss: 0.016949230805039406 - Test Loss: 0.03259977698326111\n",
            "Epoch: 1455 - Train Loss: 0.01688998192548752 - Test Loss: 0.0325375534594059\n",
            "Epoch: 1456 - Train Loss: 0.016830936074256897 - Test Loss: 0.032475546002388\n",
            "Epoch: 1457 - Train Loss: 0.016772106289863586 - Test Loss: 0.03241375833749771\n",
            "Epoch: 1458 - Train Loss: 0.016713479533791542 - Test Loss: 0.03235217183828354\n",
            "Epoch: 1459 - Train Loss: 0.01665506325662136 - Test Loss: 0.03229079768061638\n",
            "Epoch: 1460 - Train Loss: 0.016596844419836998 - Test Loss: 0.03222963586449623\n",
            "Epoch: 1461 - Train Loss: 0.0165388360619545 - Test Loss: 0.0321686789393425\n",
            "Epoch: 1462 - Train Loss: 0.016481023281812668 - Test Loss: 0.03210793063044548\n",
            "Epoch: 1463 - Train Loss: 0.016423417255282402 - Test Loss: 0.03204738348722458\n",
            "Epoch: 1464 - Train Loss: 0.01636602357029915 - Test Loss: 0.031987059861421585\n",
            "Epoch: 1465 - Train Loss: 0.016308819875121117 - Test Loss: 0.03192692622542381\n",
            "Epoch: 1466 - Train Loss: 0.0162518173456192 - Test Loss: 0.031867001205682755\n",
            "Epoch: 1467 - Train Loss: 0.016195023432374 - Test Loss: 0.03180728480219841\n",
            "Epoch: 1468 - Train Loss: 0.01613842509686947 - Test Loss: 0.03174777328968048\n",
            "Epoch: 1469 - Train Loss: 0.016082022339105606 - Test Loss: 0.03168845921754837\n",
            "Epoch: 1470 - Train Loss: 0.016025815159082413 - Test Loss: 0.031629350036382675\n",
            "Epoch: 1471 - Train Loss: 0.015969812870025635 - Test Loss: 0.0315704420208931\n",
            "Epoch: 1472 - Train Loss: 0.015914004296064377 - Test Loss: 0.03151173144578934\n",
            "Epoch: 1473 - Train Loss: 0.01585838571190834 - Test Loss: 0.031453218311071396\n",
            "Epoch: 1474 - Train Loss: 0.01580297388136387 - Test Loss: 0.03139491751790047\n",
            "Epoch: 1475 - Train Loss: 0.01574775017797947 - Test Loss: 0.03133680671453476\n",
            "Epoch: 1476 - Train Loss: 0.01569271646440029 - Test Loss: 0.03127888962626457\n",
            "Epoch: 1477 - Train Loss: 0.01563788391649723 - Test Loss: 0.0312211774289608\n",
            "Epoch: 1478 - Train Loss: 0.015583241358399391 - Test Loss: 0.031163664534687996\n",
            "Epoch: 1479 - Train Loss: 0.015528785064816475 - Test Loss: 0.031106336042284966\n",
            "Epoch: 1480 - Train Loss: 0.015474521555006504 - Test Loss: 0.031049208715558052\n",
            "Epoch: 1481 - Train Loss: 0.015420442447066307 - Test Loss: 0.03099226765334606\n",
            "Epoch: 1482 - Train Loss: 0.015366566367447376 - Test Loss: 0.030935537070035934\n",
            "Epoch: 1483 - Train Loss: 0.01531287282705307 - Test Loss: 0.030878989025950432\n",
            "Epoch: 1484 - Train Loss: 0.015259371139109135 - Test Loss: 0.030822638422250748\n",
            "Epoch: 1485 - Train Loss: 0.015206051990389824 - Test Loss: 0.030766475945711136\n",
            "Epoch: 1486 - Train Loss: 0.015152915380895138 - Test Loss: 0.030710494145751\n",
            "Epoch: 1487 - Train Loss: 0.01509997621178627 - Test Loss: 0.030654726549983025\n",
            "Epoch: 1488 - Train Loss: 0.015047215856611729 - Test Loss: 0.030599134042859077\n",
            "Epoch: 1489 - Train Loss: 0.014994639903306961 - Test Loss: 0.030543731525540352\n",
            "Epoch: 1490 - Train Loss: 0.014942247420549393 - Test Loss: 0.0304885134100914\n",
            "Epoch: 1491 - Train Loss: 0.0148900356143713 - Test Loss: 0.03043348155915737\n",
            "Epoch: 1492 - Train Loss: 0.014838016591966152 - Test Loss: 0.03037865087389946\n",
            "Epoch: 1493 - Train Loss: 0.014786175452172756 - Test Loss: 0.030324000865221024\n",
            "Epoch: 1494 - Train Loss: 0.01473451592028141 - Test Loss: 0.03026953712105751\n",
            "Epoch: 1495 - Train Loss: 0.01468303520232439 - Test Loss: 0.030215254053473473\n",
            "Epoch: 1496 - Train Loss: 0.014631730504333973 - Test Loss: 0.030161147937178612\n",
            "Epoch: 1497 - Train Loss: 0.01458060648292303 - Test Loss: 0.03010723367333412\n",
            "Epoch: 1498 - Train Loss: 0.014529665000736713 - Test Loss: 0.030053507536649704\n",
            "Epoch: 1499 - Train Loss: 0.014478906989097595 - Test Loss: 0.029999960213899612\n",
            "Epoch: 1500 - Train Loss: 0.014428314752876759 - Test Loss: 0.02994658797979355\n",
            "Epoch: 1501 - Train Loss: 0.01437790784984827 - Test Loss: 0.029893405735492706\n",
            "Epoch: 1502 - Train Loss: 0.014327670447528362 - Test Loss: 0.02984039857983589\n",
            "Epoch: 1503 - Train Loss: 0.014277607202529907 - Test Loss: 0.029787572100758553\n",
            "Epoch: 1504 - Train Loss: 0.014227727428078651 - Test Loss: 0.02973492443561554\n",
            "Epoch: 1505 - Train Loss: 0.014178019016981125 - Test Loss: 0.0296824611723423\n",
            "Epoch: 1506 - Train Loss: 0.014128479175269604 - Test Loss: 0.02963016927242279\n",
            "Epoch: 1507 - Train Loss: 0.014079117216169834 - Test Loss: 0.029578054323792458\n",
            "Epoch: 1508 - Train Loss: 0.01402992382645607 - Test Loss: 0.029526114463806152\n",
            "Epoch: 1509 - Train Loss: 0.013980901800096035 - Test Loss: 0.029474355280399323\n",
            "Epoch: 1510 - Train Loss: 0.013932054862380028 - Test Loss: 0.02942277304828167\n",
            "Epoch: 1511 - Train Loss: 0.013883375562727451 - Test Loss: 0.029371358454227448\n",
            "Epoch: 1512 - Train Loss: 0.013834867626428604 - Test Loss: 0.02932012267410755\n",
            "Epoch: 1513 - Train Loss: 0.013786524534225464 - Test Loss: 0.029269056394696236\n",
            "Epoch: 1514 - Train Loss: 0.013738350011408329 - Test Loss: 0.029218165203928947\n",
            "Epoch: 1515 - Train Loss: 0.013690349645912647 - Test Loss: 0.029167456552386284\n",
            "Epoch: 1516 - Train Loss: 0.013642514124512672 - Test Loss: 0.029116909950971603\n",
            "Epoch: 1517 - Train Loss: 0.01359484251588583 - Test Loss: 0.029066534712910652\n",
            "Epoch: 1518 - Train Loss: 0.013547345995903015 - Test Loss: 0.029016336426138878\n",
            "Epoch: 1519 - Train Loss: 0.013500004075467587 - Test Loss: 0.028966302052140236\n",
            "Epoch: 1520 - Train Loss: 0.013452829793095589 - Test Loss: 0.028916433453559875\n",
            "Epoch: 1521 - Train Loss: 0.013405823148787022 - Test Loss: 0.028866741806268692\n",
            "Epoch: 1522 - Train Loss: 0.013358972035348415 - Test Loss: 0.028817206621170044\n",
            "Epoch: 1523 - Train Loss: 0.013312290422618389 - Test Loss: 0.028767846524715424\n",
            "Epoch: 1524 - Train Loss: 0.013265769928693771 - Test Loss: 0.028718650341033936\n",
            "Epoch: 1525 - Train Loss: 0.013219408690929413 - Test Loss: 0.02866961807012558\n",
            "Epoch: 1526 - Train Loss: 0.013173215091228485 - Test Loss: 0.02862076461315155\n",
            "Epoch: 1527 - Train Loss: 0.013127180747687817 - Test Loss: 0.028572067618370056\n",
            "Epoch: 1528 - Train Loss: 0.013081304728984833 - Test Loss: 0.028523540124297142\n",
            "Epoch: 1529 - Train Loss: 0.013035589829087257 - Test Loss: 0.02847517654299736\n",
            "Epoch: 1530 - Train Loss: 0.012990031391382217 - Test Loss: 0.028426973149180412\n",
            "Epoch: 1531 - Train Loss: 0.012944634072482586 - Test Loss: 0.028378935530781746\n",
            "Epoch: 1532 - Train Loss: 0.012899397872388363 - Test Loss: 0.028331061825156212\n",
            "Epoch: 1533 - Train Loss: 0.012854315340518951 - Test Loss: 0.02828334830701351\n",
            "Epoch: 1534 - Train Loss: 0.012809391133487225 - Test Loss: 0.028235798701643944\n",
            "Epoch: 1535 - Train Loss: 0.012764621526002884 - Test Loss: 0.02818840742111206\n",
            "Epoch: 1536 - Train Loss: 0.01272000465542078 - Test Loss: 0.028141170740127563\n",
            "Epoch: 1537 - Train Loss: 0.012675545178353786 - Test Loss: 0.02809409610927105\n",
            "Epoch: 1538 - Train Loss: 0.012631237506866455 - Test Loss: 0.02804717980325222\n",
            "Epoch: 1539 - Train Loss: 0.012587086297571659 - Test Loss: 0.028000425547361374\n",
            "Epoch: 1540 - Train Loss: 0.012543091550469398 - Test Loss: 0.027953829616308212\n",
            "Epoch: 1541 - Train Loss: 0.012499246746301651 - Test Loss: 0.027907393872737885\n",
            "Epoch: 1542 - Train Loss: 0.01245555654168129 - Test Loss: 0.027861109003424644\n",
            "Epoch: 1543 - Train Loss: 0.012412016279995441 - Test Loss: 0.02781498059630394\n",
            "Epoch: 1544 - Train Loss: 0.012368623167276382 - Test Loss: 0.02776901051402092\n",
            "Epoch: 1545 - Train Loss: 0.012325383722782135 - Test Loss: 0.027723191305994987\n",
            "Epoch: 1546 - Train Loss: 0.012282292358577251 - Test Loss: 0.02767753042280674\n",
            "Epoch: 1547 - Train Loss: 0.012239353731274605 - Test Loss: 0.02763202041387558\n",
            "Epoch: 1548 - Train Loss: 0.012196565978229046 - Test Loss: 0.027586670592427254\n",
            "Epoch: 1549 - Train Loss: 0.012153924442827702 - Test Loss: 0.027541473507881165\n",
            "Epoch: 1550 - Train Loss: 0.01211143471300602 - Test Loss: 0.027496429160237312\n",
            "Epoch: 1551 - Train Loss: 0.012069090269505978 - Test Loss: 0.0274515338242054\n",
            "Epoch: 1552 - Train Loss: 0.012026890181005001 - Test Loss: 0.027406789362430573\n",
            "Epoch: 1553 - Train Loss: 0.011984840035438538 - Test Loss: 0.027362197637557983\n",
            "Epoch: 1554 - Train Loss: 0.01194293238222599 - Test Loss: 0.027317756786942482\n",
            "Epoch: 1555 - Train Loss: 0.011901171877980232 - Test Loss: 0.027273468673229218\n",
            "Epoch: 1556 - Train Loss: 0.011859563179314137 - Test Loss: 0.02722932957112789\n",
            "Epoch: 1557 - Train Loss: 0.011818093247711658 - Test Loss: 0.027185343205928802\n",
            "Epoch: 1558 - Train Loss: 0.011776769533753395 - Test Loss: 0.027141505852341652\n",
            "Epoch: 1559 - Train Loss: 0.0117355827242136 - Test Loss: 0.027097798883914948\n",
            "Epoch: 1560 - Train Loss: 0.011694538407027721 - Test Loss: 0.02705424837768078\n",
            "Epoch: 1561 - Train Loss: 0.011653642170131207 - Test Loss: 0.027010846883058548\n",
            "Epoch: 1562 - Train Loss: 0.011612879112362862 - Test Loss: 0.02696758322417736\n",
            "Epoch: 1563 - Train Loss: 0.011572265066206455 - Test Loss: 0.02692447602748871\n",
            "Epoch: 1564 - Train Loss: 0.011531789787113667 - Test Loss: 0.0268815066665411\n",
            "Epoch: 1565 - Train Loss: 0.011491452343761921 - Test Loss: 0.02683868445456028\n",
            "Epoch: 1566 - Train Loss: 0.011451263912022114 - Test Loss: 0.026796016842126846\n",
            "Epoch: 1567 - Train Loss: 0.011411205865442753 - Test Loss: 0.026753487065434456\n",
            "Epoch: 1568 - Train Loss: 0.011371289379894733 - Test Loss: 0.026711096987128258\n",
            "Epoch: 1569 - Train Loss: 0.01133151538670063 - Test Loss: 0.026668855920433998\n",
            "Epoch: 1570 - Train Loss: 0.011291870847344398 - Test Loss: 0.026626750826835632\n",
            "Epoch: 1571 - Train Loss: 0.011252370662987232 - Test Loss: 0.026584796607494354\n",
            "Epoch: 1572 - Train Loss: 0.011213008314371109 - Test Loss: 0.02654298208653927\n",
            "Epoch: 1573 - Train Loss: 0.011173776350915432 - Test Loss: 0.026501303538680077\n",
            "Epoch: 1574 - Train Loss: 0.0111346784979105 - Test Loss: 0.02645976096391678\n",
            "Epoch: 1575 - Train Loss: 0.011095723137259483 - Test Loss: 0.026418369263410568\n",
            "Epoch: 1576 - Train Loss: 0.011056897230446339 - Test Loss: 0.026377107948064804\n",
            "Epoch: 1577 - Train Loss: 0.011018207296729088 - Test Loss: 0.02633598819375038\n",
            "Epoch: 1578 - Train Loss: 0.01097965519875288 - Test Loss: 0.0262950100004673\n",
            "Epoch: 1579 - Train Loss: 0.010941228829324245 - Test Loss: 0.026254165917634964\n",
            "Epoch: 1580 - Train Loss: 0.010902942158281803 - Test Loss: 0.02621346525847912\n",
            "Epoch: 1581 - Train Loss: 0.010864786803722382 - Test Loss: 0.026172900572419167\n",
            "Epoch: 1582 - Train Loss: 0.010826761834323406 - Test Loss: 0.02613246999680996\n",
            "Epoch: 1583 - Train Loss: 0.0107888700440526 - Test Loss: 0.026092179119586945\n",
            "Epoch: 1584 - Train Loss: 0.010751107707619667 - Test Loss: 0.026052018627524376\n",
            "Epoch: 1585 - Train Loss: 0.010713479481637478 - Test Loss: 0.026011997833848\n",
            "Epoch: 1586 - Train Loss: 0.010675977915525436 - Test Loss: 0.025972111150622368\n",
            "Epoch: 1587 - Train Loss: 0.010638605803251266 - Test Loss: 0.02593235671520233\n",
            "Epoch: 1588 - Train Loss: 0.010601365007460117 - Test Loss: 0.02589274011552334\n",
            "Epoch: 1589 - Train Loss: 0.010564250871539116 - Test Loss: 0.02585325576364994\n",
            "Epoch: 1590 - Train Loss: 0.010527268052101135 - Test Loss: 0.025813907384872437\n",
            "Epoch: 1591 - Train Loss: 0.010490414686501026 - Test Loss: 0.025774694979190826\n",
            "Epoch: 1592 - Train Loss: 0.01045368704944849 - Test Loss: 0.025735609233379364\n",
            "Epoch: 1593 - Train Loss: 0.010417080484330654 - Test Loss: 0.0256966445595026\n",
            "Epoch: 1594 - Train Loss: 0.010380606167018414 - Test Loss: 0.02565782703459263\n",
            "Epoch: 1595 - Train Loss: 0.010344257578253746 - Test Loss: 0.025619132444262505\n",
            "Epoch: 1596 - Train Loss: 0.010308031924068928 - Test Loss: 0.025580570101737976\n",
            "Epoch: 1597 - Train Loss: 0.01027193944901228 - Test Loss: 0.02554214373230934\n",
            "Epoch: 1598 - Train Loss: 0.010235965251922607 - Test Loss: 0.025503845885396004\n",
            "Epoch: 1599 - Train Loss: 0.010200112126767635 - Test Loss: 0.025465665385127068\n",
            "Epoch: 1600 - Train Loss: 0.010164386592805386 - Test Loss: 0.025427618995308876\n",
            "Epoch: 1601 - Train Loss: 0.010128787718713284 - Test Loss: 0.02538970485329628\n",
            "Epoch: 1602 - Train Loss: 0.010093308985233307 - Test Loss: 0.02535191737115383\n",
            "Epoch: 1603 - Train Loss: 0.010057955048978329 - Test Loss: 0.02531426213681698\n",
            "Epoch: 1604 - Train Loss: 0.010022723115980625 - Test Loss: 0.025276729837059975\n",
            "Epoch: 1605 - Train Loss: 0.009987609460949898 - Test Loss: 0.02523932047188282\n",
            "Epoch: 1606 - Train Loss: 0.009952615015208721 - Test Loss: 0.025202034041285515\n",
            "Epoch: 1607 - Train Loss: 0.009917744435369968 - Test Loss: 0.025164877995848656\n",
            "Epoch: 1608 - Train Loss: 0.009882993064820766 - Test Loss: 0.025127844884991646\n",
            "Epoch: 1609 - Train Loss: 0.009848363697528839 - Test Loss: 0.025090942159295082\n",
            "Epoch: 1610 - Train Loss: 0.009813857264816761 - Test Loss: 0.025054164230823517\n",
            "Epoch: 1611 - Train Loss: 0.009779466316103935 - Test Loss: 0.02501750737428665\n",
            "Epoch: 1612 - Train Loss: 0.009745188988745213 - Test Loss: 0.024980967864394188\n",
            "Epoch: 1613 - Train Loss: 0.009711036458611488 - Test Loss: 0.02494455873966217\n",
            "Epoch: 1614 - Train Loss: 0.00967700220644474 - Test Loss: 0.024908270686864853\n",
            "Epoch: 1615 - Train Loss: 0.009643081575632095 - Test Loss: 0.024872105568647385\n",
            "Epoch: 1616 - Train Loss: 0.009609280154109001 - Test Loss: 0.024836061522364616\n",
            "Epoch: 1617 - Train Loss: 0.009575595147907734 - Test Loss: 0.02480013482272625\n",
            "Epoch: 1618 - Train Loss: 0.009542028419673443 - Test Loss: 0.02476433292031288\n",
            "Epoch: 1619 - Train Loss: 0.009508575312793255 - Test Loss: 0.024728653952479362\n",
            "Epoch: 1620 - Train Loss: 0.009475236758589745 - Test Loss: 0.024693094193935394\n",
            "Epoch: 1621 - Train Loss: 0.009442011825740337 - Test Loss: 0.02465764991939068\n",
            "Epoch: 1622 - Train Loss: 0.009408908896148205 - Test Loss: 0.02462233230471611\n",
            "Epoch: 1623 - Train Loss: 0.009375912137329578 - Test Loss: 0.024587126448750496\n",
            "Epoch: 1624 - Train Loss: 0.009343034587800503 - Test Loss: 0.02455204166471958\n",
            "Epoch: 1625 - Train Loss: 0.00931027252227068 - Test Loss: 0.024517083540558815\n",
            "Epoch: 1626 - Train Loss: 0.009277617558836937 - Test Loss: 0.024482233449816704\n",
            "Epoch: 1627 - Train Loss: 0.009245081804692745 - Test Loss: 0.02444750815629959\n",
            "Epoch: 1628 - Train Loss: 0.00921265222132206 - Test Loss: 0.024412894621491432\n",
            "Epoch: 1629 - Train Loss: 0.009180338121950626 - Test Loss: 0.024378396570682526\n",
            "Epoch: 1630 - Train Loss: 0.009148137643933296 - Test Loss: 0.024344023317098618\n",
            "Epoch: 1631 - Train Loss: 0.009116041474044323 - Test Loss: 0.024309754371643066\n",
            "Epoch: 1632 - Train Loss: 0.009084059856832027 - Test Loss: 0.024275608360767365\n",
            "Epoch: 1633 - Train Loss: 0.009052186273038387 - Test Loss: 0.024241570383310318\n",
            "Epoch: 1634 - Train Loss: 0.00902042631059885 - Test Loss: 0.02420765534043312\n",
            "Epoch: 1635 - Train Loss: 0.008988779038190842 - Test Loss: 0.024173857644200325\n",
            "Epoch: 1636 - Train Loss: 0.008957237005233765 - Test Loss: 0.024140166118741035\n",
            "Epoch: 1637 - Train Loss: 0.008925799280405045 - Test Loss: 0.0241065863519907\n",
            "Epoch: 1638 - Train Loss: 0.008894476108253002 - Test Loss: 0.024073125794529915\n",
            "Epoch: 1639 - Train Loss: 0.008863260969519615 - Test Loss: 0.024039780721068382\n",
            "Epoch: 1640 - Train Loss: 0.008832150138914585 - Test Loss: 0.024006539955735207\n",
            "Epoch: 1641 - Train Loss: 0.008801144547760487 - Test Loss: 0.023973409086465836\n",
            "Epoch: 1642 - Train Loss: 0.008770253509283066 - Test Loss: 0.023940401151776314\n",
            "Epoch: 1643 - Train Loss: 0.008739463053643703 - Test Loss: 0.023907499387860298\n",
            "Epoch: 1644 - Train Loss: 0.008708777837455273 - Test Loss: 0.02387470193207264\n",
            "Epoch: 1645 - Train Loss: 0.008678200654685497 - Test Loss: 0.023842021822929382\n",
            "Epoch: 1646 - Train Loss: 0.008647731505334377 - Test Loss: 0.02380945347249508\n",
            "Epoch: 1647 - Train Loss: 0.008617368526756763 - Test Loss: 0.02377699501812458\n",
            "Epoch: 1648 - Train Loss: 0.008587106131017208 - Test Loss: 0.023744642734527588\n",
            "Epoch: 1649 - Train Loss: 0.008556947112083435 - Test Loss: 0.0237123966217041\n",
            "Epoch: 1650 - Train Loss: 0.008526895195245743 - Test Loss: 0.023680265992879868\n",
            "Epoch: 1651 - Train Loss: 0.008496949449181557 - Test Loss: 0.023648245260119438\n",
            "Epoch: 1652 - Train Loss: 0.008467105217278004 - Test Loss: 0.023616326972842216\n",
            "Epoch: 1653 - Train Loss: 0.00843735970556736 - Test Loss: 0.023584512993693352\n",
            "Epoch: 1654 - Train Loss: 0.008407722227275372 - Test Loss: 0.02355281449854374\n",
            "Epoch: 1655 - Train Loss: 0.008378186263144016 - Test Loss: 0.023521224036812782\n",
            "Epoch: 1656 - Train Loss: 0.00834874901920557 - Test Loss: 0.023489734157919884\n",
            "Epoch: 1657 - Train Loss: 0.008319413289427757 - Test Loss: 0.02345835044980049\n",
            "Epoch: 1658 - Train Loss: 0.0082901855930686 - Test Loss: 0.023427076637744904\n",
            "Epoch: 1659 - Train Loss: 0.008261052891612053 - Test Loss: 0.02339591085910797\n",
            "Epoch: 1660 - Train Loss: 0.008232018910348415 - Test Loss: 0.02336484007537365\n",
            "Epoch: 1661 - Train Loss: 0.008203087374567986 - Test Loss: 0.023333881050348282\n",
            "Epoch: 1662 - Train Loss: 0.008174258284270763 - Test Loss: 0.02330303005874157\n",
            "Epoch: 1663 - Train Loss: 0.00814552791416645 - Test Loss: 0.023272277787327766\n",
            "Epoch: 1664 - Train Loss: 0.0081168906763196 - Test Loss: 0.023241626098752022\n",
            "Epoch: 1665 - Train Loss: 0.008088357746601105 - Test Loss: 0.023211084306240082\n",
            "Epoch: 1666 - Train Loss: 0.00805992353707552 - Test Loss: 0.023180648684501648\n",
            "Epoch: 1667 - Train Loss: 0.00803158525377512 - Test Loss: 0.023150306195020676\n",
            "Epoch: 1668 - Train Loss: 0.008003342896699905 - Test Loss: 0.02312006987631321\n",
            "Epoch: 1669 - Train Loss: 0.007975203916430473 - Test Loss: 0.02308993972837925\n",
            "Epoch: 1670 - Train Loss: 0.00794715341180563 - Test Loss: 0.0230599045753479\n",
            "Epoch: 1671 - Train Loss: 0.00791920442134142 - Test Loss: 0.023029973730444908\n",
            "Epoch: 1672 - Train Loss: 0.007891352288424969 - Test Loss: 0.023000145331025124\n",
            "Epoch: 1673 - Train Loss: 0.007863594219088554 - Test Loss: 0.022970419377088547\n",
            "Epoch: 1674 - Train Loss: 0.007835933938622475 - Test Loss: 0.02294079028069973\n",
            "Epoch: 1675 - Train Loss: 0.007808367721736431 - Test Loss: 0.02291126176714897\n",
            "Epoch: 1676 - Train Loss: 0.007780898828059435 - Test Loss: 0.02288183942437172\n",
            "Epoch: 1677 - Train Loss: 0.00775351794436574 - Test Loss: 0.02285250648856163\n",
            "Epoch: 1678 - Train Loss: 0.007726236246526241 - Test Loss: 0.0228232741355896\n",
            "Epoch: 1679 - Train Loss: 0.007699048612266779 - Test Loss: 0.022794144228100777\n",
            "Epoch: 1680 - Train Loss: 0.007671949453651905 - Test Loss: 0.022765103727579117\n",
            "Epoch: 1681 - Train Loss: 0.007644945289939642 - Test Loss: 0.022736165672540665\n",
            "Epoch: 1682 - Train Loss: 0.007618033792823553 - Test Loss: 0.022707324475049973\n",
            "Epoch: 1683 - Train Loss: 0.007591217756271362 - Test Loss: 0.02267858386039734\n",
            "Epoch: 1684 - Train Loss: 0.007564492058008909 - Test Loss: 0.022649938240647316\n",
            "Epoch: 1685 - Train Loss: 0.007537860423326492 - Test Loss: 0.02262139320373535\n",
            "Epoch: 1686 - Train Loss: 0.007511320058256388 - Test Loss: 0.02259294129908085\n",
            "Epoch: 1687 - Train Loss: 0.007484869100153446 - Test Loss: 0.022564584389328957\n",
            "Epoch: 1688 - Train Loss: 0.007458506617695093 - Test Loss: 0.022536320611834526\n",
            "Epoch: 1689 - Train Loss: 0.007432239595800638 - Test Loss: 0.022508155554533005\n",
            "Epoch: 1690 - Train Loss: 0.00740605965256691 - Test Loss: 0.022480081766843796\n",
            "Epoch: 1691 - Train Loss: 0.007379972841590643 - Test Loss: 0.022452110424637794\n",
            "Epoch: 1692 - Train Loss: 0.007353974971920252 - Test Loss: 0.022424230352044106\n",
            "Epoch: 1693 - Train Loss: 0.007328064180910587 - Test Loss: 0.02239643968641758\n",
            "Epoch: 1694 - Train Loss: 0.007302241865545511 - Test Loss: 0.022368744015693665\n",
            "Epoch: 1695 - Train Loss: 0.0072765108197927475 - Test Loss: 0.02234114333987236\n",
            "Epoch: 1696 - Train Loss: 0.007250865921378136 - Test Loss: 0.022313633933663368\n",
            "Epoch: 1697 - Train Loss: 0.007225312292575836 - Test Loss: 0.022286219522356987\n",
            "Epoch: 1698 - Train Loss: 0.007199844345450401 - Test Loss: 0.022258896380662918\n",
            "Epoch: 1699 - Train Loss: 0.007174463476985693 - Test Loss: 0.022231662645936012\n",
            "Epoch: 1700 - Train Loss: 0.007149169221520424 - Test Loss: 0.022204522043466568\n",
            "Epoch: 1701 - Train Loss: 0.007123963441699743 - Test Loss: 0.022177476435899734\n",
            "Epoch: 1702 - Train Loss: 0.007098843809217215 - Test Loss: 0.022150514647364616\n",
            "Epoch: 1703 - Train Loss: 0.007073812186717987 - Test Loss: 0.022123655304312706\n",
            "Epoch: 1704 - Train Loss: 0.0070488653145730495 - Test Loss: 0.02209688164293766\n",
            "Epoch: 1705 - Train Loss: 0.007024003192782402 - Test Loss: 0.02207019366323948\n",
            "Epoch: 1706 - Train Loss: 0.00699922489002347 - Test Loss: 0.02204359881579876\n",
            "Epoch: 1707 - Train Loss: 0.006974535528570414 - Test Loss: 0.022017095237970352\n",
            "Epoch: 1708 - Train Loss: 0.006949929054826498 - Test Loss: 0.02199067920446396\n",
            "Epoch: 1709 - Train Loss: 0.006925409194082022 - Test Loss: 0.021964354440569878\n",
            "Epoch: 1710 - Train Loss: 0.006900973152369261 - Test Loss: 0.02193811908364296\n",
            "Epoch: 1711 - Train Loss: 0.006876619998365641 - Test Loss: 0.021911971271038055\n",
            "Epoch: 1712 - Train Loss: 0.006852349732071161 - Test Loss: 0.021885912865400314\n",
            "Epoch: 1713 - Train Loss: 0.006828164681792259 - Test Loss: 0.02185993641614914\n",
            "Epoch: 1714 - Train Loss: 0.006804061587899923 - Test Loss: 0.021834053099155426\n",
            "Epoch: 1715 - Train Loss: 0.006780040450394154 - Test Loss: 0.021808257326483727\n",
            "Epoch: 1716 - Train Loss: 0.006756104528903961 - Test Loss: 0.02178254909813404\n",
            "Epoch: 1717 - Train Loss: 0.006732249166816473 - Test Loss: 0.02175692468881607\n",
            "Epoch: 1718 - Train Loss: 0.006708478555083275 - Test Loss: 0.021731393411755562\n",
            "Epoch: 1719 - Train Loss: 0.006684785708785057 - Test Loss: 0.02170593850314617\n",
            "Epoch: 1720 - Train Loss: 0.0066611748188734055 - Test Loss: 0.021680576726794243\n",
            "Epoch: 1721 - Train Loss: 0.006637650076299906 - Test Loss: 0.021655304357409477\n",
            "Epoch: 1722 - Train Loss: 0.006614199373871088 - Test Loss: 0.02163011021912098\n",
            "Epoch: 1723 - Train Loss: 0.006590832490473986 - Test Loss: 0.021605001762509346\n",
            "Epoch: 1724 - Train Loss: 0.006567544769495726 - Test Loss: 0.021579978987574577\n",
            "Epoch: 1725 - Train Loss: 0.006544339936226606 - Test Loss: 0.021555043756961823\n",
            "Epoch: 1726 - Train Loss: 0.0065212128683924675 - Test Loss: 0.021530192345380783\n",
            "Epoch: 1727 - Train Loss: 0.006498165428638458 - Test Loss: 0.02150542102754116\n",
            "Epoch: 1728 - Train Loss: 0.006475195754319429 - Test Loss: 0.021480735391378403\n",
            "Epoch: 1729 - Train Loss: 0.006452302914112806 - Test Loss: 0.021456129848957062\n",
            "Epoch: 1730 - Train Loss: 0.006429496221244335 - Test Loss: 0.021431611850857735\n",
            "Epoch: 1731 - Train Loss: 0.006406761705875397 - Test Loss: 0.021407179534435272\n",
            "Epoch: 1732 - Train Loss: 0.006384104490280151 - Test Loss: 0.02138282172381878\n",
            "Epoch: 1733 - Train Loss: 0.0063615282997488976 - Test Loss: 0.02135854959487915\n",
            "Epoch: 1734 - Train Loss: 0.006339032202959061 - Test Loss: 0.021334365010261536\n",
            "Epoch: 1735 - Train Loss: 0.0063166129402816296 - Test Loss: 0.021310264244675636\n",
            "Epoch: 1736 - Train Loss: 0.006294265389442444 - Test Loss: 0.021286236122250557\n",
            "Epoch: 1737 - Train Loss: 0.006271999794989824 - Test Loss: 0.02126229554414749\n",
            "Epoch: 1738 - Train Loss: 0.006249808240681887 - Test Loss: 0.021238431334495544\n",
            "Epoch: 1739 - Train Loss: 0.006227693520486355 - Test Loss: 0.021214652806520462\n",
            "Epoch: 1740 - Train Loss: 0.006205654237419367 - Test Loss: 0.021190950646996498\n",
            "Epoch: 1741 - Train Loss: 0.006183694116771221 - Test Loss: 0.0211673341691494\n",
            "Epoch: 1742 - Train Loss: 0.006161804310977459 - Test Loss: 0.02114379033446312\n",
            "Epoch: 1743 - Train Loss: 0.00613999180495739 - Test Loss: 0.021120330318808556\n",
            "Epoch: 1744 - Train Loss: 0.0061182547360658646 - Test Loss: 0.02109695039689541\n",
            "Epoch: 1745 - Train Loss: 0.0060965935699641705 - Test Loss: 0.02107365056872368\n",
            "Epoch: 1746 - Train Loss: 0.006075005512684584 - Test Loss: 0.021050430834293365\n",
            "Epoch: 1747 - Train Loss: 0.006053491495549679 - Test Loss: 0.021027283743023872\n",
            "Epoch: 1748 - Train Loss: 0.006032050121575594 - Test Loss: 0.021004218608140945\n",
            "Epoch: 1749 - Train Loss: 0.0060106851160526276 - Test Loss: 0.020981229841709137\n",
            "Epoch: 1750 - Train Loss: 0.005989391356706619 - Test Loss: 0.020958319306373596\n",
            "Epoch: 1751 - Train Loss: 0.005968174431473017 - Test Loss: 0.02093549072742462\n",
            "Epoch: 1752 - Train Loss: 0.005947025492787361 - Test Loss: 0.020912734791636467\n",
            "Epoch: 1753 - Train Loss: 0.005925951991230249 - Test Loss: 0.02089005894958973\n",
            "Epoch: 1754 - Train Loss: 0.005904949735850096 - Test Loss: 0.02086745575070381\n",
            "Epoch: 1755 - Train Loss: 0.005884023383259773 - Test Loss: 0.02084493450820446\n",
            "Epoch: 1756 - Train Loss: 0.005863162688910961 - Test Loss: 0.020822487771511078\n",
            "Epoch: 1757 - Train Loss: 0.005842380225658417 - Test Loss: 0.020800119265913963\n",
            "Epoch: 1758 - Train Loss: 0.005821665748953819 - Test Loss: 0.02077782340347767\n",
            "Epoch: 1759 - Train Loss: 0.005801019724458456 - Test Loss: 0.020755602046847343\n",
            "Epoch: 1760 - Train Loss: 0.005780449602752924 - Test Loss: 0.020733458921313286\n",
            "Epoch: 1761 - Train Loss: 0.005759947933256626 - Test Loss: 0.020711390301585197\n",
            "Epoch: 1762 - Train Loss: 0.005739519372582436 - Test Loss: 0.020689398050308228\n",
            "Epoch: 1763 - Train Loss: 0.005719160661101341 - Test Loss: 0.020667482167482376\n",
            "Epoch: 1764 - Train Loss: 0.005698871333152056 - Test Loss: 0.020645638927817345\n",
            "Epoch: 1765 - Train Loss: 0.00567864952608943 - Test Loss: 0.020623870193958282\n",
            "Epoch: 1766 - Train Loss: 0.005658500827848911 - Test Loss: 0.02060217410326004\n",
            "Epoch: 1767 - Train Loss: 0.005638419650495052 - Test Loss: 0.020580554381012917\n",
            "Epoch: 1768 - Train Loss: 0.005618406925350428 - Test Loss: 0.020559003576636314\n",
            "Epoch: 1769 - Train Loss: 0.00559846218675375 - Test Loss: 0.02053752727806568\n",
            "Epoch: 1770 - Train Loss: 0.005578592419624329 - Test Loss: 0.020516132935881615\n",
            "Epoch: 1771 - Train Loss: 0.005558783188462257 - Test Loss: 0.020494800060987473\n",
            "Epoch: 1772 - Train Loss: 0.005539047531783581 - Test Loss: 0.020473549142479897\n",
            "Epoch: 1773 - Train Loss: 0.005519379395991564 - Test Loss: 0.020452365279197693\n",
            "Epoch: 1774 - Train Loss: 0.005499778315424919 - Test Loss: 0.020431257784366608\n",
            "Epoch: 1775 - Train Loss: 0.005480242893099785 - Test Loss: 0.020410215482115746\n",
            "Epoch: 1776 - Train Loss: 0.00546077499166131 - Test Loss: 0.020389247685670853\n",
            "Epoch: 1777 - Train Loss: 0.005441376939415932 - Test Loss: 0.020368356257677078\n",
            "Epoch: 1778 - Train Loss: 0.005422044079750776 - Test Loss: 0.020347531884908676\n",
            "Epoch: 1779 - Train Loss: 0.005402781069278717 - Test Loss: 0.020326783880591393\n",
            "Epoch: 1780 - Train Loss: 0.005383579526096582 - Test Loss: 0.020306099206209183\n",
            "Epoch: 1781 - Train Loss: 0.005364446435123682 - Test Loss: 0.02028549090027809\n",
            "Epoch: 1782 - Train Loss: 0.005345378536731005 - Test Loss: 0.020264949649572372\n",
            "Epoch: 1783 - Train Loss: 0.005326378159224987 - Test Loss: 0.020244481042027473\n",
            "Epoch: 1784 - Train Loss: 0.005307439714670181 - Test Loss: 0.020224077627062798\n",
            "Epoch: 1785 - Train Loss: 0.005288568791002035 - Test Loss: 0.02020374685525894\n",
            "Epoch: 1786 - Train Loss: 0.00526976166293025 - Test Loss: 0.020183486863970757\n",
            "Epoch: 1787 - Train Loss: 0.0052510215900838375 - Test Loss: 0.020163295790553093\n",
            "Epoch: 1788 - Train Loss: 0.005232345778495073 - Test Loss: 0.0201431754976511\n",
            "Epoch: 1789 - Train Loss: 0.005213732831180096 - Test Loss: 0.020123116672039032\n",
            "Epoch: 1790 - Train Loss: 0.005195183679461479 - Test Loss: 0.020103134214878082\n",
            "Epoch: 1791 - Train Loss: 0.005176699720323086 - Test Loss: 0.020083215087652206\n",
            "Epoch: 1792 - Train Loss: 0.005158279091119766 - Test Loss: 0.02006336860358715\n",
            "Epoch: 1793 - Train Loss: 0.005139920860528946 - Test Loss: 0.02004358172416687\n",
            "Epoch: 1794 - Train Loss: 0.005121625028550625 - Test Loss: 0.02002386562526226\n",
            "Epoch: 1795 - Train Loss: 0.005103394389152527 - Test Loss: 0.020004218444228172\n",
            "Epoch: 1796 - Train Loss: 0.005085225682705641 - Test Loss: 0.019984640181064606\n",
            "Epoch: 1797 - Train Loss: 0.005067119374871254 - Test Loss: 0.019965127110481262\n",
            "Epoch: 1798 - Train Loss: 0.005049076862633228 - Test Loss: 0.01994568295776844\n",
            "Epoch: 1799 - Train Loss: 0.0050310962833464146 - Test Loss: 0.01992630586028099\n",
            "Epoch: 1800 - Train Loss: 0.005013176240026951 - Test Loss: 0.019906990230083466\n",
            "Epoch: 1801 - Train Loss: 0.004995317198336124 - Test Loss: 0.019887741655111313\n",
            "Epoch: 1802 - Train Loss: 0.004977521486580372 - Test Loss: 0.019868560135364532\n",
            "Epoch: 1803 - Train Loss: 0.004959786776453257 - Test Loss: 0.019849441945552826\n",
            "Epoch: 1804 - Train Loss: 0.004942111670970917 - Test Loss: 0.01983039081096649\n",
            "Epoch: 1805 - Train Loss: 0.004924497101455927 - Test Loss: 0.01981140486896038\n",
            "Epoch: 1806 - Train Loss: 0.004906945396214724 - Test Loss: 0.019792484119534492\n",
            "Epoch: 1807 - Train Loss: 0.004889452829957008 - Test Loss: 0.01977362670004368\n",
            "Epoch: 1808 - Train Loss: 0.004872020334005356 - Test Loss: 0.019754832610487938\n",
            "Epoch: 1809 - Train Loss: 0.004854647442698479 - Test Loss: 0.01973610371351242\n",
            "Epoch: 1810 - Train Loss: 0.0048373364843428135 - Test Loss: 0.019717440009117126\n",
            "Epoch: 1811 - Train Loss: 0.004820084199309349 - Test Loss: 0.019698841497302055\n",
            "Epoch: 1812 - Train Loss: 0.004802891053259373 - Test Loss: 0.01968030259013176\n",
            "Epoch: 1813 - Train Loss: 0.00478575611487031 - Test Loss: 0.019661827012896538\n",
            "Epoch: 1814 - Train Loss: 0.004768683109432459 - Test Loss: 0.019643418490886688\n",
            "Epoch: 1815 - Train Loss: 0.004751667845994234 - Test Loss: 0.019625073298811913\n",
            "Epoch: 1816 - Train Loss: 0.00473471125587821 - Test Loss: 0.019606787711381912\n",
            "Epoch: 1817 - Train Loss: 0.004717811942100525 - Test Loss: 0.019588565453886986\n",
            "Epoch: 1818 - Train Loss: 0.004700973629951477 - Test Loss: 0.01957041025161743\n",
            "Epoch: 1819 - Train Loss: 0.004684192128479481 - Test Loss: 0.019552314653992653\n",
            "Epoch: 1820 - Train Loss: 0.00466747023165226 - Test Loss: 0.019534284248948097\n",
            "Epoch: 1821 - Train Loss: 0.004650806076824665 - Test Loss: 0.019516311585903168\n",
            "Epoch: 1822 - Train Loss: 0.004634195938706398 - Test Loss: 0.01949840411543846\n",
            "Epoch: 1823 - Train Loss: 0.004617647267878056 - Test Loss: 0.01948055624961853\n",
            "Epoch: 1824 - Train Loss: 0.004601153079420328 - Test Loss: 0.019462769851088524\n",
            "Epoch: 1825 - Train Loss: 0.00458471430465579 - Test Loss: 0.019445041194558144\n",
            "Epoch: 1826 - Train Loss: 0.004568334668874741 - Test Loss: 0.019427377730607986\n",
            "Epoch: 1827 - Train Loss: 0.0045520104467868805 - Test Loss: 0.019409772008657455\n",
            "Epoch: 1828 - Train Loss: 0.004535744898021221 - Test Loss: 0.019392231479287148\n",
            "Epoch: 1829 - Train Loss: 0.004519535228610039 - Test Loss: 0.019374748691916466\n",
            "Epoch: 1830 - Train Loss: 0.0045033772476017475 - Test Loss: 0.01935732364654541\n",
            "Epoch: 1831 - Train Loss: 0.004487279802560806 - Test Loss: 0.01933996193110943\n",
            "Epoch: 1832 - Train Loss: 0.004471234977245331 - Test Loss: 0.019322656095027924\n",
            "Epoch: 1833 - Train Loss: 0.004455244168639183 - Test Loss: 0.019305408000946045\n",
            "Epoch: 1834 - Train Loss: 0.0044393111020326614 - Test Loss: 0.01928822696208954\n",
            "Epoch: 1835 - Train Loss: 0.0044234320521354675 - Test Loss: 0.01927109621465206\n",
            "Epoch: 1836 - Train Loss: 0.0044076102785766125 - Test Loss: 0.019254034385085106\n",
            "Epoch: 1837 - Train Loss: 0.0043918415904045105 - Test Loss: 0.01923702284693718\n",
            "Epoch: 1838 - Train Loss: 0.004376124124974012 - Test Loss: 0.01922006905078888\n",
            "Epoch: 1839 - Train Loss: 0.004360465798527002 - Test Loss: 0.019203180447220802\n",
            "Epoch: 1840 - Train Loss: 0.0043448577634990215 - Test Loss: 0.019186345860362053\n",
            "Epoch: 1841 - Train Loss: 0.004329302813857794 - Test Loss: 0.01916956715285778\n",
            "Epoch: 1842 - Train Loss: 0.004313803743571043 - Test Loss: 0.019152848049998283\n",
            "Epoch: 1843 - Train Loss: 0.004298357293009758 - Test Loss: 0.019136186689138412\n",
            "Epoch: 1844 - Train Loss: 0.004282963462173939 - Test Loss: 0.01911957748234272\n",
            "Epoch: 1845 - Train Loss: 0.004267624579370022 - Test Loss: 0.0191030316054821\n",
            "Epoch: 1846 - Train Loss: 0.004252337384968996 - Test Loss: 0.01908653974533081\n",
            "Epoch: 1847 - Train Loss: 0.0042371004819869995 - Test Loss: 0.019070101901888847\n",
            "Epoch: 1848 - Train Loss: 0.004221920855343342 - Test Loss: 0.01905372366309166\n",
            "Epoch: 1849 - Train Loss: 0.0042067901231348515 - Test Loss: 0.01903740130364895\n",
            "Epoch: 1850 - Train Loss: 0.004191710148006678 - Test Loss: 0.019021132960915565\n",
            "Epoch: 1851 - Train Loss: 0.004176684655249119 - Test Loss: 0.019004924222826958\n",
            "Epoch: 1852 - Train Loss: 0.004161710850894451 - Test Loss: 0.01898876763880253\n",
            "Epoch: 1853 - Train Loss: 0.0041467901319265366 - Test Loss: 0.018972672522068024\n",
            "Epoch: 1854 - Train Loss: 0.004131919704377651 - Test Loss: 0.018956627696752548\n",
            "Epoch: 1855 - Train Loss: 0.004117098171263933 - Test Loss: 0.01894063875079155\n",
            "Epoch: 1856 - Train Loss: 0.004102332517504692 - Test Loss: 0.018924709409475327\n",
            "Epoch: 1857 - Train Loss: 0.004087614361196756 - Test Loss: 0.018908832222223282\n",
            "Epoch: 1858 - Train Loss: 0.004072946030646563 - Test Loss: 0.018893005326390266\n",
            "Epoch: 1859 - Train Loss: 0.004058330785483122 - Test Loss: 0.018877239897847176\n",
            "Epoch: 1860 - Train Loss: 0.004043765366077423 - Test Loss: 0.018861524760723114\n",
            "Epoch: 1861 - Train Loss: 0.0040292516350746155 - Test Loss: 0.01884586736559868\n",
            "Epoch: 1862 - Train Loss: 0.00401478772982955 - Test Loss: 0.018830260261893272\n",
            "Epoch: 1863 - Train Loss: 0.004000369925051928 - Test Loss: 0.018814703449606895\n",
            "Epoch: 1864 - Train Loss: 0.003986004739999771 - Test Loss: 0.018799206241965294\n",
            "Epoch: 1865 - Train Loss: 0.003971688449382782 - Test Loss: 0.01878375932574272\n",
            "Epoch: 1866 - Train Loss: 0.003957422915846109 - Test Loss: 0.018768370151519775\n",
            "Epoch: 1867 - Train Loss: 0.003943203017115593 - Test Loss: 0.01875302568078041\n",
            "Epoch: 1868 - Train Loss: 0.003929035272449255 - Test Loss: 0.01873774081468582\n",
            "Epoch: 1869 - Train Loss: 0.003914914559572935 - Test Loss: 0.01872250624001026\n",
            "Epoch: 1870 - Train Loss: 0.003900845069438219 - Test Loss: 0.01870732754468918\n",
            "Epoch: 1871 - Train Loss: 0.003886820748448372 - Test Loss: 0.018692195415496826\n",
            "Epoch: 1872 - Train Loss: 0.0038728464860469103 - Test Loss: 0.01867711916565895\n",
            "Epoch: 1873 - Train Loss: 0.0038589199539273977 - Test Loss: 0.018662091344594955\n",
            "Epoch: 1874 - Train Loss: 0.003845043247565627 - Test Loss: 0.018647123128175735\n",
            "Epoch: 1875 - Train Loss: 0.0038312117103487253 - Test Loss: 0.018632199615240097\n",
            "Epoch: 1876 - Train Loss: 0.0038174265064299107 - Test Loss: 0.01861732453107834\n",
            "Epoch: 1877 - Train Loss: 0.0038036915939301252 - Test Loss: 0.018602507188916206\n",
            "Epoch: 1878 - Train Loss: 0.0037900025490671396 - Test Loss: 0.018587734550237656\n",
            "Epoch: 1879 - Train Loss: 0.003776363329961896 - Test Loss: 0.01857302151620388\n",
            "Epoch: 1880 - Train Loss: 0.003762770676985383 - Test Loss: 0.018558355048298836\n",
            "Epoch: 1881 - Train Loss: 0.0037492215633392334 - Test Loss: 0.01854373700916767\n",
            "Epoch: 1882 - Train Loss: 0.003735723439604044 - Test Loss: 0.018529172986745834\n",
            "Epoch: 1883 - Train Loss: 0.0037222695536911488 - Test Loss: 0.018514657393097878\n",
            "Epoch: 1884 - Train Loss: 0.0037088606040924788 - Test Loss: 0.0185001902282238\n",
            "Epoch: 1885 - Train Loss: 0.0036955007817596197 - Test Loss: 0.01848577708005905\n",
            "Epoch: 1886 - Train Loss: 0.003682186361402273 - Test Loss: 0.01847141422331333\n",
            "Epoch: 1887 - Train Loss: 0.003668916877359152 - Test Loss: 0.018457097932696342\n",
            "Epoch: 1888 - Train Loss: 0.003655695356428623 - Test Loss: 0.018442831933498383\n",
            "Epoch: 1889 - Train Loss: 0.0036425187718123198 - Test Loss: 0.018428616225719452\n",
            "Epoch: 1890 - Train Loss: 0.0036293852608650923 - Test Loss: 0.01841445080935955\n",
            "Epoch: 1891 - Train Loss: 0.0036163011100143194 - Test Loss: 0.01840033195912838\n",
            "Epoch: 1892 - Train Loss: 0.0036032593343406916 - Test Loss: 0.01838626340031624\n",
            "Epoch: 1893 - Train Loss: 0.0035902622621506453 - Test Loss: 0.018372241407632828\n",
            "Epoch: 1894 - Train Loss: 0.0035773117560893297 - Test Loss: 0.018358269706368446\n",
            "Epoch: 1895 - Train Loss: 0.0035644054878503084 - Test Loss: 0.018344346433877945\n",
            "Epoch: 1896 - Train Loss: 0.003551542991772294 - Test Loss: 0.018330469727516174\n",
            "Epoch: 1897 - Train Loss: 0.0035387245006859303 - Test Loss: 0.018316641449928284\n",
            "Epoch: 1898 - Train Loss: 0.0035259525757282972 - Test Loss: 0.018302863463759422\n",
            "Epoch: 1899 - Train Loss: 0.003513224422931671 - Test Loss: 0.01828913390636444\n",
            "Epoch: 1900 - Train Loss: 0.003500539343804121 - Test Loss: 0.01827544905245304\n",
            "Epoch: 1901 - Train Loss: 0.003487897804006934 - Test Loss: 0.01826181448996067\n",
            "Epoch: 1902 - Train Loss: 0.003475301433354616 - Test Loss: 0.01824822835624218\n",
            "Epoch: 1903 - Train Loss: 0.003462748834863305 - Test Loss: 0.01823468506336212\n",
            "Epoch: 1904 - Train Loss: 0.003450238611549139 - Test Loss: 0.018221192061901093\n",
            "Epoch: 1905 - Train Loss: 0.0034377712290734053 - Test Loss: 0.018207743763923645\n",
            "Epoch: 1906 - Train Loss: 0.0034253469202667475 - Test Loss: 0.018194343894720078\n",
            "Epoch: 1907 - Train Loss: 0.003412964753806591 - Test Loss: 0.018180986866354942\n",
            "Epoch: 1908 - Train Loss: 0.0034006298519670963 - Test Loss: 0.018167681992053986\n",
            "Epoch: 1909 - Train Loss: 0.0033883319701999426 - Test Loss: 0.01815441995859146\n",
            "Epoch: 1910 - Train Loss: 0.003376081120222807 - Test Loss: 0.018141204491257668\n",
            "Epoch: 1911 - Train Loss: 0.0033638698514550924 - Test Loss: 0.018128035590052605\n",
            "Epoch: 1912 - Train Loss: 0.0033517004922032356 - Test Loss: 0.018114911392331123\n",
            "Epoch: 1913 - Train Loss: 0.003339574672281742 - Test Loss: 0.018101831898093224\n",
            "Epoch: 1914 - Train Loss: 0.0033274907618761063 - Test Loss: 0.018088800832629204\n",
            "Epoch: 1915 - Train Loss: 0.003315447364002466 - Test Loss: 0.018075810745358467\n",
            "Epoch: 1916 - Train Loss: 0.003303447738289833 - Test Loss: 0.01806286908686161\n",
            "Epoch: 1917 - Train Loss: 0.0032914895564317703 - Test Loss: 0.018049970269203186\n",
            "Epoch: 1918 - Train Loss: 0.003279569558799267 - Test Loss: 0.01803711988031864\n",
            "Epoch: 1919 - Train Loss: 0.0032676951959729195 - Test Loss: 0.01802431233227253\n",
            "Epoch: 1920 - Train Loss: 0.0032558590173721313 - Test Loss: 0.01801155135035515\n",
            "Epoch: 1921 - Train Loss: 0.0032440631184726954 - Test Loss: 0.01799883134663105\n",
            "Epoch: 1922 - Train Loss: 0.0032323100604116917 - Test Loss: 0.017986156046390533\n",
            "Epoch: 1923 - Train Loss: 0.0032205975148826838 - Test Loss: 0.017973527312278748\n",
            "Epoch: 1924 - Train Loss: 0.0032089250162243843 - Test Loss: 0.017960939556360245\n",
            "Epoch: 1925 - Train Loss: 0.003197292098775506 - Test Loss: 0.017948396503925323\n",
            "Epoch: 1926 - Train Loss: 0.003185701323673129 - Test Loss: 0.017935901880264282\n",
            "Epoch: 1927 - Train Loss: 0.003174150362610817 - Test Loss: 0.017923444509506226\n",
            "Epoch: 1928 - Train Loss: 0.003162637585774064 - Test Loss: 0.0179110337048769\n",
            "Epoch: 1929 - Train Loss: 0.003151169279590249 - Test Loss: 0.017898667603731155\n",
            "Epoch: 1930 - Train Loss: 0.003139738691970706 - Test Loss: 0.017886346206068993\n",
            "Epoch: 1931 - Train Loss: 0.003128347685560584 - Test Loss: 0.017874063923954964\n",
            "Epoch: 1932 - Train Loss: 0.003116999054327607 - Test Loss: 0.017861831933259964\n",
            "Epoch: 1933 - Train Loss: 0.003105689538642764 - Test Loss: 0.017849642783403397\n",
            "Epoch: 1934 - Train Loss: 0.0030944189056754112 - Test Loss: 0.017837490886449814\n",
            "Epoch: 1935 - Train Loss: 0.0030831899493932724 - Test Loss: 0.01782538741827011\n",
            "Epoch: 1936 - Train Loss: 0.0030719994101673365 - Test Loss: 0.01781332492828369\n",
            "Epoch: 1937 - Train Loss: 0.003060846356675029 - Test Loss: 0.017801303416490555\n",
            "Epoch: 1938 - Train Loss: 0.0030497354455292225 - Test Loss: 0.017789330333471298\n",
            "Epoch: 1939 - Train Loss: 0.003038660855963826 - Test Loss: 0.017777396366000175\n",
            "Epoch: 1940 - Train Loss: 0.0030276253819465637 - Test Loss: 0.017765501514077187\n",
            "Epoch: 1941 - Train Loss: 0.0030166292563080788 - Test Loss: 0.01775364950299263\n",
            "Epoch: 1942 - Train Loss: 0.003005671314895153 - Test Loss: 0.017741844058036804\n",
            "Epoch: 1943 - Train Loss: 0.00299475179053843 - Test Loss: 0.017730077728629112\n",
            "Epoch: 1944 - Train Loss: 0.0029838697519153357 - Test Loss: 0.017718354240059853\n",
            "Epoch: 1945 - Train Loss: 0.0029730272945016623 - Test Loss: 0.017706668004393578\n",
            "Epoch: 1946 - Train Loss: 0.0029622220899909735 - Test Loss: 0.017695026472210884\n",
            "Epoch: 1947 - Train Loss: 0.00295145227573812 - Test Loss: 0.017683424055576324\n",
            "Epoch: 1948 - Train Loss: 0.0029407236725091934 - Test Loss: 0.017671864479780197\n",
            "Epoch: 1949 - Train Loss: 0.0029300302267074585 - Test Loss: 0.017660344019532204\n",
            "Epoch: 1950 - Train Loss: 0.0029193733353167772 - Test Loss: 0.017648864537477493\n",
            "Epoch: 1951 - Train Loss: 0.002908754860982299 - Test Loss: 0.017637426033616066\n",
            "Epoch: 1952 - Train Loss: 0.002898173639550805 - Test Loss: 0.017626028507947922\n",
            "Epoch: 1953 - Train Loss: 0.002887628274038434 - Test Loss: 0.01761467009782791\n",
            "Epoch: 1954 - Train Loss: 0.0028771196957677603 - Test Loss: 0.017603352665901184\n",
            "Epoch: 1955 - Train Loss: 0.0028666488360613585 - Test Loss: 0.01759207248687744\n",
            "Epoch: 1956 - Train Loss: 0.0028562142979353666 - Test Loss: 0.017580833286046982\n",
            "Epoch: 1957 - Train Loss: 0.002845813287422061 - Test Loss: 0.017569633200764656\n",
            "Epoch: 1958 - Train Loss: 0.0028354523237794638 - Test Loss: 0.017558475956320763\n",
            "Epoch: 1959 - Train Loss: 0.002825123257935047 - Test Loss: 0.017547354102134705\n",
            "Epoch: 1960 - Train Loss: 0.0028148340061306953 - Test Loss: 0.01753627136349678\n",
            "Epoch: 1961 - Train Loss: 0.002804579446092248 - Test Loss: 0.01752523146569729\n",
            "Epoch: 1962 - Train Loss: 0.0027943605091422796 - Test Loss: 0.017514226958155632\n",
            "Epoch: 1963 - Train Loss: 0.002784176729619503 - Test Loss: 0.01750326342880726\n",
            "Epoch: 1964 - Train Loss: 0.002774027641862631 - Test Loss: 0.01749233528971672\n",
            "Epoch: 1965 - Train Loss: 0.002763913944363594 - Test Loss: 0.017481448128819466\n",
            "Epoch: 1966 - Train Loss: 0.002753834705799818 - Test Loss: 0.017470596358180046\n",
            "Epoch: 1967 - Train Loss: 0.0027437948156148195 - Test Loss: 0.017459789291024208\n",
            "Epoch: 1968 - Train Loss: 0.0027337847277522087 - Test Loss: 0.017449015751481056\n",
            "Epoch: 1969 - Train Loss: 0.002723813522607088 - Test Loss: 0.017438281327486038\n",
            "Epoch: 1970 - Train Loss: 0.0027138744480907917 - Test Loss: 0.017427586019039154\n",
            "Epoch: 1971 - Train Loss: 0.002703969832509756 - Test Loss: 0.017416926100850105\n",
            "Epoch: 1972 - Train Loss: 0.002694101072847843 - Test Loss: 0.017406301572918892\n",
            "Epoch: 1973 - Train Loss: 0.00268426607362926 - Test Loss: 0.01739571802318096\n",
            "Epoch: 1974 - Train Loss: 0.0026744648348540068 - Test Loss: 0.017385171726346016\n",
            "Epoch: 1975 - Train Loss: 0.0026646980550140142 - Test Loss: 0.017374662682414055\n",
            "Epoch: 1976 - Train Loss: 0.002654966665431857 - Test Loss: 0.017364192754030228\n",
            "Epoch: 1977 - Train Loss: 0.00264526903629303 - Test Loss: 0.017353756353259087\n",
            "Epoch: 1978 - Train Loss: 0.0026356030721217394 - Test Loss: 0.01734335720539093\n",
            "Epoch: 1979 - Train Loss: 0.0026259738951921463 - Test Loss: 0.017332997173070908\n",
            "Epoch: 1980 - Train Loss: 0.00261637382209301 - Test Loss: 0.017322668805718422\n",
            "Epoch: 1981 - Train Loss: 0.0026068114675581455 - Test Loss: 0.01731238327920437\n",
            "Epoch: 1982 - Train Loss: 0.0025972817093133926 - Test Loss: 0.017302131280303\n",
            "Epoch: 1983 - Train Loss: 0.0025877852458506823 - Test Loss: 0.017291920259594917\n",
            "Epoch: 1984 - Train Loss: 0.0025783211458474398 - Test Loss: 0.01728174276649952\n",
            "Epoch: 1985 - Train Loss: 0.002568890107795596 - Test Loss: 0.017271598801016808\n",
            "Epoch: 1986 - Train Loss: 0.0025594914332032204 - Test Loss: 0.01726149208843708\n",
            "Epoch: 1987 - Train Loss: 0.002550124889239669 - Test Loss: 0.01725142076611519\n",
            "Epoch: 1988 - Train Loss: 0.0025407925713807344 - Test Loss: 0.017241384834051132\n",
            "Epoch: 1989 - Train Loss: 0.0025314930826425552 - Test Loss: 0.01723138615489006\n",
            "Epoch: 1990 - Train Loss: 0.0025222254917025566 - Test Loss: 0.017221421003341675\n",
            "Epoch: 1991 - Train Loss: 0.0025129893328994513 - Test Loss: 0.017211491242051125\n",
            "Epoch: 1992 - Train Loss: 0.002503787400200963 - Test Loss: 0.01720159873366356\n",
            "Epoch: 1993 - Train Loss: 0.00249461573548615 - Test Loss: 0.01719173975288868\n",
            "Epoch: 1994 - Train Loss: 0.0024854771327227354 - Test Loss: 0.017181916162371635\n",
            "Epoch: 1995 - Train Loss: 0.0024763704277575016 - Test Loss: 0.017172129824757576\n",
            "Epoch: 1996 - Train Loss: 0.0024672949220985174 - Test Loss: 0.017162375152111053\n",
            "Epoch: 1997 - Train Loss: 0.0024582529440522194 - Test Loss: 0.017152659595012665\n",
            "Epoch: 1998 - Train Loss: 0.0024492403026670218 - Test Loss: 0.017142973840236664\n",
            "Epoch: 1999 - Train Loss: 0.0024402583949267864 - Test Loss: 0.017133323475718498\n",
            "Epoch: 2000 - Train Loss: 0.002431311644613743 - Test Loss: 0.017123712226748466\n",
            "Epoch: 2001 - Train Loss: 0.0024223916698247194 - Test Loss: 0.017114127054810524\n",
            "Epoch: 2002 - Train Loss: 0.002413506619632244 - Test Loss: 0.017104579135775566\n",
            "Epoch: 2003 - Train Loss: 0.002404650440439582 - Test Loss: 0.017095066606998444\n",
            "Epoch: 2004 - Train Loss: 0.0023958247620612383 - Test Loss: 0.017085587605834007\n",
            "Epoch: 2005 - Train Loss: 0.0023870314471423626 - Test Loss: 0.017076140269637108\n",
            "Epoch: 2006 - Train Loss: 0.002378268400207162 - Test Loss: 0.017066728323698044\n",
            "Epoch: 2007 - Train Loss: 0.002369535621255636 - Test Loss: 0.017057349905371666\n",
            "Epoch: 2008 - Train Loss: 0.0023608331102877855 - Test Loss: 0.017048006877303123\n",
            "Epoch: 2009 - Train Loss: 0.0023521629627794027 - Test Loss: 0.017038695514202118\n",
            "Epoch: 2010 - Train Loss: 0.0023435226175934076 - Test Loss: 0.0170294176787138\n",
            "Epoch: 2011 - Train Loss: 0.0023349104449152946 - Test Loss: 0.017020173370838165\n",
            "Epoch: 2012 - Train Loss: 0.002326331799849868 - Test Loss: 0.017010964453220367\n",
            "Epoch: 2013 - Train Loss: 0.0023177789989858866 - Test Loss: 0.01700177974998951\n",
            "Epoch: 2014 - Train Loss: 0.0023092597257345915 - Test Loss: 0.016992637887597084\n",
            "Epoch: 2015 - Train Loss: 0.0023007700219750404 - Test Loss: 0.016983525827527046\n",
            "Epoch: 2016 - Train Loss: 0.0022923077922314405 - Test Loss: 0.016974443569779396\n",
            "Epoch: 2017 - Train Loss: 0.0022838767617940903 - Test Loss: 0.016965394839644432\n",
            "Epoch: 2018 - Train Loss: 0.0022754750680178404 - Test Loss: 0.016956381499767303\n",
            "Epoch: 2019 - Train Loss: 0.00226710201241076 - Test Loss: 0.016947396099567413\n",
            "Epoch: 2020 - Train Loss: 0.0022587606217712164 - Test Loss: 0.01693844608962536\n",
            "Epoch: 2021 - Train Loss: 0.002250446006655693 - Test Loss: 0.01692952588200569\n",
            "Epoch: 2022 - Train Loss: 0.002242160029709339 - Test Loss: 0.016920635476708412\n",
            "Epoch: 2023 - Train Loss: 0.002233906416222453 - Test Loss: 0.016911782324314117\n",
            "Epoch: 2024 - Train Loss: 0.002225677715614438 - Test Loss: 0.016902955248951912\n",
            "Epoch: 2025 - Train Loss: 0.002217481378465891 - Test Loss: 0.016894163563847542\n",
            "Epoch: 2026 - Train Loss: 0.0022093113511800766 - Test Loss: 0.01688540354371071\n",
            "Epoch: 2027 - Train Loss: 0.002201169729232788 - Test Loss: 0.016876671463251114\n",
            "Epoch: 2028 - Train Loss: 0.002193058142438531 - Test Loss: 0.016867972910404205\n",
            "Epoch: 2029 - Train Loss: 0.002184974728152156 - Test Loss: 0.016859306022524834\n",
            "Epoch: 2030 - Train Loss: 0.0021769190207123756 - Test Loss: 0.016850670799613\n",
            "Epoch: 2031 - Train Loss: 0.0021688914857804775 - Test Loss: 0.016842061653733253\n",
            "Epoch: 2032 - Train Loss: 0.002160891890525818 - Test Loss: 0.016833489760756493\n",
            "Epoch: 2033 - Train Loss: 0.0021529218647629023 - Test Loss: 0.01682494580745697\n",
            "Epoch: 2034 - Train Loss: 0.002144978614524007 - Test Loss: 0.016816431656479836\n",
            "Epoch: 2035 - Train Loss: 0.0021370621398091316 - Test Loss: 0.016807951033115387\n",
            "Epoch: 2036 - Train Loss: 0.0021291763987392187 - Test Loss: 0.016799500212073326\n",
            "Epoch: 2037 - Train Loss: 0.0021213144063949585 - Test Loss: 0.016791075468063354\n",
            "Epoch: 2038 - Train Loss: 0.0021134831476956606 - Test Loss: 0.016782686114311218\n",
            "Epoch: 2039 - Train Loss: 0.00210567913018167 - Test Loss: 0.01677432470023632\n",
            "Epoch: 2040 - Train Loss: 0.002097900491207838 - Test Loss: 0.016765989363193512\n",
            "Epoch: 2041 - Train Loss: 0.0020901502575725317 - Test Loss: 0.016757691279053688\n",
            "Epoch: 2042 - Train Loss: 0.0020824274979531765 - Test Loss: 0.016749421134591103\n",
            "Epoch: 2043 - Train Loss: 0.002074731281027198 - Test Loss: 0.016741177067160606\n",
            "Epoch: 2044 - Train Loss: 0.002067063469439745 - Test Loss: 0.016732966527342796\n",
            "Epoch: 2045 - Train Loss: 0.002059420570731163 - Test Loss: 0.016724783927202225\n",
            "Epoch: 2046 - Train Loss: 0.002051803981885314 - Test Loss: 0.01671662926673889\n",
            "Epoch: 2047 - Train Loss: 0.0020442174281924963 - Test Loss: 0.016708508133888245\n",
            "Epoch: 2048 - Train Loss: 0.0020366539247334003 - Test Loss: 0.016700414940714836\n",
            "Epoch: 2049 - Train Loss: 0.002029120223596692 - Test Loss: 0.016692349687218666\n",
            "Epoch: 2050 - Train Loss: 0.002021610736846924 - Test Loss: 0.016684314236044884\n",
            "Epoch: 2051 - Train Loss: 0.0020141275599598885 - Test Loss: 0.01667630672454834\n",
            "Epoch: 2052 - Train Loss: 0.002006672089919448 - Test Loss: 0.016668327152729034\n",
            "Epoch: 2053 - Train Loss: 0.0019992422312498093 - Test Loss: 0.016660382971167564\n",
            "Epoch: 2054 - Train Loss: 0.0019918386824429035 - Test Loss: 0.016652461141347885\n",
            "Epoch: 2055 - Train Loss: 0.001984460512176156 - Test Loss: 0.016644567251205444\n",
            "Epoch: 2056 - Train Loss: 0.0019771079532802105 - Test Loss: 0.01663670502603054\n",
            "Epoch: 2057 - Train Loss: 0.0019697826355695724 - Test Loss: 0.016628872603178024\n",
            "Epoch: 2058 - Train Loss: 0.0019624829292297363 - Test Loss: 0.016621064394712448\n",
            "Epoch: 2059 - Train Loss: 0.0019552065059542656 - Test Loss: 0.01661328598856926\n",
            "Epoch: 2060 - Train Loss: 0.001947958953678608 - Test Loss: 0.016605539247393608\n",
            "Epoch: 2061 - Train Loss: 0.0019407332874834538 - Test Loss: 0.016597812995314598\n",
            "Epoch: 2062 - Train Loss: 0.0019335360266268253 - Test Loss: 0.016590118408203125\n",
            "Epoch: 2063 - Train Loss: 0.0019263626309111714 - Test Loss: 0.01658245176076889\n",
            "Epoch: 2064 - Train Loss: 0.0019192136824131012 - Test Loss: 0.016574811190366745\n",
            "Epoch: 2065 - Train Loss: 0.0019120913930237293 - Test Loss: 0.016567198559641838\n",
            "Epoch: 2066 - Train Loss: 0.0019049940165132284 - Test Loss: 0.01655961573123932\n",
            "Epoch: 2067 - Train Loss: 0.0018979213200509548 - Test Loss: 0.016552060842514038\n",
            "Epoch: 2068 - Train Loss: 0.0018908728379756212 - Test Loss: 0.016544528305530548\n",
            "Epoch: 2069 - Train Loss: 0.0018838493851944804 - Test Loss: 0.016537027433514595\n",
            "Epoch: 2070 - Train Loss: 0.0018768515437841415 - Test Loss: 0.01652955450117588\n",
            "Epoch: 2071 - Train Loss: 0.0018698780331760645 - Test Loss: 0.016522105783224106\n",
            "Epoch: 2072 - Train Loss: 0.0018629278056323528 - Test Loss: 0.01651468314230442\n",
            "Epoch: 2073 - Train Loss: 0.0018560043536126614 - Test Loss: 0.01650729402899742\n",
            "Epoch: 2074 - Train Loss: 0.0018491018563508987 - Test Loss: 0.016499923542141914\n",
            "Epoch: 2075 - Train Loss: 0.0018422267166897655 - Test Loss: 0.016492586582899094\n",
            "Epoch: 2076 - Train Loss: 0.0018353754421696067 - Test Loss: 0.016485275700688362\n",
            "Epoch: 2077 - Train Loss: 0.0018285464029759169 - Test Loss: 0.01647798717021942\n",
            "Epoch: 2078 - Train Loss: 0.0018217412289232016 - Test Loss: 0.01647072285413742\n",
            "Epoch: 2079 - Train Loss: 0.0018149618990719318 - Test Loss: 0.016463488340377808\n",
            "Epoch: 2080 - Train Loss: 0.0018082058522850275 - Test Loss: 0.016456279903650284\n",
            "Epoch: 2081 - Train Loss: 0.001801472739316523 - Test Loss: 0.0164490956813097\n",
            "Epoch: 2082 - Train Loss: 0.0017947652377188206 - Test Loss: 0.016441943123936653\n",
            "Epoch: 2083 - Train Loss: 0.0017880807863548398 - Test Loss: 0.016434814780950546\n",
            "Epoch: 2084 - Train Loss: 0.0017814182210713625 - Test Loss: 0.01642770878970623\n",
            "Epoch: 2085 - Train Loss: 0.0017747804522514343 - Test Loss: 0.016420632600784302\n",
            "Epoch: 2086 - Train Loss: 0.0017681658500805497 - Test Loss: 0.016413580626249313\n",
            "Epoch: 2087 - Train Loss: 0.0017615742981433868 - Test Loss: 0.016406554728746414\n",
            "Epoch: 2088 - Train Loss: 0.0017550059128552675 - Test Loss: 0.016399554908275604\n",
            "Epoch: 2089 - Train Loss: 0.00174846185836941 - Test Loss: 0.016392581164836884\n",
            "Epoch: 2090 - Train Loss: 0.0017419406212866306 - Test Loss: 0.016385633498430252\n",
            "Epoch: 2091 - Train Loss: 0.0017354408046230674 - Test Loss: 0.016378704458475113\n",
            "Epoch: 2092 - Train Loss: 0.0017289650859311223 - Test Loss: 0.01637180708348751\n",
            "Epoch: 2093 - Train Loss: 0.0017225120682269335 - Test Loss: 0.01636493392288685\n",
            "Epoch: 2094 - Train Loss: 0.0017160814022645354 - Test Loss: 0.016358084976673126\n",
            "Epoch: 2095 - Train Loss: 0.0017096750671043992 - Test Loss: 0.016351263970136642\n",
            "Epoch: 2096 - Train Loss: 0.00170328957028687 - Test Loss: 0.0163444634526968\n",
            "Epoch: 2097 - Train Loss: 0.0016969264252111316 - Test Loss: 0.016337687149643898\n",
            "Epoch: 2098 - Train Loss: 0.0016905867960304022 - Test Loss: 0.016330938786268234\n",
            "Epoch: 2099 - Train Loss: 0.001684269867837429 - Test Loss: 0.01632421463727951\n",
            "Epoch: 2100 - Train Loss: 0.0016779747093096375 - Test Loss: 0.016317512840032578\n",
            "Epoch: 2101 - Train Loss: 0.0016717020189389586 - Test Loss: 0.016310837119817734\n",
            "Epoch: 2102 - Train Loss: 0.0016654526116326451 - Test Loss: 0.01630418933928013\n",
            "Epoch: 2103 - Train Loss: 0.0016592234605923295 - Test Loss: 0.016297562047839165\n",
            "Epoch: 2104 - Train Loss: 0.0016530161956325173 - Test Loss: 0.016290957108139992\n",
            "Epoch: 2105 - Train Loss: 0.0016468324465677142 - Test Loss: 0.016284383833408356\n",
            "Epoch: 2106 - Train Loss: 0.001640670234337449 - Test Loss: 0.016277829185128212\n",
            "Epoch: 2107 - Train Loss: 0.0016345293261110783 - Test Loss: 0.016271300613880157\n",
            "Epoch: 2108 - Train Loss: 0.001628410303965211 - Test Loss: 0.016264794394373894\n",
            "Epoch: 2109 - Train Loss: 0.0016223143320530653 - Test Loss: 0.01625831425189972\n",
            "Epoch: 2110 - Train Loss: 0.0016162398969754577 - Test Loss: 0.016251858323812485\n",
            "Epoch: 2111 - Train Loss: 0.0016101852525025606 - Test Loss: 0.016245422884821892\n",
            "Epoch: 2112 - Train Loss: 0.0016041535418480635 - Test Loss: 0.016239015385508537\n",
            "Epoch: 2113 - Train Loss: 0.001598142902366817 - Test Loss: 0.016232630237936974\n",
            "Epoch: 2114 - Train Loss: 0.0015921532176434994 - Test Loss: 0.01622626557946205\n",
            "Epoch: 2115 - Train Loss: 0.0015861850697547197 - Test Loss: 0.01621992699801922\n",
            "Epoch: 2116 - Train Loss: 0.001580239157192409 - Test Loss: 0.016213612630963326\n",
            "Epoch: 2117 - Train Loss: 0.0015743128024041653 - Test Loss: 0.016207322478294373\n",
            "Epoch: 2118 - Train Loss: 0.0015684071695432067 - Test Loss: 0.016201047226786613\n",
            "Epoch: 2119 - Train Loss: 0.0015625241212546825 - Test Loss: 0.01619480550289154\n",
            "Epoch: 2120 - Train Loss: 0.0015566613292321563 - Test Loss: 0.01618858426809311\n",
            "Epoch: 2121 - Train Loss: 0.0015508192591369152 - Test Loss: 0.01618238352239132\n",
            "Epoch: 2122 - Train Loss: 0.0015449976781383157 - Test Loss: 0.01617620512843132\n",
            "Epoch: 2123 - Train Loss: 0.0015391980996355414 - Test Loss: 0.01617005467414856\n",
            "Epoch: 2124 - Train Loss: 0.001533418893814087 - Test Loss: 0.01616392284631729\n",
            "Epoch: 2125 - Train Loss: 0.0015276584308594465 - Test Loss: 0.016157811507582664\n",
            "Epoch: 2126 - Train Loss: 0.0015219199704006314 - Test Loss: 0.016151728108525276\n",
            "Epoch: 2127 - Train Loss: 0.001516201300546527 - Test Loss: 0.01614566519856453\n",
            "Epoch: 2128 - Train Loss: 0.001510503119789064 - Test Loss: 0.016139624640345573\n",
            "Epoch: 2129 - Train Loss: 0.0015048246132209897 - Test Loss: 0.01613360457122326\n",
            "Epoch: 2130 - Train Loss: 0.001499167992733419 - Test Loss: 0.016127610579133034\n",
            "Epoch: 2131 - Train Loss: 0.0014935294166207314 - Test Loss: 0.01612163707613945\n",
            "Epoch: 2132 - Train Loss: 0.0014879109803587198 - Test Loss: 0.016115684062242508\n",
            "Epoch: 2133 - Train Loss: 0.0014823134988546371 - Test Loss: 0.016109757125377655\n",
            "Epoch: 2134 - Train Loss: 0.0014767360407859087 - Test Loss: 0.016103848814964294\n",
            "Epoch: 2135 - Train Loss: 0.0014711781404912472 - Test Loss: 0.016097964718937874\n",
            "Epoch: 2136 - Train Loss: 0.0014656393323093653 - Test Loss: 0.016092099249362946\n",
            "Epoch: 2137 - Train Loss: 0.0014601218281313777 - Test Loss: 0.016086259856820107\n",
            "Epoch: 2138 - Train Loss: 0.0014546236488968134 - Test Loss: 0.01608043909072876\n",
            "Epoch: 2139 - Train Loss: 0.001449143746867776 - Test Loss: 0.016074640676379204\n",
            "Epoch: 2140 - Train Loss: 0.0014436831697821617 - Test Loss: 0.01606886088848114\n",
            "Epoch: 2141 - Train Loss: 0.0014382448280230165 - Test Loss: 0.016063107177615166\n",
            "Epoch: 2142 - Train Loss: 0.0014328226679936051 - Test Loss: 0.016057372093200684\n",
            "Epoch: 2143 - Train Loss: 0.0014274225104600191 - Test Loss: 0.016051659360527992\n",
            "Epoch: 2144 - Train Loss: 0.0014220403973013163 - Test Loss: 0.01604596897959709\n",
            "Epoch: 2145 - Train Loss: 0.0014166771434247494 - Test Loss: 0.016040300950407982\n",
            "Epoch: 2146 - Train Loss: 0.0014113340293988585 - Test Loss: 0.016034651547670364\n",
            "Epoch: 2147 - Train Loss: 0.0014060098910704255 - Test Loss: 0.01602902263402939\n",
            "Epoch: 2148 - Train Loss: 0.0014007044956088066 - Test Loss: 0.016023417934775352\n",
            "Epoch: 2149 - Train Loss: 0.0013954180758446455 - Test Loss: 0.016017833724617958\n",
            "Epoch: 2150 - Train Loss: 0.0013901501661166549 - Test Loss: 0.016012268140912056\n",
            "Epoch: 2151 - Train Loss: 0.0013849007664248347 - Test Loss: 0.016006721183657646\n",
            "Epoch: 2152 - Train Loss: 0.0013796701095998287 - Test Loss: 0.016001198440790176\n",
            "Epoch: 2153 - Train Loss: 0.0013744579628109932 - Test Loss: 0.01599569246172905\n",
            "Epoch: 2154 - Train Loss: 0.0013692653737962246 - Test Loss: 0.015990210697054863\n",
            "Epoch: 2155 - Train Loss: 0.0013640915276482701 - Test Loss: 0.015984753146767616\n",
            "Epoch: 2156 - Train Loss: 0.0013589360751211643 - Test Loss: 0.015979310497641563\n",
            "Epoch: 2157 - Train Loss: 0.0013537987833842635 - Test Loss: 0.01597389206290245\n",
            "Epoch: 2158 - Train Loss: 0.0013486793031916022 - Test Loss: 0.01596848852932453\n",
            "Epoch: 2159 - Train Loss: 0.0013435782166197896 - Test Loss: 0.015963109210133553\n",
            "Epoch: 2160 - Train Loss: 0.0013384963385760784 - Test Loss: 0.015957752242684364\n",
            "Epoch: 2161 - Train Loss: 0.0013334323884919286 - Test Loss: 0.01595241203904152\n",
            "Epoch: 2162 - Train Loss: 0.0013283849693834782 - Test Loss: 0.015947090461850166\n",
            "Epoch: 2163 - Train Loss: 0.001323356875218451 - Test Loss: 0.015941791236400604\n",
            "Epoch: 2164 - Train Loss: 0.0013183465925976634 - Test Loss: 0.015936510637402534\n",
            "Epoch: 2165 - Train Loss: 0.0013133538886904716 - Test Loss: 0.015931250527501106\n",
            "Epoch: 2166 - Train Loss: 0.0013083789963275194 - Test Loss: 0.01592600904405117\n",
            "Epoch: 2167 - Train Loss: 0.0013034229632467031 - Test Loss: 0.015920789912343025\n",
            "Epoch: 2168 - Train Loss: 0.0012984831118956208 - Test Loss: 0.015915589407086372\n",
            "Epoch: 2169 - Train Loss: 0.0012935609556734562 - Test Loss: 0.015910403802990913\n",
            "Epoch: 2170 - Train Loss: 0.0012886574259027839 - Test Loss: 0.015905240550637245\n",
            "Epoch: 2171 - Train Loss: 0.0012837713584303856 - Test Loss: 0.015900099650025368\n",
            "Epoch: 2172 - Train Loss: 0.0012789025204256177 - Test Loss: 0.015894977375864983\n",
            "Epoch: 2173 - Train Loss: 0.0012740509118884802 - Test Loss: 0.01588987186551094\n",
            "Epoch: 2174 - Train Loss: 0.0012692171148955822 - Test Loss: 0.01588478311896324\n",
            "Epoch: 2175 - Train Loss: 0.001264400314539671 - Test Loss: 0.015879720449447632\n",
            "Epoch: 2176 - Train Loss: 0.0012596020242199302 - Test Loss: 0.015874674543738365\n",
            "Epoch: 2177 - Train Loss: 0.001254820846952498 - Test Loss: 0.01586964540183544\n",
            "Epoch: 2178 - Train Loss: 0.0012500553857535124 - Test Loss: 0.01586463488638401\n",
            "Epoch: 2179 - Train Loss: 0.0012453082017600536 - Test Loss: 0.01585964672267437\n",
            "Epoch: 2180 - Train Loss: 0.0012405778979882598 - Test Loss: 0.01585467718541622\n",
            "Epoch: 2181 - Train Loss: 0.0012358644744381309 - Test Loss: 0.015849724411964417\n",
            "Epoch: 2182 - Train Loss: 0.001231167814694345 - Test Loss: 0.015844788402318954\n",
            "Epoch: 2183 - Train Loss: 0.0012264890829101205 - Test Loss: 0.015839876607060432\n",
            "Epoch: 2184 - Train Loss: 0.001221825834363699 - Test Loss: 0.015834981575608253\n",
            "Epoch: 2185 - Train Loss: 0.0012171793496236205 - Test Loss: 0.015830103307962418\n",
            "Epoch: 2186 - Train Loss: 0.001212550327181816 - Test Loss: 0.015825239941477776\n",
            "Epoch: 2187 - Train Loss: 0.0012079381849616766 - Test Loss: 0.015820400789380074\n",
            "Epoch: 2188 - Train Loss: 0.0012033421080559492 - Test Loss: 0.015815580263733864\n",
            "Epoch: 2189 - Train Loss: 0.0011987625621259212 - Test Loss: 0.015810778364539146\n",
            "Epoch: 2190 - Train Loss: 0.0011941990815103054 - Test Loss: 0.015805987641215324\n",
            "Epoch: 2191 - Train Loss: 0.001189652131870389 - Test Loss: 0.015801221132278442\n",
            "Epoch: 2192 - Train Loss: 0.001185122411698103 - Test Loss: 0.015796475112438202\n",
            "Epoch: 2193 - Train Loss: 0.001180608756840229 - Test Loss: 0.015791742131114006\n",
            "Epoch: 2194 - Train Loss: 0.001176110003143549 - Test Loss: 0.015787025913596153\n",
            "Epoch: 2195 - Train Loss: 0.0011716285953298211 - Test Loss: 0.01578233391046524\n",
            "Epoch: 2196 - Train Loss: 0.0011671631364151835 - Test Loss: 0.01577765680849552\n",
            "Epoch: 2197 - Train Loss: 0.0011627136263996363 - Test Loss: 0.015772994607686996\n",
            "Epoch: 2198 - Train Loss: 0.0011582797160372138 - Test Loss: 0.015768352895975113\n",
            "Epoch: 2199 - Train Loss: 0.0011538631515577435 - Test Loss: 0.01576372981071472\n",
            "Epoch: 2200 - Train Loss: 0.0011494607897475362 - Test Loss: 0.015759123489260674\n",
            "Epoch: 2201 - Train Loss: 0.001145074376836419 - Test Loss: 0.01575453393161297\n",
            "Epoch: 2202 - Train Loss: 0.0011407046113163233 - Test Loss: 0.015749961137771606\n",
            "Epoch: 2203 - Train Loss: 0.0011363503290340304 - Test Loss: 0.015745408833026886\n",
            "Epoch: 2204 - Train Loss: 0.001132011879235506 - Test Loss: 0.01574087329208851\n",
            "Epoch: 2205 - Train Loss: 0.0011276885634288192 - Test Loss: 0.015736350789666176\n",
            "Epoch: 2206 - Train Loss: 0.0011233807308599353 - Test Loss: 0.015731846913695335\n",
            "Epoch: 2207 - Train Loss: 0.0011190882651135325 - Test Loss: 0.015727363526821136\n",
            "Epoch: 2208 - Train Loss: 0.0011148122139275074 - Test Loss: 0.01572289504110813\n",
            "Epoch: 2209 - Train Loss: 0.0011105515295639634 - Test Loss: 0.015718447044491768\n",
            "Epoch: 2210 - Train Loss: 0.0011063046986237168 - Test Loss: 0.01571400836110115\n",
            "Epoch: 2211 - Train Loss: 0.0011020743986591697 - Test Loss: 0.015709595754742622\n",
            "Epoch: 2212 - Train Loss: 0.0010978591162711382 - Test Loss: 0.01570519432425499\n",
            "Epoch: 2213 - Train Loss: 0.0010936586186289787 - Test Loss: 0.01570081152021885\n",
            "Epoch: 2214 - Train Loss: 0.0010894732549786568 - Test Loss: 0.015696443617343903\n",
            "Epoch: 2215 - Train Loss: 0.001085302559658885 - Test Loss: 0.01569209061563015\n",
            "Epoch: 2216 - Train Loss: 0.0010811483953148127 - Test Loss: 0.01568775810301304\n",
            "Epoch: 2217 - Train Loss: 0.0010770076187327504 - Test Loss: 0.01568344235420227\n",
            "Epoch: 2218 - Train Loss: 0.001072881743311882 - Test Loss: 0.015679141506552696\n",
            "Epoch: 2219 - Train Loss: 0.0010687715839594603 - Test Loss: 0.015674853697419167\n",
            "Epoch: 2220 - Train Loss: 0.0010646763257682323 - Test Loss: 0.015670588240027428\n",
            "Epoch: 2221 - Train Loss: 0.001060595503076911 - Test Loss: 0.015666335821151733\n",
            "Epoch: 2222 - Train Loss: 0.0010565294651314616 - Test Loss: 0.01566210202872753\n",
            "Epoch: 2223 - Train Loss: 0.0010524779791012406 - Test Loss: 0.015657881274819374\n",
            "Epoch: 2224 - Train Loss: 0.001048441044986248 - Test Loss: 0.01565368101000786\n",
            "Epoch: 2225 - Train Loss: 0.0010444193612784147 - Test Loss: 0.015649497509002686\n",
            "Epoch: 2226 - Train Loss: 0.0010404124623164535 - Test Loss: 0.015645327046513557\n",
            "Epoch: 2227 - Train Loss: 0.0010364186018705368 - Test Loss: 0.015641171485185623\n",
            "Epoch: 2228 - Train Loss: 0.001032440341077745 - Test Loss: 0.01563703641295433\n",
            "Epoch: 2229 - Train Loss: 0.0010284761665388942 - Test Loss: 0.015632914379239082\n",
            "Epoch: 2230 - Train Loss: 0.0010245265439152718 - Test Loss: 0.015628809109330177\n",
            "Epoch: 2231 - Train Loss: 0.0010205908911302686 - Test Loss: 0.015624716877937317\n",
            "Epoch: 2232 - Train Loss: 0.0010166693245992064 - Test Loss: 0.015620642341673374\n",
            "Epoch: 2233 - Train Loss: 0.0010127628920599818 - Test Loss: 0.0156165836378932\n",
            "Epoch: 2234 - Train Loss: 0.0010088696144521236 - Test Loss: 0.015612540766596794\n",
            "Epoch: 2235 - Train Loss: 0.0010049901902675629 - Test Loss: 0.015608512796461582\n",
            "Epoch: 2236 - Train Loss: 0.0010011257836595178 - Test Loss: 0.015604499727487564\n",
            "Epoch: 2237 - Train Loss: 0.0009972754633054137 - Test Loss: 0.015600506216287613\n",
            "Epoch: 2238 - Train Loss: 0.0009934387635439634 - Test Loss: 0.015596526674926281\n",
            "Epoch: 2239 - Train Loss: 0.0009896159172058105 - Test Loss: 0.015592561103403568\n",
            "Epoch: 2240 - Train Loss: 0.0009858066914603114 - Test Loss: 0.0155886085703969\n",
            "Epoch: 2241 - Train Loss: 0.0009820112027227879 - Test Loss: 0.015584672801196575\n",
            "Epoch: 2242 - Train Loss: 0.0009782303823158145 - Test Loss: 0.015580754727125168\n",
            "Epoch: 2243 - Train Loss: 0.0009744621347635984 - Test Loss: 0.01557685062289238\n",
            "Epoch: 2244 - Train Loss: 0.0009707074495963752 - Test Loss: 0.015572960488498211\n",
            "Epoch: 2245 - Train Loss: 0.0009669674327597022 - Test Loss: 0.015569085255265236\n",
            "Epoch: 2246 - Train Loss: 0.0009632411529310048 - Test Loss: 0.015565229579806328\n",
            "Epoch: 2247 - Train Loss: 0.0009595282026566565 - Test Loss: 0.01556138601154089\n",
            "Epoch: 2248 - Train Loss: 0.0009558286401443183 - Test Loss: 0.01555755827575922\n",
            "Epoch: 2249 - Train Loss: 0.0009521427564322948 - Test Loss: 0.015553743578493595\n",
            "Epoch: 2250 - Train Loss: 0.0009484699694439769 - Test Loss: 0.015549943782389164\n",
            "Epoch: 2251 - Train Loss: 0.0009448106284253299 - Test Loss: 0.015546157956123352\n",
            "Epoch: 2252 - Train Loss: 0.0009411654900759459 - Test Loss: 0.015542389824986458\n",
            "Epoch: 2253 - Train Loss: 0.0009375336230732501 - Test Loss: 0.015538637526333332\n",
            "Epoch: 2254 - Train Loss: 0.0009339136304333806 - Test Loss: 0.015534895472228527\n",
            "Epoch: 2255 - Train Loss: 0.0009303081315010786 - Test Loss: 0.01553117111325264\n",
            "Epoch: 2256 - Train Loss: 0.0009267153800465167 - Test Loss: 0.01552746444940567\n",
            "Epoch: 2257 - Train Loss: 0.0009231356089003384 - Test Loss: 0.015523766167461872\n",
            "Epoch: 2258 - Train Loss: 0.000919568701647222 - Test Loss: 0.015520084649324417\n",
            "Epoch: 2259 - Train Loss: 0.0009160148329101503 - Test Loss: 0.015516416169703007\n",
            "Epoch: 2260 - Train Loss: 0.0009124738862738013 - Test Loss: 0.015512765385210514\n",
            "Epoch: 2261 - Train Loss: 0.0009089467930607498 - Test Loss: 0.015509127639234066\n",
            "Epoch: 2262 - Train Loss: 0.0009054329129867256 - Test Loss: 0.015505504794418812\n",
            "Epoch: 2263 - Train Loss: 0.0009019309072755277 - Test Loss: 0.015501894988119602\n",
            "Epoch: 2264 - Train Loss: 0.0008984431042335927 - Test Loss: 0.01549830287694931\n",
            "Epoch: 2265 - Train Loss: 0.0008949681068770587 - Test Loss: 0.015494721941649914\n",
            "Epoch: 2266 - Train Loss: 0.0008915059152059257 - Test Loss: 0.015491156838834286\n",
            "Epoch: 2267 - Train Loss: 0.0008880566456355155 - Test Loss: 0.015487604774534702\n",
            "Epoch: 2268 - Train Loss: 0.0008846213459037244 - Test Loss: 0.015484072268009186\n",
            "Epoch: 2269 - Train Loss: 0.0008811972802504897 - Test Loss: 0.01548055000603199\n",
            "Epoch: 2270 - Train Loss: 0.0008777859038673341 - Test Loss: 0.015477042645215988\n",
            "Epoch: 2271 - Train Loss: 0.0008743878570385277 - Test Loss: 0.015473547391593456\n",
            "Epoch: 2272 - Train Loss: 0.0008710024412721395 - Test Loss: 0.01547006331384182\n",
            "Epoch: 2273 - Train Loss: 0.0008676282595843077 - Test Loss: 0.01546659879386425\n",
            "Epoch: 2274 - Train Loss: 0.0008642683387733996 - Test Loss: 0.0154631482437253\n",
            "Epoch: 2275 - Train Loss: 0.0008609188953414559 - Test Loss: 0.01545970793813467\n",
            "Epoch: 2276 - Train Loss: 0.0008575833635404706 - Test Loss: 0.01545628160238266\n",
            "Epoch: 2277 - Train Loss: 0.000854258076287806 - Test Loss: 0.015452866442501545\n",
            "Epoch: 2278 - Train Loss: 0.0008509469917044044 - Test Loss: 0.015449469909071922\n",
            "Epoch: 2279 - Train Loss: 0.0008476480725221336 - Test Loss: 0.015446086414158344\n",
            "Epoch: 2280 - Train Loss: 0.0008443600963801146 - Test Loss: 0.015442713163793087\n",
            "Epoch: 2281 - Train Loss: 0.000841083936393261 - Test Loss: 0.015439353883266449\n",
            "Epoch: 2282 - Train Loss: 0.0008378208731301129 - Test Loss: 0.015436011366546154\n",
            "Epoch: 2283 - Train Loss: 0.0008345695096068084 - Test Loss: 0.015432680957019329\n",
            "Epoch: 2284 - Train Loss: 0.000831330253276974 - Test Loss: 0.015429362654685974\n",
            "Epoch: 2285 - Train Loss: 0.0008281024056486785 - Test Loss: 0.015426057390868664\n",
            "Epoch: 2286 - Train Loss: 0.0008248864323832095 - Test Loss: 0.015422762371599674\n",
            "Epoch: 2287 - Train Loss: 0.0008216832065954804 - Test Loss: 0.015419485978782177\n",
            "Epoch: 2288 - Train Loss: 0.0008184915059246123 - Test Loss: 0.01541622169315815\n",
            "Epoch: 2289 - Train Loss: 0.0008153105736710131 - Test Loss: 0.015412968583405018\n",
            "Epoch: 2290 - Train Loss: 0.0008121414575725794 - Test Loss: 0.015409727580845356\n",
            "Epoch: 2291 - Train Loss: 0.0008089850307442248 - Test Loss: 0.015406499616801739\n",
            "Epoch: 2292 - Train Loss: 0.0008058400126174092 - Test Loss: 0.01540328748524189\n",
            "Epoch: 2293 - Train Loss: 0.0008027065778151155 - Test Loss: 0.015400088392198086\n",
            "Epoch: 2294 - Train Loss: 0.0007995847263373435 - Test Loss: 0.015396898612380028\n",
            "Epoch: 2295 - Train Loss: 0.0007964741671457887 - Test Loss: 0.015393724665045738\n",
            "Epoch: 2296 - Train Loss: 0.000793374958448112 - Test Loss: 0.01539055909961462\n",
            "Epoch: 2297 - Train Loss: 0.0007902883226051927 - Test Loss: 0.015387413091957569\n",
            "Epoch: 2298 - Train Loss: 0.0007872117566876113 - Test Loss: 0.015384276397526264\n",
            "Epoch: 2299 - Train Loss: 0.0007841463666409254 - Test Loss: 0.01538114808499813\n",
            "Epoch: 2300 - Train Loss: 0.0007810929673723876 - Test Loss: 0.015378037467598915\n",
            "Epoch: 2301 - Train Loss: 0.0007780509768053889 - Test Loss: 0.01537493709474802\n",
            "Epoch: 2302 - Train Loss: 0.0007750200456939638 - Test Loss: 0.01537184976041317\n",
            "Epoch: 2303 - Train Loss: 0.0007720005232840776 - Test Loss: 0.015368775464594364\n",
            "Epoch: 2304 - Train Loss: 0.0007689919439144433 - Test Loss: 0.015365712344646454\n",
            "Epoch: 2305 - Train Loss: 0.0007659943657927215 - Test Loss: 0.015362662263214588\n",
            "Epoch: 2306 - Train Loss: 0.0007630079635418952 - Test Loss: 0.015359622426331043\n",
            "Epoch: 2307 - Train Loss: 0.0007600325043313205 - Test Loss: 0.015356597490608692\n",
            "Epoch: 2308 - Train Loss: 0.0007570679299533367 - Test Loss: 0.015353584662079811\n",
            "Epoch: 2309 - Train Loss: 0.0007541151135228574 - Test Loss: 0.0153505839407444\n",
            "Epoch: 2310 - Train Loss: 0.000751173123717308 - Test Loss: 0.01534759346395731\n",
            "Epoch: 2311 - Train Loss: 0.0007482410874217749 - Test Loss: 0.015344616025686264\n",
            "Epoch: 2312 - Train Loss: 0.0007453206344507635 - Test Loss: 0.015341649763286114\n",
            "Epoch: 2313 - Train Loss: 0.0007424108334816992 - Test Loss: 0.015338699333369732\n",
            "Epoch: 2314 - Train Loss: 0.0007395116263069212 - Test Loss: 0.015335758216679096\n",
            "Epoch: 2315 - Train Loss: 0.0007366230129264295 - Test Loss: 0.015332828275859356\n",
            "Epoch: 2316 - Train Loss: 0.0007337449351325631 - Test Loss: 0.015329907648265362\n",
            "Epoch: 2317 - Train Loss: 0.0007308773929253221 - Test Loss: 0.015327000990509987\n",
            "Epoch: 2318 - Train Loss: 0.000728021317627281 - Test Loss: 0.015324109233915806\n",
            "Epoch: 2319 - Train Loss: 0.0007251757779158652 - Test Loss: 0.01532122865319252\n",
            "Epoch: 2320 - Train Loss: 0.0007223395514301956 - Test Loss: 0.015318354591727257\n",
            "Epoch: 2321 - Train Loss: 0.0007195136277005076 - Test Loss: 0.015315493568778038\n",
            "Epoch: 2322 - Train Loss: 0.000716698938049376 - Test Loss: 0.015312645584344864\n",
            "Epoch: 2323 - Train Loss: 0.0007138947257772088 - Test Loss: 0.015309811569750309\n",
            "Epoch: 2324 - Train Loss: 0.0007111006416380405 - Test Loss: 0.0153069868683815\n",
            "Epoch: 2325 - Train Loss: 0.0007083166856318712 - Test Loss: 0.015304172411561012\n",
            "Epoch: 2326 - Train Loss: 0.0007055428577587008 - Test Loss: 0.015301368199288845\n",
            "Epoch: 2327 - Train Loss: 0.0007027792744338512 - Test Loss: 0.015298575162887573\n",
            "Epoch: 2328 - Train Loss: 0.0007000265177339315 - Test Loss: 0.015295795165002346\n",
            "Epoch: 2329 - Train Loss: 0.0006972840055823326 - Test Loss: 0.015293029136955738\n",
            "Epoch: 2330 - Train Loss: 0.0006945506320334971 - Test Loss: 0.015290268696844578\n",
            "Epoch: 2331 - Train Loss: 0.0006918271537870169 - Test Loss: 0.015287521295249462\n",
            "Epoch: 2332 - Train Loss: 0.0006891143857501447 - Test Loss: 0.015284785069525242\n",
            "Epoch: 2333 - Train Loss: 0.0006864116876386106 - Test Loss: 0.015282061882317066\n",
            "Epoch: 2334 - Train Loss: 0.0006837190594524145 - Test Loss: 0.015279347077012062\n",
            "Epoch: 2335 - Train Loss: 0.0006810360937379301 - Test Loss: 0.015276646241545677\n",
            "Epoch: 2336 - Train Loss: 0.00067836296511814 - Test Loss: 0.015273953787982464\n",
            "Epoch: 2337 - Train Loss: 0.0006756994989700615 - Test Loss: 0.015271272510290146\n",
            "Epoch: 2338 - Train Loss: 0.0006730457535013556 - Test Loss: 0.015268598683178425\n",
            "Epoch: 2339 - Train Loss: 0.0006704018451273441 - Test Loss: 0.015265941619873047\n",
            "Epoch: 2340 - Train Loss: 0.0006677685305476189 - Test Loss: 0.015263295732438564\n",
            "Epoch: 2341 - Train Loss: 0.0006651447038166225 - Test Loss: 0.015260658226907253\n",
            "Epoch: 2342 - Train Loss: 0.0006625296082347631 - Test Loss: 0.015258029103279114\n",
            "Epoch: 2343 - Train Loss: 0.0006599242333322763 - Test Loss: 0.015255410224199295\n",
            "Epoch: 2344 - Train Loss: 0.000657329277601093 - Test Loss: 0.01525280624628067\n",
            "Epoch: 2345 - Train Loss: 0.0006547438097186387 - Test Loss: 0.015250214375555515\n",
            "Epoch: 2346 - Train Loss: 0.0006521677132695913 - Test Loss: 0.015247629024088383\n",
            "Epoch: 2347 - Train Loss: 0.0006496009882539511 - Test Loss: 0.015245053917169571\n",
            "Epoch: 2348 - Train Loss: 0.0006470437510870397 - Test Loss: 0.015242490917444229\n",
            "Epoch: 2349 - Train Loss: 0.0006444960017688572 - Test Loss: 0.015239935368299484\n",
            "Epoch: 2350 - Train Loss: 0.0006419584387913346 - Test Loss: 0.015237394720315933\n",
            "Epoch: 2351 - Train Loss: 0.0006394300144165754 - Test Loss: 0.015234862454235554\n",
            "Epoch: 2352 - Train Loss: 0.0006369101465679705 - Test Loss: 0.015232340432703495\n",
            "Epoch: 2353 - Train Loss: 0.0006343995337374508 - Test Loss: 0.015229826793074608\n",
            "Epoch: 2354 - Train Loss: 0.0006318988744169474 - Test Loss: 0.015227324329316616\n",
            "Epoch: 2355 - Train Loss: 0.0006294073536992073 - Test Loss: 0.015224834904074669\n",
            "Epoch: 2356 - Train Loss: 0.0006269252044148743 - Test Loss: 0.015222354792058468\n",
            "Epoch: 2357 - Train Loss: 0.000624452019110322 - Test Loss: 0.015219885855913162\n",
            "Epoch: 2358 - Train Loss: 0.0006219879142008722 - Test Loss: 0.015217425301671028\n",
            "Epoch: 2359 - Train Loss: 0.0006195328896865249 - Test Loss: 0.015214972198009491\n",
            "Epoch: 2360 - Train Loss: 0.00061708694556728 - Test Loss: 0.015212531201541424\n",
            "Epoch: 2361 - Train Loss: 0.0006146499654278159 - Test Loss: 0.015210099518299103\n",
            "Epoch: 2362 - Train Loss: 0.0006122227059677243 - Test Loss: 0.015207681804895401\n",
            "Epoch: 2363 - Train Loss: 0.0006098043522797525 - Test Loss: 0.015205271542072296\n",
            "Epoch: 2364 - Train Loss: 0.0006073941476643085 - Test Loss: 0.015202868729829788\n",
            "Epoch: 2365 - Train Loss: 0.0006049929070286453 - Test Loss: 0.015200476162135601\n",
            "Epoch: 2366 - Train Loss: 0.0006026012124493718 - Test Loss: 0.01519809477031231\n",
            "Epoch: 2367 - Train Loss: 0.0006002182490192354 - Test Loss: 0.015195724554359913\n",
            "Epoch: 2368 - Train Loss: 0.0005978441331535578 - Test Loss: 0.015193362720310688\n",
            "Epoch: 2369 - Train Loss: 0.0005954787484370172 - Test Loss: 0.015191011130809784\n",
            "Epoch: 2370 - Train Loss: 0.0005931222112849355 - Test Loss: 0.015188668854534626\n",
            "Epoch: 2371 - Train Loss: 0.0005907741142436862 - Test Loss: 0.015186335891485214\n",
            "Epoch: 2372 - Train Loss: 0.0005884348065592349 - Test Loss: 0.015184009447693825\n",
            "Epoch: 2373 - Train Loss: 0.0005861044046469033 - Test Loss: 0.01518169604241848\n",
            "Epoch: 2374 - Train Loss: 0.0005837833741679788 - Test Loss: 0.015179394744336605\n",
            "Epoch: 2375 - Train Loss: 0.0005814707837998867 - Test Loss: 0.015177101828157902\n",
            "Epoch: 2376 - Train Loss: 0.000579166051466018 - Test Loss: 0.01517481543123722\n",
            "Epoch: 2377 - Train Loss: 0.0005768698174506426 - Test Loss: 0.015172538347542286\n",
            "Epoch: 2378 - Train Loss: 0.0005745835369452834 - Test Loss: 0.01517027523368597\n",
            "Epoch: 2379 - Train Loss: 0.0005723044159822166 - Test Loss: 0.015168020501732826\n",
            "Epoch: 2380 - Train Loss: 0.0005700351321138442 - Test Loss: 0.015165774151682854\n",
            "Epoch: 2381 - Train Loss: 0.0005677728331647813 - Test Loss: 0.015163535252213478\n",
            "Epoch: 2382 - Train Loss: 0.0005655206041410565 - Test Loss: 0.015161307528614998\n",
            "Epoch: 2383 - Train Loss: 0.000563275592867285 - Test Loss: 0.01515908446162939\n",
            "Epoch: 2384 - Train Loss: 0.0005610405933111906 - Test Loss: 0.015156880021095276\n",
            "Epoch: 2385 - Train Loss: 0.0005588140920735896 - Test Loss: 0.015154681168496609\n",
            "Epoch: 2386 - Train Loss: 0.0005565952160395682 - Test Loss: 0.015152490697801113\n",
            "Epoch: 2387 - Train Loss: 0.0005543847801163793 - Test Loss: 0.015150308609008789\n",
            "Epoch: 2388 - Train Loss: 0.0005521834245882928 - Test Loss: 0.015148136764764786\n",
            "Epoch: 2389 - Train Loss: 0.0005499901599250734 - Test Loss: 0.015145973302423954\n",
            "Epoch: 2390 - Train Loss: 0.0005478053353726864 - Test Loss: 0.015143821947276592\n",
            "Epoch: 2391 - Train Loss: 0.0005456287181004882 - Test Loss: 0.015141678974032402\n",
            "Epoch: 2392 - Train Loss: 0.0005434600752778351 - Test Loss: 0.015139542520046234\n",
            "Epoch: 2393 - Train Loss: 0.0005412998143583536 - Test Loss: 0.015137418173253536\n",
            "Epoch: 2394 - Train Loss: 0.0005391476443037391 - Test Loss: 0.015135297551751137\n",
            "Epoch: 2395 - Train Loss: 0.0005370035651139915 - Test Loss: 0.015133187174797058\n",
            "Epoch: 2396 - Train Loss: 0.0005348672857508063 - Test Loss: 0.015131089836359024\n",
            "Epoch: 2397 - Train Loss: 0.000532739155460149 - Test Loss: 0.015128999017179012\n",
            "Epoch: 2398 - Train Loss: 0.0005306196981109679 - Test Loss: 0.015126919373869896\n",
            "Epoch: 2399 - Train Loss: 0.00052850809879601 - Test Loss: 0.015124845318496227\n",
            "Epoch: 2400 - Train Loss: 0.0005264036008156836 - Test Loss: 0.01512277964502573\n",
            "Epoch: 2401 - Train Loss: 0.0005243070190772414 - Test Loss: 0.01512072142213583\n",
            "Epoch: 2402 - Train Loss: 0.0005222195759415627 - Test Loss: 0.015118677169084549\n",
            "Epoch: 2403 - Train Loss: 0.0005201385356485844 - Test Loss: 0.01511663943529129\n",
            "Epoch: 2404 - Train Loss: 0.0005180668085813522 - Test Loss: 0.015114610083401203\n",
            "Epoch: 2405 - Train Loss: 0.0005160011351108551 - Test Loss: 0.015112590044736862\n",
            "Epoch: 2406 - Train Loss: 0.0005139445420354605 - Test Loss: 0.015110575594007969\n",
            "Epoch: 2407 - Train Loss: 0.0005118942935951054 - Test Loss: 0.015108567662537098\n",
            "Epoch: 2408 - Train Loss: 0.0005098528927192092 - Test Loss: 0.01510657649487257\n",
            "Epoch: 2409 - Train Loss: 0.0005078192334622145 - Test Loss: 0.015104589983820915\n",
            "Epoch: 2410 - Train Loss: 0.0005057924427092075 - Test Loss: 0.015102609992027283\n",
            "Epoch: 2411 - Train Loss: 0.0005037731607444584 - Test Loss: 0.015100640244781971\n",
            "Epoch: 2412 - Train Loss: 0.0005017619696445763 - Test Loss: 0.015098676085472107\n",
            "Epoch: 2413 - Train Loss: 0.0004997584619559348 - Test Loss: 0.015096721239387989\n",
            "Epoch: 2414 - Train Loss: 0.0004977622302249074 - Test Loss: 0.015094776637852192\n",
            "Epoch: 2415 - Train Loss: 0.0004957736236974597 - Test Loss: 0.015092841349542141\n",
            "Epoch: 2416 - Train Loss: 0.0004937922931276262 - Test Loss: 0.015090912580490112\n",
            "Epoch: 2417 - Train Loss: 0.0004918181803077459 - Test Loss: 0.015088990330696106\n",
            "Epoch: 2418 - Train Loss: 0.0004898514598608017 - Test Loss: 0.015087077394127846\n",
            "Epoch: 2419 - Train Loss: 0.0004878921899944544 - Test Loss: 0.015085170976817608\n",
            "Epoch: 2420 - Train Loss: 0.0004859402251895517 - Test Loss: 0.015083272010087967\n",
            "Epoch: 2421 - Train Loss: 0.0004839955654460937 - Test Loss: 0.015081385150551796\n",
            "Epoch: 2422 - Train Loss: 0.00048205803614109755 - Test Loss: 0.015079505741596222\n",
            "Epoch: 2423 - Train Loss: 0.00048012856859713793 - Test Loss: 0.01507763285189867\n",
            "Epoch: 2424 - Train Loss: 0.0004782062896993011 - Test Loss: 0.01507576834410429\n",
            "Epoch: 2425 - Train Loss: 0.00047629058826714754 - Test Loss: 0.015073910355567932\n",
            "Epoch: 2426 - Train Loss: 0.0004743820463772863 - Test Loss: 0.015072060748934746\n",
            "Epoch: 2427 - Train Loss: 0.0004724821192212403 - Test Loss: 0.01507022138684988\n",
            "Epoch: 2428 - Train Loss: 0.0004705880128312856 - Test Loss: 0.015068390406668186\n",
            "Epoch: 2429 - Train Loss: 0.00046870228834450245 - Test Loss: 0.015066566877067089\n",
            "Epoch: 2430 - Train Loss: 0.000466822471935302 - Test Loss: 0.015064749866724014\n",
            "Epoch: 2431 - Train Loss: 0.00046495095011778176 - Test Loss: 0.015062939375638962\n",
            "Epoch: 2432 - Train Loss: 0.0004630851617548615 - Test Loss: 0.015061136335134506\n",
            "Epoch: 2433 - Train Loss: 0.0004612278426066041 - Test Loss: 0.01505934726446867\n",
            "Epoch: 2434 - Train Loss: 0.00045937745016999543 - Test Loss: 0.015057561919093132\n",
            "Epoch: 2435 - Train Loss: 0.00045753337326459587 - Test Loss: 0.01505578588694334\n",
            "Epoch: 2436 - Train Loss: 0.0004556963103823364 - Test Loss: 0.015054015442728996\n",
            "Epoch: 2437 - Train Loss: 0.0004538667853921652 - Test Loss: 0.0150522505864501\n",
            "Epoch: 2438 - Train Loss: 0.0004520442453213036 - Test Loss: 0.015050496906042099\n",
            "Epoch: 2439 - Train Loss: 0.0004502278461586684 - Test Loss: 0.015048747882246971\n",
            "Epoch: 2440 - Train Loss: 0.0004484191013034433 - Test Loss: 0.015047009103000164\n",
            "Epoch: 2441 - Train Loss: 0.00044661725405603647 - Test Loss: 0.015045279636979103\n",
            "Epoch: 2442 - Train Loss: 0.0004448221006896347 - Test Loss: 0.015043554827570915\n",
            "Epoch: 2443 - Train Loss: 0.0004430336703080684 - Test Loss: 0.015041837468743324\n",
            "Epoch: 2444 - Train Loss: 0.00044125207932665944 - Test Loss: 0.01504012756049633\n",
            "Epoch: 2445 - Train Loss: 0.0004394770658109337 - Test Loss: 0.015038423240184784\n",
            "Epoch: 2446 - Train Loss: 0.00043770953197963536 - Test Loss: 0.015036731958389282\n",
            "Epoch: 2447 - Train Loss: 0.00043594787712208927 - Test Loss: 0.015035046264529228\n",
            "Epoch: 2448 - Train Loss: 0.00043419297435320914 - Test Loss: 0.015033365227282047\n",
            "Epoch: 2449 - Train Loss: 0.0004324453475419432 - Test Loss: 0.015031692571938038\n",
            "Epoch: 2450 - Train Loss: 0.0004307043273001909 - Test Loss: 0.0150300282984972\n",
            "Epoch: 2451 - Train Loss: 0.00042896915692836046 - Test Loss: 0.01502836961299181\n",
            "Epoch: 2452 - Train Loss: 0.0004272404476068914 - Test Loss: 0.015026714652776718\n",
            "Epoch: 2453 - Train Loss: 0.00042551965452730656 - Test Loss: 0.015025075525045395\n",
            "Epoch: 2454 - Train Loss: 0.000423804041929543 - Test Loss: 0.015023438259959221\n",
            "Epoch: 2455 - Train Loss: 0.00042209605453535914 - Test Loss: 0.015021810308098793\n",
            "Epoch: 2456 - Train Loss: 0.00042039333493448794 - Test Loss: 0.015020187944173813\n",
            "Epoch: 2457 - Train Loss: 0.0004186982405371964 - Test Loss: 0.015018573962152004\n",
            "Epoch: 2458 - Train Loss: 0.00041700821020640433 - Test Loss: 0.015016963705420494\n",
            "Epoch: 2459 - Train Loss: 0.0004153258341830224 - Test Loss: 0.01501536089926958\n",
            "Epoch: 2460 - Train Loss: 0.00041364922071807086 - Test Loss: 0.015013767406344414\n",
            "Epoch: 2461 - Train Loss: 0.0004119789809919894 - Test Loss: 0.015012180432677269\n",
            "Epoch: 2462 - Train Loss: 0.0004103155224584043 - Test Loss: 0.015010600909590721\n",
            "Epoch: 2463 - Train Loss: 0.0004086584085598588 - Test Loss: 0.01500902883708477\n",
            "Epoch: 2464 - Train Loss: 0.0004070067952852696 - Test Loss: 0.015007460489869118\n",
            "Epoch: 2465 - Train Loss: 0.00040536152664572 - Test Loss: 0.015005899593234062\n",
            "Epoch: 2466 - Train Loss: 0.00040372295188717544 - Test Loss: 0.015004347078502178\n",
            "Epoch: 2467 - Train Loss: 0.00040209069265984 - Test Loss: 0.015002800151705742\n",
            "Epoch: 2468 - Train Loss: 0.00040046460344456136 - Test Loss: 0.015001262538135052\n",
            "Epoch: 2469 - Train Loss: 0.0003988446551375091 - Test Loss: 0.014999731443822384\n",
            "Epoch: 2470 - Train Loss: 0.00039723081863485277 - Test Loss: 0.014998205006122589\n",
            "Epoch: 2471 - Train Loss: 0.0003956230648327619 - Test Loss: 0.014996686019003391\n",
            "Epoch: 2472 - Train Loss: 0.00039402139373123646 - Test Loss: 0.014995172619819641\n",
            "Epoch: 2473 - Train Loss: 0.00039242583443410695 - Test Loss: 0.014993663877248764\n",
            "Epoch: 2474 - Train Loss: 0.00039083624142222106 - Test Loss: 0.014992167241871357\n",
            "Epoch: 2475 - Train Loss: 0.0003892532258760184 - Test Loss: 0.014990677125751972\n",
            "Epoch: 2476 - Train Loss: 0.00038767626392655075 - Test Loss: 0.014989192597568035\n",
            "Epoch: 2477 - Train Loss: 0.0003861045988742262 - Test Loss: 0.014987712725996971\n",
            "Epoch: 2478 - Train Loss: 0.0003845389874186367 - Test Loss: 0.01498623751103878\n",
            "Epoch: 2479 - Train Loss: 0.00038297977880574763 - Test Loss: 0.01498477254062891\n",
            "Epoch: 2480 - Train Loss: 0.0003814264782704413 - Test Loss: 0.014983310364186764\n",
            "Epoch: 2481 - Train Loss: 0.0003798784746322781 - Test Loss: 0.014981858432292938\n",
            "Epoch: 2482 - Train Loss: 0.00037833754322491586 - Test Loss: 0.014980414882302284\n",
            "Epoch: 2483 - Train Loss: 0.0003768012684304267 - Test Loss: 0.014978974126279354\n",
            "Epoch: 2484 - Train Loss: 0.00037527194945141673 - Test Loss: 0.014977540820837021\n",
            "Epoch: 2485 - Train Loss: 0.0003737472288776189 - Test Loss: 0.014976111240684986\n",
            "Epoch: 2486 - Train Loss: 0.0003722294932231307 - Test Loss: 0.014974690042436123\n",
            "Epoch: 2487 - Train Loss: 0.0003707162686623633 - Test Loss: 0.014973272569477558\n",
            "Epoch: 2488 - Train Loss: 0.00036920991260558367 - Test Loss: 0.01497186254709959\n",
            "Epoch: 2489 - Train Loss: 0.0003677088243421167 - Test Loss: 0.014970462769269943\n",
            "Epoch: 2490 - Train Loss: 0.000366213294910267 - Test Loss: 0.01496906578540802\n",
            "Epoch: 2491 - Train Loss: 0.00036472405190579593 - Test Loss: 0.014967676252126694\n",
            "Epoch: 2492 - Train Loss: 0.0003632405132520944 - Test Loss: 0.01496629323810339\n",
            "Epoch: 2493 - Train Loss: 0.0003617619804572314 - Test Loss: 0.014964913949370384\n",
            "Epoch: 2494 - Train Loss: 0.00036028906470164657 - Test Loss: 0.014963541179895401\n",
            "Epoch: 2495 - Train Loss: 0.000358822348061949 - Test Loss: 0.01496217306703329\n",
            "Epoch: 2496 - Train Loss: 0.00035736136487685144 - Test Loss: 0.01496081706136465\n",
            "Epoch: 2497 - Train Loss: 0.00035590602783486247 - Test Loss: 0.014959464780986309\n",
            "Epoch: 2498 - Train Loss: 0.00035445630783215165 - Test Loss: 0.014958119951188564\n",
            "Epoch: 2499 - Train Loss: 0.0003530121175572276 - Test Loss: 0.014956778846681118\n",
            "Epoch: 2500 - Train Loss: 0.0003515735443215817 - Test Loss: 0.014955444261431694\n",
            "Epoch: 2501 - Train Loss: 0.0003501405590213835 - Test Loss: 0.014954112470149994\n",
            "Epoch: 2502 - Train Loss: 0.00034871307434514165 - Test Loss: 0.01495278999209404\n",
            "Epoch: 2503 - Train Loss: 0.00034729097387753427 - Test Loss: 0.014951474964618683\n",
            "Epoch: 2504 - Train Loss: 0.00034587489790283144 - Test Loss: 0.014950167387723923\n",
            "Epoch: 2505 - Train Loss: 0.0003444644680712372 - Test Loss: 0.014948863536119461\n",
            "Epoch: 2506 - Train Loss: 0.0003430590149946511 - Test Loss: 0.014947566203773022\n",
            "Epoch: 2507 - Train Loss: 0.0003416591207496822 - Test Loss: 0.014946271665394306\n",
            "Epoch: 2508 - Train Loss: 0.000340265134582296 - Test Loss: 0.014944986440241337\n",
            "Epoch: 2509 - Train Loss: 0.00033887673635035753 - Test Loss: 0.014943704940378666\n",
            "Epoch: 2510 - Train Loss: 0.0003374934021849185 - Test Loss: 0.014942432753741741\n",
            "Epoch: 2511 - Train Loss: 0.00033611649996601045 - Test Loss: 0.014941166155040264\n",
            "Epoch: 2512 - Train Loss: 0.0003347439633216709 - Test Loss: 0.01493990421295166\n",
            "Epoch: 2513 - Train Loss: 0.0003333778295200318 - Test Loss: 0.014938647858798504\n",
            "Epoch: 2514 - Train Loss: 0.0003320159448776394 - Test Loss: 0.014937397092580795\n",
            "Epoch: 2515 - Train Loss: 0.0003306604630779475 - Test Loss: 0.01493615098297596\n",
            "Epoch: 2516 - Train Loss: 0.00032930908491835 - Test Loss: 0.014934910461306572\n",
            "Epoch: 2517 - Train Loss: 0.00032796410960145295 - Test Loss: 0.014933675527572632\n",
            "Epoch: 2518 - Train Loss: 0.0003266239073127508 - Test Loss: 0.014932449907064438\n",
            "Epoch: 2519 - Train Loss: 0.0003252887399867177 - Test Loss: 0.014931228943169117\n",
            "Epoch: 2520 - Train Loss: 0.000323959335219115 - Test Loss: 0.014930013567209244\n",
            "Epoch: 2521 - Train Loss: 0.0003226350818295032 - Test Loss: 0.014928803779184818\n",
            "Epoch: 2522 - Train Loss: 0.0003213154268451035 - Test Loss: 0.014927598647773266\n",
            "Epoch: 2523 - Train Loss: 0.0003200008941348642 - Test Loss: 0.014926397241652012\n",
            "Epoch: 2524 - Train Loss: 0.0003186919493600726 - Test Loss: 0.01492520235478878\n",
            "Epoch: 2525 - Train Loss: 0.0003173881850671023 - Test Loss: 0.01492401584982872\n",
            "Epoch: 2526 - Train Loss: 0.00031608936842530966 - Test Loss: 0.014922835864126682\n",
            "Epoch: 2527 - Train Loss: 0.0003147955285385251 - Test Loss: 0.014921657741069794\n",
            "Epoch: 2528 - Train Loss: 0.0003135066945105791 - Test Loss: 0.014920487068593502\n",
            "Epoch: 2529 - Train Loss: 0.0003122227790299803 - Test Loss: 0.014919321984052658\n",
            "Epoch: 2530 - Train Loss: 0.0003109439858235419 - Test Loss: 0.014918157830834389\n",
            "Epoch: 2531 - Train Loss: 0.0003096699365414679 - Test Loss: 0.014917002059519291\n",
            "Epoch: 2532 - Train Loss: 0.0003084006893914193 - Test Loss: 0.014915850944817066\n",
            "Epoch: 2533 - Train Loss: 0.0003071364772040397 - Test Loss: 0.014914706349372864\n",
            "Epoch: 2534 - Train Loss: 0.00030587753280997276 - Test Loss: 0.014913570135831833\n",
            "Epoch: 2535 - Train Loss: 0.000304623506963253 - Test Loss: 0.014912436716258526\n",
            "Epoch: 2536 - Train Loss: 0.00030337373027577996 - Test Loss: 0.014911305159330368\n",
            "Epoch: 2537 - Train Loss: 0.0003021288139279932 - Test Loss: 0.014910181984305382\n",
            "Epoch: 2538 - Train Loss: 0.0003008891362696886 - Test Loss: 0.014909062534570694\n",
            "Epoch: 2539 - Train Loss: 0.0002996542025357485 - Test Loss: 0.014907948672771454\n",
            "Epoch: 2540 - Train Loss: 0.00029842357616871595 - Test Loss: 0.014906838536262512\n",
            "Epoch: 2541 - Train Loss: 0.0002971976646222174 - Test Loss: 0.014905733987689018\n",
            "Epoch: 2542 - Train Loss: 0.00029597754473797977 - Test Loss: 0.014904635027050972\n",
            "Epoch: 2543 - Train Loss: 0.00029476112104021013 - Test Loss: 0.014903544448316097\n",
            "Epoch: 2544 - Train Loss: 0.0002935503434855491 - Test Loss: 0.014902455732226372\n",
            "Epoch: 2545 - Train Loss: 0.0002923431165982038 - Test Loss: 0.014901372604072094\n",
            "Epoch: 2546 - Train Loss: 0.00029114162316545844 - Test Loss: 0.014900295063853264\n",
            "Epoch: 2547 - Train Loss: 0.00028994379681535065 - Test Loss: 0.014899219386279583\n",
            "Epoch: 2548 - Train Loss: 0.00028875155840069056 - Test Loss: 0.014898153021931648\n",
            "Epoch: 2549 - Train Loss: 0.0002875628706533462 - Test Loss: 0.014897085726261139\n",
            "Epoch: 2550 - Train Loss: 0.00028637979994527996 - Test Loss: 0.014896029606461525\n",
            "Epoch: 2551 - Train Loss: 0.0002852013858500868 - Test Loss: 0.014894979074597359\n",
            "Epoch: 2552 - Train Loss: 0.00028402707539498806 - Test Loss: 0.014893931336700916\n",
            "Epoch: 2553 - Train Loss: 0.00028285730513744056 - Test Loss: 0.014892888255417347\n",
            "Epoch: 2554 - Train Loss: 0.0002816927735693753 - Test Loss: 0.0148918516933918\n",
            "Epoch: 2555 - Train Loss: 0.000280532956821844 - Test Loss: 0.014890817925333977\n",
            "Epoch: 2556 - Train Loss: 0.000279377301922068 - Test Loss: 0.014889787882566452\n",
            "Epoch: 2557 - Train Loss: 0.0002782261581160128 - Test Loss: 0.014888763427734375\n",
            "Epoch: 2558 - Train Loss: 0.00027708010748028755 - Test Loss: 0.014887748286128044\n",
            "Epoch: 2559 - Train Loss: 0.00027593859704211354 - Test Loss: 0.014886734075844288\n",
            "Epoch: 2560 - Train Loss: 0.00027480177232064307 - Test Loss: 0.014885728247463703\n",
            "Epoch: 2561 - Train Loss: 0.00027366940048523247 - Test Loss: 0.014884726144373417\n",
            "Epoch: 2562 - Train Loss: 0.0002725414524320513 - Test Loss: 0.014883727766573429\n",
            "Epoch: 2563 - Train Loss: 0.0002714181027840823 - Test Loss: 0.01488273311406374\n",
            "Epoch: 2564 - Train Loss: 0.00027029908960685134 - Test Loss: 0.014881744980812073\n",
            "Epoch: 2565 - Train Loss: 0.00026918502408079803 - Test Loss: 0.014880763366818428\n",
            "Epoch: 2566 - Train Loss: 0.00026807538233697414 - Test Loss: 0.014879787340760231\n",
            "Epoch: 2567 - Train Loss: 0.00026696958229877055 - Test Loss: 0.014878814108669758\n",
            "Epoch: 2568 - Train Loss: 0.000265868118731305 - Test Loss: 0.014877845533192158\n",
            "Epoch: 2569 - Train Loss: 0.00026477142819203436 - Test Loss: 0.01487688347697258\n",
            "Epoch: 2570 - Train Loss: 0.00026367910322733223 - Test Loss: 0.014875924214720726\n",
            "Epoch: 2571 - Train Loss: 0.00026259050355292857 - Test Loss: 0.014874966815114021\n",
            "Epoch: 2572 - Train Loss: 0.0002615061239339411 - Test Loss: 0.014874013140797615\n",
            "Epoch: 2573 - Train Loss: 0.0002604264300316572 - Test Loss: 0.014873066917061806\n",
            "Epoch: 2574 - Train Loss: 0.0002593510434962809 - Test Loss: 0.014872126281261444\n",
            "Epoch: 2575 - Train Loss: 0.000258279760600999 - Test Loss: 0.014871192164719105\n",
            "Epoch: 2576 - Train Loss: 0.0002572127559687942 - Test Loss: 0.014870261773467064\n",
            "Epoch: 2577 - Train Loss: 0.0002561498258728534 - Test Loss: 0.014869333244860172\n",
            "Epoch: 2578 - Train Loss: 0.00025509099941700697 - Test Loss: 0.014868409372866154\n",
            "Epoch: 2579 - Train Loss: 0.00025403627660125494 - Test Loss: 0.014867491088807583\n",
            "Epoch: 2580 - Train Loss: 0.0002529856574255973 - Test Loss: 0.014866575598716736\n",
            "Epoch: 2581 - Train Loss: 0.00025193902547471225 - Test Loss: 0.014865663833916187\n",
            "Epoch: 2582 - Train Loss: 0.00025089658447541296 - Test Loss: 0.014864755794405937\n",
            "Epoch: 2583 - Train Loss: 0.00024985859636217356 - Test Loss: 0.014863857999444008\n",
            "Epoch: 2584 - Train Loss: 0.0002488242171239108 - Test Loss: 0.014862961135804653\n",
            "Epoch: 2585 - Train Loss: 0.00024779376690275967 - Test Loss: 0.014862067997455597\n",
            "Epoch: 2586 - Train Loss: 0.0002467677986714989 - Test Loss: 0.014861179515719414\n",
            "Epoch: 2587 - Train Loss: 0.0002457458758726716 - Test Loss: 0.014860296621918678\n",
            "Epoch: 2588 - Train Loss: 0.00024472741642966866 - Test Loss: 0.014859415590763092\n",
            "Epoch: 2589 - Train Loss: 0.0002437129005556926 - Test Loss: 0.014858539216220379\n",
            "Epoch: 2590 - Train Loss: 0.00024270279391203076 - Test Loss: 0.014857664704322815\n",
            "Epoch: 2591 - Train Loss: 0.00024169670359697193 - Test Loss: 0.014856795780360699\n",
            "Epoch: 2592 - Train Loss: 0.00024069452774710953 - Test Loss: 0.014855935238301754\n",
            "Epoch: 2593 - Train Loss: 0.00023969625181052834 - Test Loss: 0.014855078421533108\n",
            "Epoch: 2594 - Train Loss: 0.0002387019048910588 - Test Loss: 0.01485422533005476\n",
            "Epoch: 2595 - Train Loss: 0.00023771135602146387 - Test Loss: 0.014853375032544136\n",
            "Epoch: 2596 - Train Loss: 0.00023672476527281106 - Test Loss: 0.014852529391646385\n",
            "Epoch: 2597 - Train Loss: 0.0002357420016778633 - Test Loss: 0.014851686544716358\n",
            "Epoch: 2598 - Train Loss: 0.00023476302158087492 - Test Loss: 0.014850848354399204\n",
            "Epoch: 2599 - Train Loss: 0.00023378791229333729 - Test Loss: 0.014850011095404625\n",
            "Epoch: 2600 - Train Loss: 0.00023281660105567425 - Test Loss: 0.014849180355668068\n",
            "Epoch: 2601 - Train Loss: 0.0002318490733159706 - Test Loss: 0.01484835147857666\n",
            "Epoch: 2602 - Train Loss: 0.00023088534362614155 - Test Loss: 0.014847530983388424\n",
            "Epoch: 2603 - Train Loss: 0.00022992538288235664 - Test Loss: 0.014846714213490486\n",
            "Epoch: 2604 - Train Loss: 0.00022896952577866614 - Test Loss: 0.014845903031527996\n",
            "Epoch: 2605 - Train Loss: 0.0002280173939652741 - Test Loss: 0.014845091849565506\n",
            "Epoch: 2606 - Train Loss: 0.0002270685654366389 - Test Loss: 0.014844285324215889\n",
            "Epoch: 2607 - Train Loss: 0.00022612346219830215 - Test Loss: 0.014843481592833996\n",
            "Epoch: 2608 - Train Loss: 0.00022518241894431412 - Test Loss: 0.014842684380710125\n",
            "Epoch: 2609 - Train Loss: 0.00022424498456530273 - Test Loss: 0.014841888099908829\n",
            "Epoch: 2610 - Train Loss: 0.00022331094078253955 - Test Loss: 0.014841095544397831\n",
            "Epoch: 2611 - Train Loss: 0.0002223808114649728 - Test Loss: 0.014840311370790005\n",
            "Epoch: 2612 - Train Loss: 0.00022145439288578928 - Test Loss: 0.014839530922472477\n",
            "Epoch: 2613 - Train Loss: 0.00022053155407775193 - Test Loss: 0.014838753268122673\n",
            "Epoch: 2614 - Train Loss: 0.0002196124114561826 - Test Loss: 0.014837978407740593\n",
            "Epoch: 2615 - Train Loss: 0.00021869690681342036 - Test Loss: 0.014837206341326237\n",
            "Epoch: 2616 - Train Loss: 0.00021778482187073678 - Test Loss: 0.014836439862847328\n",
            "Epoch: 2617 - Train Loss: 0.0002168763312511146 - Test Loss: 0.014835675247013569\n",
            "Epoch: 2618 - Train Loss: 0.0002159713621949777 - Test Loss: 0.014834916219115257\n",
            "Epoch: 2619 - Train Loss: 0.0002150699874619022 - Test Loss: 0.01483415812253952\n",
            "Epoch: 2620 - Train Loss: 0.00021417198877315968 - Test Loss: 0.014833403751254082\n",
            "Epoch: 2621 - Train Loss: 0.00021327803551685065 - Test Loss: 0.01483265683054924\n",
            "Epoch: 2622 - Train Loss: 0.0002123871527146548 - Test Loss: 0.014831915497779846\n",
            "Epoch: 2623 - Train Loss: 0.0002114998351316899 - Test Loss: 0.014831174165010452\n",
            "Epoch: 2624 - Train Loss: 0.00021061633015051484 - Test Loss: 0.014830438420176506\n",
            "Epoch: 2625 - Train Loss: 0.0002097364340443164 - Test Loss: 0.014829704537987709\n",
            "Epoch: 2626 - Train Loss: 0.00020885959384031594 - Test Loss: 0.014828975312411785\n",
            "Epoch: 2627 - Train Loss: 0.00020798624609597027 - Test Loss: 0.014828247018158436\n",
            "Epoch: 2628 - Train Loss: 0.00020711668184958398 - Test Loss: 0.014827524311840534\n",
            "Epoch: 2629 - Train Loss: 0.00020625069737434387 - Test Loss: 0.014826803468167782\n",
            "Epoch: 2630 - Train Loss: 0.00020538810349535197 - Test Loss: 0.014826091006398201\n",
            "Epoch: 2631 - Train Loss: 0.00020452892931643873 - Test Loss: 0.014825383201241493\n",
            "Epoch: 2632 - Train Loss: 0.0002036731457337737 - Test Loss: 0.01482467446476221\n",
            "Epoch: 2633 - Train Loss: 0.0002028207090916112 - Test Loss: 0.014823971316218376\n",
            "Epoch: 2634 - Train Loss: 0.000201971604838036 - Test Loss: 0.01482327375560999\n",
            "Epoch: 2635 - Train Loss: 0.0002011259348364547 - Test Loss: 0.014822574332356453\n",
            "Epoch: 2636 - Train Loss: 0.00020028348080813885 - Test Loss: 0.014821881428360939\n",
            "Epoch: 2637 - Train Loss: 0.0001994444610318169 - Test Loss: 0.014821189455688\n",
            "Epoch: 2638 - Train Loss: 0.00019860867178067565 - Test Loss: 0.014820503070950508\n",
            "Epoch: 2639 - Train Loss: 0.00019777609850279987 - Test Loss: 0.014819818548858166\n",
            "Epoch: 2640 - Train Loss: 0.00019694690126925707 - Test Loss: 0.01481914147734642\n",
            "Epoch: 2641 - Train Loss: 0.00019612093456089497 - Test Loss: 0.014818466268479824\n",
            "Epoch: 2642 - Train Loss: 0.00019529864948708564 - Test Loss: 0.014817794784903526\n",
            "Epoch: 2643 - Train Loss: 0.00019447958038654178 - Test Loss: 0.014817128889262676\n",
            "Epoch: 2644 - Train Loss: 0.00019366329070180655 - Test Loss: 0.014816462993621826\n",
            "Epoch: 2645 - Train Loss: 0.00019285024609416723 - Test Loss: 0.0148157998919487\n",
            "Epoch: 2646 - Train Loss: 0.0001920407376019284 - Test Loss: 0.014815140515565872\n",
            "Epoch: 2647 - Train Loss: 0.00019123448873870075 - Test Loss: 0.014814483933150768\n",
            "Epoch: 2648 - Train Loss: 0.00019043088832404464 - Test Loss: 0.014813830144703388\n",
            "Epoch: 2649 - Train Loss: 0.00018963048933073878 - Test Loss: 0.014813181012868881\n",
            "Epoch: 2650 - Train Loss: 0.0001888337719719857 - Test Loss: 0.014812532812356949\n",
            "Epoch: 2651 - Train Loss: 0.00018804016872309148 - Test Loss: 0.014811892993748188\n",
            "Epoch: 2652 - Train Loss: 0.00018724963592831045 - Test Loss: 0.014811254106462002\n",
            "Epoch: 2653 - Train Loss: 0.0001864621153799817 - Test Loss: 0.014810621738433838\n",
            "Epoch: 2654 - Train Loss: 0.00018567778170108795 - Test Loss: 0.014809989370405674\n",
            "Epoch: 2655 - Train Loss: 0.00018489648937247694 - Test Loss: 0.014809361658990383\n",
            "Epoch: 2656 - Train Loss: 0.00018411832570564002 - Test Loss: 0.014808733016252518\n",
            "Epoch: 2657 - Train Loss: 0.00018334317428525537 - Test Loss: 0.0148081099614501\n",
            "Epoch: 2658 - Train Loss: 0.00018257112242281437 - Test Loss: 0.014807489700615406\n",
            "Epoch: 2659 - Train Loss: 0.00018180205370299518 - Test Loss: 0.014806872233748436\n",
            "Epoch: 2660 - Train Loss: 0.00018103599722962826 - Test Loss: 0.01480625756084919\n",
            "Epoch: 2661 - Train Loss: 0.00018027298210654408 - Test Loss: 0.014805649407207966\n",
            "Epoch: 2662 - Train Loss: 0.00017951343033928424 - Test Loss: 0.014805044047534466\n",
            "Epoch: 2663 - Train Loss: 0.0001787568035069853 - Test Loss: 0.014804445207118988\n",
            "Epoch: 2664 - Train Loss: 0.00017800279601942748 - Test Loss: 0.014803842641413212\n",
            "Epoch: 2665 - Train Loss: 0.00017725177167449147 - Test Loss: 0.014803246594965458\n",
            "Epoch: 2666 - Train Loss: 0.00017650407971814275 - Test Loss: 0.014802653342485428\n",
            "Epoch: 2667 - Train Loss: 0.00017575932724867016 - Test Loss: 0.014802061952650547\n",
            "Epoch: 2668 - Train Loss: 0.000175017150468193 - Test Loss: 0.01480147335678339\n",
            "Epoch: 2669 - Train Loss: 0.00017427785496693105 - Test Loss: 0.014800887554883957\n",
            "Epoch: 2670 - Train Loss: 0.00017354181909468025 - Test Loss: 0.014800303615629673\n",
            "Epoch: 2671 - Train Loss: 0.0001728087809169665 - Test Loss: 0.014799724332988262\n",
            "Epoch: 2672 - Train Loss: 0.00017207856581080705 - Test Loss: 0.014799150638282299\n",
            "Epoch: 2673 - Train Loss: 0.00017135124653577805 - Test Loss: 0.014798578806221485\n",
            "Epoch: 2674 - Train Loss: 0.0001706267794361338 - Test Loss: 0.01479801069945097\n",
            "Epoch: 2675 - Train Loss: 0.00016990517906378955 - Test Loss: 0.014797445386648178\n",
            "Epoch: 2676 - Train Loss: 0.0001691864599706605 - Test Loss: 0.014796881936490536\n",
            "Epoch: 2677 - Train Loss: 0.00016847059305291623 - Test Loss: 0.014796322211623192\n",
            "Epoch: 2678 - Train Loss: 0.0001677574764471501 - Test Loss: 0.014795764349400997\n",
            "Epoch: 2679 - Train Loss: 0.00016704727022442967 - Test Loss: 0.014795208349823952\n",
            "Epoch: 2680 - Train Loss: 0.00016633991617709398 - Test Loss: 0.01479465514421463\n",
            "Epoch: 2681 - Train Loss: 0.00016563525423407555 - Test Loss: 0.014794103801250458\n",
            "Epoch: 2682 - Train Loss: 0.0001649334153626114 - Test Loss: 0.014793556183576584\n",
            "Epoch: 2683 - Train Loss: 0.00016423439956270158 - Test Loss: 0.014793016016483307\n",
            "Epoch: 2684 - Train Loss: 0.000163538075867109 - Test Loss: 0.014792478643357754\n",
            "Epoch: 2685 - Train Loss: 0.00016284495359286666 - Test Loss: 0.014791942201554775\n",
            "Epoch: 2686 - Train Loss: 0.00016215452342294157 - Test Loss: 0.014791407622396946\n",
            "Epoch: 2687 - Train Loss: 0.00016146639245562255 - Test Loss: 0.01479087769985199\n",
            "Epoch: 2688 - Train Loss: 0.00016078112821560353 - Test Loss: 0.014790348708629608\n",
            "Epoch: 2689 - Train Loss: 0.000160098890773952 - Test Loss: 0.014789823442697525\n",
            "Epoch: 2690 - Train Loss: 0.00015941938909236342 - Test Loss: 0.014789300039410591\n",
            "Epoch: 2691 - Train Loss: 0.00015874215750955045 - Test Loss: 0.014788779430091381\n",
            "Epoch: 2692 - Train Loss: 0.00015806771989446133 - Test Loss: 0.01478826068341732\n",
            "Epoch: 2693 - Train Loss: 0.00015739622176624835 - Test Loss: 0.014787742868065834\n",
            "Epoch: 2694 - Train Loss: 0.00015672743029426783 - Test Loss: 0.014787229709327221\n",
            "Epoch: 2695 - Train Loss: 0.00015606101078446954 - Test Loss: 0.01478672120720148\n",
            "Epoch: 2696 - Train Loss: 0.00015539790911134332 - Test Loss: 0.014786217361688614\n",
            "Epoch: 2697 - Train Loss: 0.00015473674284294248 - Test Loss: 0.014785715378820896\n",
            "Epoch: 2698 - Train Loss: 0.00015407893806695938 - Test Loss: 0.014785216189920902\n",
            "Epoch: 2699 - Train Loss: 0.0001534229813842103 - Test Loss: 0.014784716069698334\n",
            "Epoch: 2700 - Train Loss: 0.00015277040074579418 - Test Loss: 0.014784223400056362\n",
            "Epoch: 2701 - Train Loss: 0.00015211982827167958 - Test Loss: 0.014783729799091816\n",
            "Epoch: 2702 - Train Loss: 0.00015147248632274568 - Test Loss: 0.014783239923417568\n",
            "Epoch: 2703 - Train Loss: 0.00015082709433045238 - Test Loss: 0.014782750979065895\n",
            "Epoch: 2704 - Train Loss: 0.0001501849910710007 - Test Loss: 0.014782264828681946\n",
            "Epoch: 2705 - Train Loss: 0.00014954482321627438 - Test Loss: 0.014781780540943146\n",
            "Epoch: 2706 - Train Loss: 0.00014890792954247445 - Test Loss: 0.014781305566430092\n",
            "Epoch: 2707 - Train Loss: 0.0001482735387980938 - Test Loss: 0.014780830591917038\n",
            "Epoch: 2708 - Train Loss: 0.00014764146180823445 - Test Loss: 0.014780357480049133\n",
            "Epoch: 2709 - Train Loss: 0.0001470118440920487 - Test Loss: 0.014779886230826378\n",
            "Epoch: 2710 - Train Loss: 0.00014638509310316294 - Test Loss: 0.01477942056953907\n",
            "Epoch: 2711 - Train Loss: 0.00014576088869944215 - Test Loss: 0.014778953976929188\n",
            "Epoch: 2712 - Train Loss: 0.00014513877977151424 - Test Loss: 0.014778491109609604\n",
            "Epoch: 2713 - Train Loss: 0.00014451926108449697 - Test Loss: 0.01477802824229002\n",
            "Epoch: 2714 - Train Loss: 0.00014390243450179696 - Test Loss: 0.014777570962905884\n",
            "Epoch: 2715 - Train Loss: 0.0001432881545042619 - Test Loss: 0.014777114614844322\n",
            "Epoch: 2716 - Train Loss: 0.00014267598453443497 - Test Loss: 0.01477666012942791\n",
            "Epoch: 2717 - Train Loss: 0.00014206663763616234 - Test Loss: 0.01477621216326952\n",
            "Epoch: 2718 - Train Loss: 0.00014145970635581762 - Test Loss: 0.014775766059756279\n",
            "Epoch: 2719 - Train Loss: 0.00014085524890106171 - Test Loss: 0.014775322750210762\n",
            "Epoch: 2720 - Train Loss: 0.00014025323616806418 - Test Loss: 0.014774881303310394\n",
            "Epoch: 2721 - Train Loss: 0.00013965368270874023 - Test Loss: 0.014774441719055176\n",
            "Epoch: 2722 - Train Loss: 0.00013905655941925943 - Test Loss: 0.014774003066122532\n",
            "Epoch: 2723 - Train Loss: 0.0001384618371957913 - Test Loss: 0.014773569069802761\n",
            "Epoch: 2724 - Train Loss: 0.0001378695305902511 - Test Loss: 0.014773136004805565\n",
            "Epoch: 2725 - Train Loss: 0.000137279654154554 - Test Loss: 0.014772704802453518\n",
            "Epoch: 2726 - Train Loss: 0.00013669213512912393 - Test Loss: 0.01477227546274662\n",
            "Epoch: 2727 - Train Loss: 0.0001361069589620456 - Test Loss: 0.014771849848330021\n",
            "Epoch: 2728 - Train Loss: 0.00013552421296481043 - Test Loss: 0.014771425165235996\n",
            "Epoch: 2729 - Train Loss: 0.00013494407176040113 - Test Loss: 0.014771007001399994\n",
            "Epoch: 2730 - Train Loss: 0.00013436593872029334 - Test Loss: 0.014770589768886566\n",
            "Epoch: 2731 - Train Loss: 0.00013379020674619824 - Test Loss: 0.014770175330340862\n",
            "Epoch: 2732 - Train Loss: 0.00013321705046109855 - Test Loss: 0.014769763685762882\n",
            "Epoch: 2733 - Train Loss: 0.00013264620793052018 - Test Loss: 0.014769354835152626\n",
            "Epoch: 2734 - Train Loss: 0.00013207734446041286 - Test Loss: 0.01476894598454237\n",
            "Epoch: 2735 - Train Loss: 0.00013151078019291162 - Test Loss: 0.014768540859222412\n",
            "Epoch: 2736 - Train Loss: 0.00013094680616632104 - Test Loss: 0.01476813480257988\n",
            "Epoch: 2737 - Train Loss: 0.00013038502947892994 - Test Loss: 0.014767734333872795\n",
            "Epoch: 2738 - Train Loss: 0.00012982524640392512 - Test Loss: 0.014767332933843136\n",
            "Epoch: 2739 - Train Loss: 0.00012926773342769593 - Test Loss: 0.014766937121748924\n",
            "Epoch: 2740 - Train Loss: 0.0001287127670366317 - Test Loss: 0.014766540378332138\n",
            "Epoch: 2741 - Train Loss: 0.00012816005619242787 - Test Loss: 0.014766150154173374\n",
            "Epoch: 2742 - Train Loss: 0.00012760967365466058 - Test Loss: 0.014765762723982334\n",
            "Epoch: 2743 - Train Loss: 0.000127061503008008 - Test Loss: 0.014765375293791294\n",
            "Epoch: 2744 - Train Loss: 0.00012651560246013105 - Test Loss: 0.014764990657567978\n",
            "Epoch: 2745 - Train Loss: 0.0001259720156667754 - Test Loss: 0.01476460974663496\n",
            "Epoch: 2746 - Train Loss: 0.00012543061166070402 - Test Loss: 0.014764231629669666\n",
            "Epoch: 2747 - Train Loss: 0.0001248914486495778 - Test Loss: 0.014763851650059223\n",
            "Epoch: 2748 - Train Loss: 0.00012435448297765106 - Test Loss: 0.014763474464416504\n",
            "Epoch: 2749 - Train Loss: 0.0001238197583006695 - Test Loss: 0.014763101004064083\n",
            "Epoch: 2750 - Train Loss: 0.0001232872309628874 - Test Loss: 0.014762728475034237\n",
            "Epoch: 2751 - Train Loss: 0.00012275685730855912 - Test Loss: 0.01476235780864954\n",
            "Epoch: 2752 - Train Loss: 0.00012222865188959986 - Test Loss: 0.014761989936232567\n",
            "Epoch: 2753 - Train Loss: 0.00012170259287813678 - Test Loss: 0.014761628583073616\n",
            "Epoch: 2754 - Train Loss: 0.0001211789931403473 - Test Loss: 0.014761267229914665\n",
            "Epoch: 2755 - Train Loss: 0.00012065747432643548 - Test Loss: 0.014760908670723438\n",
            "Epoch: 2756 - Train Loss: 0.00012013778177788481 - Test Loss: 0.014760551042854786\n",
            "Epoch: 2757 - Train Loss: 0.00011962023563683033 - Test Loss: 0.014760195277631283\n",
            "Epoch: 2758 - Train Loss: 0.00011910506873391569 - Test Loss: 0.014759843237698078\n",
            "Epoch: 2759 - Train Loss: 0.0001185919827548787 - Test Loss: 0.014759492129087448\n",
            "Epoch: 2760 - Train Loss: 0.00011808067938545719 - Test Loss: 0.014759141020476818\n",
            "Epoch: 2761 - Train Loss: 0.00011757150787161663 - Test Loss: 0.014758792705833912\n",
            "Epoch: 2762 - Train Loss: 0.00011706461373250932 - Test Loss: 0.014758446253836155\n",
            "Epoch: 2763 - Train Loss: 0.00011655984417302534 - Test Loss: 0.014758101664483547\n",
            "Epoch: 2764 - Train Loss: 0.000116056835395284 - Test Loss: 0.014757758006453514\n",
            "Epoch: 2765 - Train Loss: 0.00011555586388567463 - Test Loss: 0.014757417142391205\n",
            "Epoch: 2766 - Train Loss: 0.00011505718430271372 - Test Loss: 0.014757082797586918\n",
            "Epoch: 2767 - Train Loss: 0.00011456052743596956 - Test Loss: 0.01475675031542778\n",
            "Epoch: 2768 - Train Loss: 0.00011406590783735737 - Test Loss: 0.014756416901946068\n",
            "Epoch: 2769 - Train Loss: 0.00011357331095496193 - Test Loss: 0.014756087213754654\n",
            "Epoch: 2770 - Train Loss: 0.0001130826713051647 - Test Loss: 0.014755760319530964\n",
            "Epoch: 2771 - Train Loss: 0.00011259403254371136 - Test Loss: 0.014755435287952423\n",
            "Epoch: 2772 - Train Loss: 0.00011210735101485625 - Test Loss: 0.014755108393728733\n",
            "Epoch: 2773 - Train Loss: 0.00011162261944264174 - Test Loss: 0.014754786156117916\n",
            "Epoch: 2774 - Train Loss: 0.00011113992513855919 - Test Loss: 0.0147544639185071\n",
            "Epoch: 2775 - Train Loss: 0.0001106591516872868 - Test Loss: 0.014754144474864006\n",
            "Epoch: 2776 - Train Loss: 0.00011018034274457023 - Test Loss: 0.014753826893866062\n",
            "Epoch: 2777 - Train Loss: 0.00010970341827487573 - Test Loss: 0.014753512106835842\n",
            "Epoch: 2778 - Train Loss: 0.00010922847286565229 - Test Loss: 0.014753195457160473\n",
            "Epoch: 2779 - Train Loss: 0.00010875571751967072 - Test Loss: 0.014752887189388275\n",
            "Epoch: 2780 - Train Loss: 0.00010828487575054169 - Test Loss: 0.014752579852938652\n",
            "Epoch: 2781 - Train Loss: 0.00010781571472762153 - Test Loss: 0.014752272516489029\n",
            "Epoch: 2782 - Train Loss: 0.00010734840907389298 - Test Loss: 0.014751969836652279\n",
            "Epoch: 2783 - Train Loss: 0.00010688332986319438 - Test Loss: 0.014751666225492954\n",
            "Epoch: 2784 - Train Loss: 0.00010642007691785693 - Test Loss: 0.014751366339623928\n",
            "Epoch: 2785 - Train Loss: 0.00010595849744277075 - Test Loss: 0.014751066453754902\n",
            "Epoch: 2786 - Train Loss: 0.00010549873695708811 - Test Loss: 0.014750770293176174\n",
            "Epoch: 2787 - Train Loss: 0.00010504115198273212 - Test Loss: 0.014750473201274872\n",
            "Epoch: 2788 - Train Loss: 0.00010458540782565251 - Test Loss: 0.014750177972018719\n",
            "Epoch: 2789 - Train Loss: 0.0001041312498273328 - Test Loss: 0.014749884605407715\n",
            "Epoch: 2790 - Train Loss: 0.0001036789471982047 - Test Loss: 0.01474959310144186\n",
            "Epoch: 2791 - Train Loss: 0.0001032287473208271 - Test Loss: 0.01474930252879858\n",
            "Epoch: 2792 - Train Loss: 0.00010278038098476827 - Test Loss: 0.014749014750123024\n",
            "Epoch: 2793 - Train Loss: 0.00010233367356704548 - Test Loss: 0.014748731628060341\n",
            "Epoch: 2794 - Train Loss: 0.00010188938904320821 - Test Loss: 0.014748452231287956\n",
            "Epoch: 2795 - Train Loss: 0.0001014464651234448 - Test Loss: 0.014748170040547848\n",
            "Epoch: 2796 - Train Loss: 0.00010100594226969406 - Test Loss: 0.014747895300388336\n",
            "Epoch: 2797 - Train Loss: 0.00010056671453639865 - Test Loss: 0.014747616834938526\n",
            "Epoch: 2798 - Train Loss: 0.00010012988786911592 - Test Loss: 0.01474734116345644\n",
            "Epoch: 2799 - Train Loss: 9.969435632228851e-05 - Test Loss: 0.014747069217264652\n",
            "Epoch: 2800 - Train Loss: 9.926117490977049e-05 - Test Loss: 0.01474679633975029\n",
            "Epoch: 2801 - Train Loss: 9.88293468253687e-05 - Test Loss: 0.014746527187526226\n",
            "Epoch: 2802 - Train Loss: 9.839981066761538e-05 - Test Loss: 0.014746258035302162\n",
            "Epoch: 2803 - Train Loss: 9.797159145819023e-05 - Test Loss: 0.014745988883078098\n",
            "Epoch: 2804 - Train Loss: 9.754557686392218e-05 - Test Loss: 0.014745722524821758\n",
            "Epoch: 2805 - Train Loss: 9.712086466606706e-05 - Test Loss: 0.014745457097887993\n",
            "Epoch: 2806 - Train Loss: 9.669837163528427e-05 - Test Loss: 0.014745197258889675\n",
            "Epoch: 2807 - Train Loss: 9.627770486986265e-05 - Test Loss: 0.014744941145181656\n",
            "Epoch: 2808 - Train Loss: 9.585847874404863e-05 - Test Loss: 0.014744684100151062\n",
            "Epoch: 2809 - Train Loss: 9.54409915721044e-05 - Test Loss: 0.014744429849088192\n",
            "Epoch: 2810 - Train Loss: 9.502546890871599e-05 - Test Loss: 0.014744175598025322\n",
            "Epoch: 2811 - Train Loss: 9.461167792323977e-05 - Test Loss: 0.014743926003575325\n",
            "Epoch: 2812 - Train Loss: 9.419932030141354e-05 - Test Loss: 0.014743673615157604\n",
            "Epoch: 2813 - Train Loss: 9.378868708154187e-05 - Test Loss: 0.014743424952030182\n",
            "Epoch: 2814 - Train Loss: 9.338001109426841e-05 - Test Loss: 0.014743177220225334\n",
            "Epoch: 2815 - Train Loss: 9.297303040511906e-05 - Test Loss: 0.01474293228238821\n",
            "Epoch: 2816 - Train Loss: 9.25674830796197e-05 - Test Loss: 0.014742685481905937\n",
            "Epoch: 2817 - Train Loss: 9.216358012054116e-05 - Test Loss: 0.014742441475391388\n",
            "Epoch: 2818 - Train Loss: 9.176160529023036e-05 - Test Loss: 0.014742199331521988\n",
            "Epoch: 2819 - Train Loss: 9.136122389463708e-05 - Test Loss: 0.014741957187652588\n",
            "Epoch: 2820 - Train Loss: 9.096234134631231e-05 - Test Loss: 0.014741717837750912\n",
            "Epoch: 2821 - Train Loss: 9.05653287190944e-05 - Test Loss: 0.014741484075784683\n",
            "Epoch: 2822 - Train Loss: 9.016992407850921e-05 - Test Loss: 0.014741250313818455\n",
            "Epoch: 2823 - Train Loss: 8.977617835626006e-05 - Test Loss: 0.01474101934581995\n",
            "Epoch: 2824 - Train Loss: 8.938404789660126e-05 - Test Loss: 0.01474078930914402\n",
            "Epoch: 2825 - Train Loss: 8.899356180336326e-05 - Test Loss: 0.01474055927246809\n",
            "Epoch: 2826 - Train Loss: 8.860469824867323e-05 - Test Loss: 0.014740332029759884\n",
            "Epoch: 2827 - Train Loss: 8.821747178444639e-05 - Test Loss: 0.014740104787051678\n",
            "Epoch: 2828 - Train Loss: 8.783186785876751e-05 - Test Loss: 0.01473988126963377\n",
            "Epoch: 2829 - Train Loss: 8.74478864716366e-05 - Test Loss: 0.014739657752215862\n",
            "Epoch: 2830 - Train Loss: 8.706548396730796e-05 - Test Loss: 0.01473943516612053\n",
            "Epoch: 2831 - Train Loss: 8.668471855344251e-05 - Test Loss: 0.014739212580025196\n",
            "Epoch: 2832 - Train Loss: 8.630548109067604e-05 - Test Loss: 0.014738994650542736\n",
            "Epoch: 2833 - Train Loss: 8.592789527028799e-05 - Test Loss: 0.014738774858415127\n",
            "Epoch: 2834 - Train Loss: 8.555181557312608e-05 - Test Loss: 0.014738558791577816\n",
            "Epoch: 2835 - Train Loss: 8.517727110302076e-05 - Test Loss: 0.014738340862095356\n",
            "Epoch: 2836 - Train Loss: 8.480456745019183e-05 - Test Loss: 0.014738132245838642\n",
            "Epoch: 2837 - Train Loss: 8.443317346973345e-05 - Test Loss: 0.014737923629581928\n",
            "Epoch: 2838 - Train Loss: 8.406329288845882e-05 - Test Loss: 0.014737713150680065\n",
            "Epoch: 2839 - Train Loss: 8.369518036488444e-05 - Test Loss: 0.014737505465745926\n",
            "Epoch: 2840 - Train Loss: 8.332856668857858e-05 - Test Loss: 0.01473730243742466\n",
            "Epoch: 2841 - Train Loss: 8.29632772365585e-05 - Test Loss: 0.014737095683813095\n",
            "Epoch: 2842 - Train Loss: 8.259950845967978e-05 - Test Loss: 0.014736891724169254\n",
            "Epoch: 2843 - Train Loss: 8.223743498092517e-05 - Test Loss: 0.014736691489815712\n",
            "Epoch: 2844 - Train Loss: 8.187684579752386e-05 - Test Loss: 0.014736490324139595\n",
            "Epoch: 2845 - Train Loss: 8.151753718266264e-05 - Test Loss: 0.014736289158463478\n",
            "Epoch: 2846 - Train Loss: 8.115969103528187e-05 - Test Loss: 0.014736092649400234\n",
            "Epoch: 2847 - Train Loss: 8.080360566964373e-05 - Test Loss: 0.014735894277691841\n",
            "Epoch: 2848 - Train Loss: 8.044904825510457e-05 - Test Loss: 0.014735697768628597\n",
            "Epoch: 2849 - Train Loss: 8.009579323697835e-05 - Test Loss: 0.014735504053533077\n",
            "Epoch: 2850 - Train Loss: 7.974403706612065e-05 - Test Loss: 0.014735308475792408\n",
            "Epoch: 2851 - Train Loss: 7.939420902403072e-05 - Test Loss: 0.01473512127995491\n",
            "Epoch: 2852 - Train Loss: 7.904548692749813e-05 - Test Loss: 0.014734933152794838\n",
            "Epoch: 2853 - Train Loss: 7.869862020015717e-05 - Test Loss: 0.014734748750925064\n",
            "Epoch: 2854 - Train Loss: 7.835281576262787e-05 - Test Loss: 0.014734562486410141\n",
            "Epoch: 2855 - Train Loss: 7.80089249019511e-05 - Test Loss: 0.014734379015862942\n",
            "Epoch: 2856 - Train Loss: 7.76660381234251e-05 - Test Loss: 0.014734196476638317\n",
            "Epoch: 2857 - Train Loss: 7.732505036983639e-05 - Test Loss: 0.014734015800058842\n",
            "Epoch: 2858 - Train Loss: 7.698505942244083e-05 - Test Loss: 0.014733835123479366\n",
            "Epoch: 2859 - Train Loss: 7.664688746444881e-05 - Test Loss: 0.01473365630954504\n",
            "Epoch: 2860 - Train Loss: 7.630969776073471e-05 - Test Loss: 0.014733477495610714\n",
            "Epoch: 2861 - Train Loss: 7.597435615025461e-05 - Test Loss: 0.014733300544321537\n",
            "Epoch: 2862 - Train Loss: 7.564004044979811e-05 - Test Loss: 0.01473312359303236\n",
            "Epoch: 2863 - Train Loss: 7.530747097916901e-05 - Test Loss: 0.014732951298356056\n",
            "Epoch: 2864 - Train Loss: 7.497587648686022e-05 - Test Loss: 0.014732775278389454\n",
            "Epoch: 2865 - Train Loss: 7.464614463970065e-05 - Test Loss: 0.01473260298371315\n",
            "Epoch: 2866 - Train Loss: 7.431730773532763e-05 - Test Loss: 0.014732430689036846\n",
            "Epoch: 2867 - Train Loss: 7.399030437227339e-05 - Test Loss: 0.01473226398229599\n",
            "Epoch: 2868 - Train Loss: 7.366466161329299e-05 - Test Loss: 0.014732101000845432\n",
            "Epoch: 2869 - Train Loss: 7.334013207582757e-05 - Test Loss: 0.014731934294104576\n",
            "Epoch: 2870 - Train Loss: 7.301699952222407e-05 - Test Loss: 0.014731770381331444\n",
            "Epoch: 2871 - Train Loss: 7.269538764376193e-05 - Test Loss: 0.01473161019384861\n",
            "Epoch: 2872 - Train Loss: 7.237512909341604e-05 - Test Loss: 0.014731450006365776\n",
            "Epoch: 2873 - Train Loss: 7.205602742033079e-05 - Test Loss: 0.014731287956237793\n",
            "Epoch: 2874 - Train Loss: 7.17382354196161e-05 - Test Loss: 0.014731128700077534\n",
            "Epoch: 2875 - Train Loss: 7.142201502574608e-05 - Test Loss: 0.014730971306562424\n",
            "Epoch: 2876 - Train Loss: 7.110711885616183e-05 - Test Loss: 0.014730814844369888\n",
            "Epoch: 2877 - Train Loss: 7.079329952830449e-05 - Test Loss: 0.014730657450854778\n",
            "Epoch: 2878 - Train Loss: 7.0480840804521e-05 - Test Loss: 0.014730502851307392\n",
            "Epoch: 2879 - Train Loss: 7.016988092800602e-05 - Test Loss: 0.014730348251760006\n",
            "Epoch: 2880 - Train Loss: 6.98602307238616e-05 - Test Loss: 0.014730195514857769\n",
            "Epoch: 2881 - Train Loss: 6.955164280952886e-05 - Test Loss: 0.014730042777955532\n",
            "Epoch: 2882 - Train Loss: 6.924432091182098e-05 - Test Loss: 0.01472989097237587\n",
            "Epoch: 2883 - Train Loss: 6.893855606904253e-05 - Test Loss: 0.014729740098118782\n",
            "Epoch: 2884 - Train Loss: 6.863405724288896e-05 - Test Loss: 0.014729594811797142\n",
            "Epoch: 2885 - Train Loss: 6.833083898527548e-05 - Test Loss: 0.014729452319443226\n",
            "Epoch: 2886 - Train Loss: 6.802890857215971e-05 - Test Loss: 0.014729308895766735\n",
            "Epoch: 2887 - Train Loss: 6.772828055545688e-05 - Test Loss: 0.014729166403412819\n",
            "Epoch: 2888 - Train Loss: 6.74290131428279e-05 - Test Loss: 0.014729023911058903\n",
            "Epoch: 2889 - Train Loss: 6.713101174682379e-05 - Test Loss: 0.014728885143995285\n",
            "Epoch: 2890 - Train Loss: 6.683426909148693e-05 - Test Loss: 0.014728745445609093\n",
            "Epoch: 2891 - Train Loss: 6.653874879702926e-05 - Test Loss: 0.014728606678545475\n",
            "Epoch: 2892 - Train Loss: 6.624452362302691e-05 - Test Loss: 0.014728468842804432\n",
            "Epoch: 2893 - Train Loss: 6.595152808586136e-05 - Test Loss: 0.014728332869708538\n",
            "Epoch: 2894 - Train Loss: 6.565982039319351e-05 - Test Loss: 0.014728196896612644\n",
            "Epoch: 2895 - Train Loss: 6.536934233736247e-05 - Test Loss: 0.014728063717484474\n",
            "Epoch: 2896 - Train Loss: 6.508009391836822e-05 - Test Loss: 0.014727928675711155\n",
            "Epoch: 2897 - Train Loss: 6.479208241216838e-05 - Test Loss: 0.014727797359228134\n",
            "Epoch: 2898 - Train Loss: 6.450526416301727e-05 - Test Loss: 0.014727665111422539\n",
            "Epoch: 2899 - Train Loss: 6.421969010261819e-05 - Test Loss: 0.014727533794939518\n",
            "Epoch: 2900 - Train Loss: 6.393533840309829e-05 - Test Loss: 0.01472740713506937\n",
            "Epoch: 2901 - Train Loss: 6.365236913552508e-05 - Test Loss: 0.014727282337844372\n",
            "Epoch: 2902 - Train Loss: 6.337060040095821e-05 - Test Loss: 0.014727158471941948\n",
            "Epoch: 2903 - Train Loss: 6.308985757641494e-05 - Test Loss: 0.014727034606039524\n",
            "Epoch: 2904 - Train Loss: 6.281023524934426e-05 - Test Loss: 0.014726911671459675\n",
            "Epoch: 2905 - Train Loss: 6.25320099061355e-05 - Test Loss: 0.014726790599524975\n",
            "Epoch: 2906 - Train Loss: 6.225497054401785e-05 - Test Loss: 0.014726671390235424\n",
            "Epoch: 2907 - Train Loss: 6.197886250447482e-05 - Test Loss: 0.014726552180945873\n",
            "Epoch: 2908 - Train Loss: 6.17039404460229e-05 - Test Loss: 0.014726432040333748\n",
            "Epoch: 2909 - Train Loss: 6.143034988781437e-05 - Test Loss: 0.014726312831044197\n",
            "Epoch: 2910 - Train Loss: 6.115793075878173e-05 - Test Loss: 0.01472619641572237\n",
            "Epoch: 2911 - Train Loss: 6.088645022828132e-05 - Test Loss: 0.014726080000400543\n",
            "Epoch: 2912 - Train Loss: 6.061610110918991e-05 - Test Loss: 0.014725964516401291\n",
            "Epoch: 2913 - Train Loss: 6.034708712832071e-05 - Test Loss: 0.014725848101079464\n",
            "Epoch: 2914 - Train Loss: 6.0079211834818125e-05 - Test Loss: 0.014725733548402786\n",
            "Epoch: 2915 - Train Loss: 5.9812282415805385e-05 - Test Loss: 0.014725619927048683\n",
            "Epoch: 2916 - Train Loss: 5.954646985628642e-05 - Test Loss: 0.01472550630569458\n",
            "Epoch: 2917 - Train Loss: 5.9281937865307555e-05 - Test Loss: 0.014725394546985626\n",
            "Epoch: 2918 - Train Loss: 5.9018559113610536e-05 - Test Loss: 0.01472529023885727\n",
            "Epoch: 2919 - Train Loss: 5.8756257203640416e-05 - Test Loss: 0.014725182205438614\n",
            "Epoch: 2920 - Train Loss: 5.849506487720646e-05 - Test Loss: 0.014725077897310257\n",
            "Epoch: 2921 - Train Loss: 5.823499304824509e-05 - Test Loss: 0.0147249735891819\n",
            "Epoch: 2922 - Train Loss: 5.7975972595158964e-05 - Test Loss: 0.014724869281053543\n",
            "Epoch: 2923 - Train Loss: 5.771805444965139e-05 - Test Loss: 0.01472476590424776\n",
            "Epoch: 2924 - Train Loss: 5.7461274991510436e-05 - Test Loss: 0.014724665321409702\n",
            "Epoch: 2925 - Train Loss: 5.720550689147785e-05 - Test Loss: 0.01472456380724907\n",
            "Epoch: 2926 - Train Loss: 5.69508301850874e-05 - Test Loss: 0.014724462293088436\n",
            "Epoch: 2927 - Train Loss: 5.669723032042384e-05 - Test Loss: 0.014724362641572952\n",
            "Epoch: 2928 - Train Loss: 5.644474731525406e-05 - Test Loss: 0.014724262058734894\n",
            "Epoch: 2929 - Train Loss: 5.619325747829862e-05 - Test Loss: 0.014724165201187134\n",
            "Epoch: 2930 - Train Loss: 5.594289905275218e-05 - Test Loss: 0.0147240674123168\n",
            "Epoch: 2931 - Train Loss: 5.569358836510219e-05 - Test Loss: 0.01472396869212389\n",
            "Epoch: 2932 - Train Loss: 5.544528903556056e-05 - Test Loss: 0.014723874628543854\n",
            "Epoch: 2933 - Train Loss: 5.519803016795777e-05 - Test Loss: 0.014723777770996094\n",
            "Epoch: 2934 - Train Loss: 5.495184086612426e-05 - Test Loss: 0.014723684638738632\n",
            "Epoch: 2935 - Train Loss: 5.4706641094526276e-05 - Test Loss: 0.014723590575158596\n",
            "Epoch: 2936 - Train Loss: 5.446249997476116e-05 - Test Loss: 0.01472349651157856\n",
            "Epoch: 2937 - Train Loss: 5.421961031970568e-05 - Test Loss: 0.014723407104611397\n",
            "Epoch: 2938 - Train Loss: 5.3977491916157305e-05 - Test Loss: 0.014723317697644234\n",
            "Epoch: 2939 - Train Loss: 5.3736457630293444e-05 - Test Loss: 0.014723232947289944\n",
            "Epoch: 2940 - Train Loss: 5.349663115339354e-05 - Test Loss: 0.01472314540296793\n",
            "Epoch: 2941 - Train Loss: 5.325773963704705e-05 - Test Loss: 0.01472306065261364\n",
            "Epoch: 2942 - Train Loss: 5.301974670146592e-05 - Test Loss: 0.01472297590225935\n",
            "Epoch: 2943 - Train Loss: 5.278276148601435e-05 - Test Loss: 0.01472289115190506\n",
            "Epoch: 2944 - Train Loss: 5.254689676803537e-05 - Test Loss: 0.01472280640155077\n",
            "Epoch: 2945 - Train Loss: 5.231207251199521e-05 - Test Loss: 0.014722723513841629\n",
            "Epoch: 2946 - Train Loss: 5.207807771512307e-05 - Test Loss: 0.014722641557455063\n",
            "Epoch: 2947 - Train Loss: 5.18450397066772e-05 - Test Loss: 0.014722559601068497\n",
            "Epoch: 2948 - Train Loss: 5.161313674761914e-05 - Test Loss: 0.01472247950732708\n",
            "Epoch: 2949 - Train Loss: 5.13822706125211e-05 - Test Loss: 0.014722398482263088\n",
            "Epoch: 2950 - Train Loss: 5.1152183004887775e-05 - Test Loss: 0.014722318388521671\n",
            "Epoch: 2951 - Train Loss: 5.092305946163833e-05 - Test Loss: 0.014722239226102829\n",
            "Epoch: 2952 - Train Loss: 5.069512553745881e-05 - Test Loss: 0.014722161926329136\n",
            "Epoch: 2953 - Train Loss: 5.046813748776913e-05 - Test Loss: 0.014722083695232868\n",
            "Epoch: 2954 - Train Loss: 5.0241938879480585e-05 - Test Loss: 0.014722004532814026\n",
            "Epoch: 2955 - Train Loss: 5.001671888749115e-05 - Test Loss: 0.014721929095685482\n",
            "Epoch: 2956 - Train Loss: 4.979258301318623e-05 - Test Loss: 0.014721858315169811\n",
            "Epoch: 2957 - Train Loss: 4.956940756528638e-05 - Test Loss: 0.014721786603331566\n",
            "Epoch: 2958 - Train Loss: 4.9347207095706835e-05 - Test Loss: 0.01472171675413847\n",
            "Epoch: 2959 - Train Loss: 4.912587610306218e-05 - Test Loss: 0.014721645973622799\n",
            "Epoch: 2960 - Train Loss: 4.8905538278631866e-05 - Test Loss: 0.014721577987074852\n",
            "Epoch: 2961 - Train Loss: 4.868613905273378e-05 - Test Loss: 0.014721508137881756\n",
            "Epoch: 2962 - Train Loss: 4.846765659749508e-05 - Test Loss: 0.014721442013978958\n",
            "Epoch: 2963 - Train Loss: 4.825010182685219e-05 - Test Loss: 0.014721374958753586\n",
            "Epoch: 2964 - Train Loss: 4.8033500206656754e-05 - Test Loss: 0.014721308834850788\n",
            "Epoch: 2965 - Train Loss: 4.7817782615311444e-05 - Test Loss: 0.014721241779625416\n",
            "Epoch: 2966 - Train Loss: 4.7602989070583135e-05 - Test Loss: 0.014721173793077469\n",
            "Epoch: 2967 - Train Loss: 4.73891559522599e-05 - Test Loss: 0.014721112325787544\n",
            "Epoch: 2968 - Train Loss: 4.717618867289275e-05 - Test Loss: 0.014721045270562172\n",
            "Epoch: 2969 - Train Loss: 4.696411997429095e-05 - Test Loss: 0.014720981940627098\n",
            "Epoch: 2970 - Train Loss: 4.675298623624258e-05 - Test Loss: 0.014720918610692024\n",
            "Epoch: 2971 - Train Loss: 4.654270378523506e-05 - Test Loss: 0.014720854349434376\n",
            "Epoch: 2972 - Train Loss: 4.633333082892932e-05 - Test Loss: 0.014720792882144451\n",
            "Epoch: 2973 - Train Loss: 4.612488919519819e-05 - Test Loss: 0.014720732346177101\n",
            "Epoch: 2974 - Train Loss: 4.591726610669866e-05 - Test Loss: 0.014720669947564602\n",
            "Epoch: 2975 - Train Loss: 4.571058525471017e-05 - Test Loss: 0.014720611274242401\n",
            "Epoch: 2976 - Train Loss: 4.5504788431571797e-05 - Test Loss: 0.014720551669597626\n",
            "Epoch: 2977 - Train Loss: 4.5300093916011974e-05 - Test Loss: 0.014720495790243149\n",
            "Epoch: 2978 - Train Loss: 4.509616701398045e-05 - Test Loss: 0.014720441773533821\n",
            "Epoch: 2979 - Train Loss: 4.4893000449519604e-05 - Test Loss: 0.014720386825501919\n",
            "Epoch: 2980 - Train Loss: 4.469071427593008e-05 - Test Loss: 0.014720333740115166\n",
            "Epoch: 2981 - Train Loss: 4.448946492630057e-05 - Test Loss: 0.014720282517373562\n",
            "Epoch: 2982 - Train Loss: 4.4289088691584766e-05 - Test Loss: 0.014720231294631958\n",
            "Epoch: 2983 - Train Loss: 4.4089447328587994e-05 - Test Loss: 0.014720178209245205\n",
            "Epoch: 2984 - Train Loss: 4.3890679080504924e-05 - Test Loss: 0.014720126986503601\n",
            "Epoch: 2985 - Train Loss: 4.3692885810742155e-05 - Test Loss: 0.014720076695084572\n",
            "Epoch: 2986 - Train Loss: 4.3495943828020245e-05 - Test Loss: 0.014720028266310692\n",
            "Epoch: 2987 - Train Loss: 4.3299700337229297e-05 - Test Loss: 0.014719977043569088\n",
            "Epoch: 2988 - Train Loss: 4.310432996135205e-05 - Test Loss: 0.014719928614795208\n",
            "Epoch: 2989 - Train Loss: 4.290997458156198e-05 - Test Loss: 0.014719880186021328\n",
            "Epoch: 2990 - Train Loss: 4.271641228115186e-05 - Test Loss: 0.014719831757247448\n",
            "Epoch: 2991 - Train Loss: 4.252351209288463e-05 - Test Loss: 0.014719783328473568\n",
            "Epoch: 2992 - Train Loss: 4.233145227772184e-05 - Test Loss: 0.014719737693667412\n",
            "Epoch: 2993 - Train Loss: 4.214034925098531e-05 - Test Loss: 0.014719691127538681\n",
            "Epoch: 2994 - Train Loss: 4.1950079321395606e-05 - Test Loss: 0.01471964456140995\n",
            "Epoch: 2995 - Train Loss: 4.1760493331821635e-05 - Test Loss: 0.014719598926603794\n",
            "Epoch: 2996 - Train Loss: 4.15717477153521e-05 - Test Loss: 0.014719555154442787\n",
            "Epoch: 2997 - Train Loss: 4.1384046198800206e-05 - Test Loss: 0.014719515107572079\n",
            "Epoch: 2998 - Train Loss: 4.119689765502699e-05 - Test Loss: 0.014719473198056221\n",
            "Epoch: 2999 - Train Loss: 4.101084778085351e-05 - Test Loss: 0.014719436876475811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "qEglvhg0WQPZ",
        "outputId": "00b2b49e-4feb-424a-a3b2-e60b4361d0b8"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.plot(train_mses)\r\n",
        "plt.plot(test_mses)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5270e18908>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCMy61eqWcv2",
        "outputId": "361bfec8-5b91-438c-dc14-f957e471ca83"
      },
      "source": [
        "print(masked_mse(tf.matmul(user_emb, item_emb, transpose_b=True) + global_effects, train_urm, mask,0.,0.,0.,0.))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(4.1010848e-05, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh8FGg1kWf9k",
        "outputId": "e0581d48-a628-4b99-e0ce-c1bf813b4e89"
      },
      "source": [
        "test_mask = tf.not_equal(test_urm, 0.)\r\n",
        "masked_mse(tf.matmul(user_emb, item_emb, transpose_b=True) + global_effects, test_urm, test_mask, 0.,0.,0.,0.)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.014719437>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2ovesnPWh-F",
        "outputId": "804e3fa1-3f24-400d-ffcd-cf0becdecf7c"
      },
      "source": [
        "print(tf.boolean_mask(tf.matmul(user_emb, item_emb, transpose_b=True) + global_effects, test_mask))\r\n",
        "print(tf.boolean_mask(urm, test_mask))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([5.0182815 3.7925973 5.0045686 ... 3.2204056 3.504787  4.144094 ], shape=(10084,), dtype=float32)\n",
            "tf.Tensor([5.0187025 3.7919607 5.0068946 ... 3.2200649 3.501555  4.1439705], shape=(10084,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyzRWmiyWmUv"
      },
      "source": [
        "# Hybrid Linear Combination\r\n",
        "\r\n",
        "def get_hybrid_rating_given_user(u_ix, item_ix, k, alpha, beta):\r\n",
        "  return alpha * cbf_get_rating_given_user(u_ix, item_ix, k)[0] + \\\r\n",
        "        beta * ucf_get_rating_given_user(u_ix, item_ix, k)[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRXeiXwIWrYa",
        "outputId": "ae22c8ee-ad29-42a1-e18a-2b322277251c"
      },
      "source": [
        "ratings = pd.read_csv(CUR_DIR + '/ml-latest-small/ratings.csv')\r\n",
        "\r\n",
        "C = 3\r\n",
        "total_mean = ratings.rating.mean()\r\n",
        "ratings['normalized_rating'] = ratings.rating - total_mean\r\n",
        "\r\n",
        "b_item = ratings.groupby('movieId').normalized_rating.sum() / (ratings.groupby('movieId').userId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_item, columns=['b_item']), left_on='movieId', right_index=True, how='inner')\r\n",
        "ratings['norm_item_rating'] = ratings.normalized_rating - ratings.b_item\r\n",
        "\r\n",
        "b_user = ratings.groupby('userId').norm_item_rating.sum() / (ratings.groupby('userId').movieId.count() + C)\r\n",
        "ratings = ratings.merge(pd.DataFrame(b_user, columns=['b_user']), left_on='userId', right_index=True, how='inner')\r\n",
        "\r\n",
        "ratings['normr_user_item_rating'] = total_mean + ratings.b_item + ratings.b_user\r\n",
        "urm = ratings.pivot(index='userId', columns='movieId', values='normr_user_item_rating').fillna(0.).values\r\n",
        "\r\n",
        "get_hybrid_rating_given_user(25,15,100, 0.9, 1.9)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.5497309], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f92JfJtaWuAA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}