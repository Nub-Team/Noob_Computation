{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceClassification_using_LSTM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbuKNCG199xjfZRotc+iJP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nub-T/Noob_Computation/blob/main/SequenceClassification_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-aY-31UXODS"
      },
      "source": [
        "<b>SEQUENCE CLASSIFICATION</b><br>\r\n",
        "--\r\n",
        "Coded by :\r\n",
        "* <a href=\"https://github.com/Nub-T\">Nub-T</a>\r\n",
        "* <a href=\"https://github.com/Andi-Nov\">Andi-Nov</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmlFKMbtXExf",
        "outputId": "2217aaa4-3047-4314-c153-69a821d7e2fb"
      },
      "source": [
        "!wget http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 21:32:41--  http://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  58.5MB/s    in 1.4s    \n",
            "\n",
            "2020-12-09 21:32:42 (58.5 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tyBPlYZX04p"
      },
      "source": [
        "!tar -xzf aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GZA-JEQX9b0",
        "outputId": "7a892602-10b9-4355-e41a-94a8d9fe4fe1"
      },
      "source": [
        "# Now import the library!\r\n",
        "import torch\r\n",
        "import math\r\n",
        "import numpy as np\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch import optim\r\n",
        "import os\r\n",
        "from collections import namedtuple\r\n",
        "\r\n",
        "np.random.seed(42)\r\n",
        "torch.manual_seed(42)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2dafd7cba0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJN4LipwYDm3"
      },
      "source": [
        "train_path = \"aclImdb/train/\" \r\n",
        "test_path = \"aclImdb/test/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McD0BOfbYUAk"
      },
      "source": [
        "batch_size = 100\r\n",
        "max_len = 300\r\n",
        "embedding_size = 300\r\n",
        "min_count = 2\r\n",
        "device = torch.device('cuda')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvk_wivRYWGD"
      },
      "source": [
        "Sentence = namedtuple('Sentence', ['index', 'tokens', 'label'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKcDF9cTYZev"
      },
      "source": [
        "def read_imdb_movie_dataset(dataset_path):\r\n",
        "    indices = []\r\n",
        "    text = []\r\n",
        "    rating = []\r\n",
        "    i = 0\r\n",
        "\r\n",
        "    for filename in os.listdir(os.path.join(dataset_path, \"pos\")):\r\n",
        "        file_path = os.path.join(dataset_path, \"pos\", filename)\r\n",
        "        data = open(file_path, 'r', encoding=\"ISO-8859-1\").read()\r\n",
        "        indices.append(i)\r\n",
        "        text.append(data)\r\n",
        "        rating.append(1)\r\n",
        "        i = i + 1\r\n",
        "\r\n",
        "    for filename in os.listdir(os.path.join(dataset_path, \"neg\")):\r\n",
        "        file_path = os.path.join(dataset_path, \"neg\", filename)\r\n",
        "        data = open(file_path, 'r', encoding=\"ISO-8859-1\").read()\r\n",
        "        indices.append(i)\r\n",
        "        text.append(data)\r\n",
        "        rating.append(0)\r\n",
        "        i = i + 1\r\n",
        "\r\n",
        "    sentences = [ Sentence(index, text.split(), rating)\r\n",
        "                  for index, text, rating in zip(indices, text, rating)]\r\n",
        "\r\n",
        "    return sentences"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgklVlUuYdx_"
      },
      "source": [
        "train_examples = read_imdb_movie_dataset(train_path)\r\n",
        "test_examples = read_imdb_movie_dataset(test_path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoLj1p9oYmC1",
        "outputId": "4a7b3623-a15b-4278-b42c-e48894c0b113"
      },
      "source": [
        "len(train_examples)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkVN-YORYpF9",
        "outputId": "4ddfc889-d199-4eb5-e7b6-6a617ba9622a"
      },
      "source": [
        "len(test_examples)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pq3iDToYq1t"
      },
      "source": [
        "UNK = '<UNK>'\r\n",
        "PAD = '<PAD>'\r\n",
        "BOS = '<BOS>'\r\n",
        "EOS = '<EOS>'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCMbZnLzYsX2"
      },
      "source": [
        "class VocabItem:\r\n",
        "    def __init__(self, string, hash=None):\r\n",
        "        self.string = string\r\n",
        "        self.count = 0\r\n",
        "        self.hash = hash\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        return 'VocabItem({})'.format(self.string)\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return self.__str__()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLvrZ3YkYxH6"
      },
      "source": [
        "class Vocab:\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        min_count=0,\r\n",
        "        no_unk=False,\r\n",
        "        add_padding=False,\r\n",
        "        add_bos=False,\r\n",
        "        add_eos=False,\r\n",
        "        unk=None):\r\n",
        "      \r\n",
        "        self.no_unk = no_unk\r\n",
        "        self.vocab_items = []\r\n",
        "        self.vocab_hash = {}\r\n",
        "        self.word_count = 0\r\n",
        "        self.special_tokens = []\r\n",
        "        self.min_count = min_count\r\n",
        "        self.add_padding = add_padding\r\n",
        "        self.add_bos = add_bos\r\n",
        "        self.add_eos = add_eos\r\n",
        "        self.unk = unk\r\n",
        "\r\n",
        "        self.UNK = None\r\n",
        "        self.PAD = None\r\n",
        "        self.BOS = None\r\n",
        "        self.EOS = None\r\n",
        "\r\n",
        "        self.index2token = []\r\n",
        "        self.token2index = {}\r\n",
        "\r\n",
        "        self.finished = False\r\n",
        "\r\n",
        "    def add_tokens(self, tokens):\r\n",
        "        if self.finished:\r\n",
        "            raise RuntimeError('Vocabulary is finished')\r\n",
        "\r\n",
        "        for token in tokens:\r\n",
        "            if token not in self.vocab_hash:\r\n",
        "                self.vocab_hash[token] = len(self.vocab_items)\r\n",
        "                self.vocab_items.append(VocabItem(token))\r\n",
        "\r\n",
        "            self.vocab_items[self.vocab_hash[token]].count += 1\r\n",
        "            self.word_count += 1\r\n",
        "\r\n",
        "    def finish(self):\r\n",
        "\r\n",
        "        token2index = self.token2index\r\n",
        "        index2token = self.index2token\r\n",
        "\r\n",
        "        tmp = []\r\n",
        "\r\n",
        "        if not self.no_unk:\r\n",
        "            if self.unk:\r\n",
        "                self.UNK = VocabItem(self.unk, hash=0)\r\n",
        "                self.UNK.count = self.vocab_items[self.vocab_hash[self.unk]].count\r\n",
        "                index2token.append(self.UNK)\r\n",
        "                self.special_tokens.append(self.UNK)\r\n",
        "\r\n",
        "                for token in self.vocab_items:\r\n",
        "                    if token.string != self.unk:\r\n",
        "                        tmp.append(token)\r\n",
        "\r\n",
        "            else:\r\n",
        "                self.UNK = VocabItem(UNK, hash=0)\r\n",
        "                index2token.append(self.UNK)\r\n",
        "                self.special_tokens.append(self.UNK)\r\n",
        "\r\n",
        "                for token in self.vocab_items:\r\n",
        "                    if token.count <= self.min_count:\r\n",
        "                        self.UNK.count += token.count\r\n",
        "                    else:\r\n",
        "                        tmp.append(token)\r\n",
        "        else:\r\n",
        "            for token in self.vocab_items:\r\n",
        "                tmp.append(token)\r\n",
        "                \r\n",
        "        tmp.sort(key=lambda token: token.count, reverse=True)\r\n",
        "\r\n",
        "        if self.add_bos:\r\n",
        "            self.BOS = VocabItem(BOS)\r\n",
        "            tmp.append(self.BOS)\r\n",
        "            self.special_tokens.append(self.BOS)\r\n",
        "\r\n",
        "        if self.add_eos:\r\n",
        "            self.EOS = VocabItem(EOS)\r\n",
        "            tmp.append(self.EOS)\r\n",
        "            self.special_tokens.append(self.EOS)\r\n",
        "\r\n",
        "        if self.add_padding:\r\n",
        "            self.PAD = VocabItem(PAD)\r\n",
        "            tmp.append(self.PAD)\r\n",
        "            self.special_tokens.append(self.PAD)\r\n",
        "\r\n",
        "        index2token += tmp\r\n",
        "        for i, token in enumerate(self.index2token):\r\n",
        "            token2index[token.string] = i\r\n",
        "            token.hash = i\r\n",
        "\r\n",
        "        self.index2token = index2token\r\n",
        "        self.token2index = token2index\r\n",
        "\r\n",
        "        if not self.no_unk:\r\n",
        "            print('Unknown vocab size:', self.UNK.count)\r\n",
        "\r\n",
        "        print('Vocab size: %d' % len(self))\r\n",
        "\r\n",
        "        self.finished = True\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "        return self.index2token[i]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.index2token)\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        return iter(self.index2token)\r\n",
        "\r\n",
        "    def __contains__(self, key):\r\n",
        "        return key in self.token2index\r\n",
        "\r\n",
        "    def tokens2indices(self, tokens, add_bos=False, add_eos=False):\r\n",
        "        string_seq = []\r\n",
        "        if add_bos:\r\n",
        "            string_seq.append(self.BOS.hash)\r\n",
        "        for token in tokens:\r\n",
        "            if self.no_unk:\r\n",
        "                string_seq.append(self.token2index[token])\r\n",
        "            else:\r\n",
        "                string_seq.append(self.token2index.get(token, self.UNK.hash))\r\n",
        "        if add_eos:\r\n",
        "            string_seq.append(self.EOS.hash)\r\n",
        "        return string_seq\r\n",
        "\r\n",
        "    def indices2tokens(self, indices, ignore_ids=()):\r\n",
        "        tokens = []\r\n",
        "        for idx in indices:\r\n",
        "            if idx in ignore_ids:\r\n",
        "                continue\r\n",
        "            tokens.append(self.index2token[idx].string)\r\n",
        "\r\n",
        "        return tokens"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwr4144kY9xg"
      },
      "source": [
        "# We will use one vocabulary for the input data (the sentences), and another vocabulary object for the output data, the class labels. In this way our code is generic and should work out-of-the-box for any number of output labels.\r\n",
        "src_vocab = Vocab(min_count=min_count, add_padding=True)\r\n",
        "tgt_vocab = Vocab(no_unk=True, add_padding=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrf9S2q1ZFIA",
        "outputId": "c7ca4451-c28a-4746-992f-000fbcbad867"
      },
      "source": [
        "for sentence in train_examples:\r\n",
        "    src_vocab.add_tokens(sentence.tokens[:max_len])\r\n",
        "    tgt_vocab.add_tokens([sentence.label])\r\n",
        "\r\n",
        "src_vocab.finish()\r\n",
        "tgt_vocab.finish()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unknown vocab size: 211949\n",
            "Vocab size: 65055\n",
            "Vocab size: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL4vyhfCZGgU",
        "outputId": "4afb33d7-3a07-4590-cf6c-854a4d292d66"
      },
      "source": [
        "src_vocab.tokens2indices('the movie was bad'.split())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 19, 13, 97]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDFUs7QgZME0"
      },
      "source": [
        "Vocabs = namedtuple('Vocabs', ['src', 'tgt'])\r\n",
        "vocabs = Vocabs(src_vocab, tgt_vocab)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52KVcAyXZO3t"
      },
      "source": [
        "embeddings = nn.Embedding(\r\n",
        "    len(src_vocab),\r\n",
        "    embedding_size,\r\n",
        "    padding_idx=src_vocab.PAD.hash\r\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3IhQEdDZTYK",
        "outputId": "fd3cba69-602d-42d1-8c2e-02376d764577"
      },
      "source": [
        "embeddings.weight.size()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([65055, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvL1Ee3KZWRn"
      },
      "source": [
        "# We will combine our Batch object with a BatchTuple object that will hold data relevant to a specific input of the model.\r\n",
        "\r\n",
        "class Batch(dict):\r\n",
        "    def __init__(self, *args, **kwargs):\r\n",
        "        super(Batch, self).__init__(*args, **kwargs)\r\n",
        "        self.__dict__ = self\r\n",
        "        self._is_torch = False\r\n",
        "\r\n",
        "    def to_torch_(self, device):\r\n",
        "        self._is_torch = False\r\n",
        "        for key in self.keys():\r\n",
        "            value = self[key]\r\n",
        "            \r\n",
        "            if isinstance(value, BatchTuple):\r\n",
        "                value.to_torch_(device)\r\n",
        "                \r\n",
        "            if isinstance(value, np.ndarray):\r\n",
        "                self[key] = torch.from_numpy(value).to(device)\r\n",
        "\r\n",
        "\r\n",
        "class BatchTuple(object):\r\n",
        "    def __init__(self, sequences, lengths, sublengths, masks):\r\n",
        "        self.sequences = sequences\r\n",
        "        self.lengths = lengths\r\n",
        "        self.sublengths = sublengths\r\n",
        "        self.masks = masks\r\n",
        "        self._is_torch = False\r\n",
        "\r\n",
        "    def to_torch_(self, device):\r\n",
        "        if not self._is_torch:\r\n",
        "            self.sequences = torch.tensor(\r\n",
        "                self.sequences, device=device, dtype=torch.long\r\n",
        "            )\r\n",
        "\r\n",
        "            if self.lengths is not None:\r\n",
        "                self.lengths = torch.tensor(\r\n",
        "                    self.lengths, device=device, dtype=torch.long\r\n",
        "                )\r\n",
        "\r\n",
        "            if self.sublengths is not None:\r\n",
        "                self.sublengths = torch.tensor(\r\n",
        "                    self.sublengths, device=device, dtype=torch.long\r\n",
        "                )\r\n",
        "            if self.masks is not None:\r\n",
        "                self.masks = torch.tensor(\r\n",
        "                    self.masks, device=device, dtype=torch.float\r\n",
        "                )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJp-u412ZfEI"
      },
      "source": [
        "def pad_list(\r\n",
        "    sequences,\r\n",
        "    dim0_pad=None,\r\n",
        "    dim1_pad=None,\r\n",
        "    align_right=False,\r\n",
        "    pad_value=0\r\n",
        "):\r\n",
        "    sequences = [np.asarray(sublist) for sublist in sequences]\r\n",
        "    if not dim0_pad:\r\n",
        "        dim0_pad = len(sequences)\r\n",
        "\r\n",
        "    if not dim1_pad:\r\n",
        "        dim1_pad = max(len(seq) for seq in sequences)\r\n",
        "\r\n",
        "    out = np.full(shape=(dim0_pad, dim1_pad), fill_value=pad_value)\r\n",
        "    lengths = []\r\n",
        "\r\n",
        "    for i in range(len(sequences)):\r\n",
        "        data_length = len(sequences[i])\r\n",
        "        lengths.append(data_length)\r\n",
        "        offset = dim1_pad - data_length if align_right else 0\r\n",
        "        np.put(out[i], range(offset, offset + data_length), sequences[i])\r\n",
        "\r\n",
        "    lengths = np.array(lengths)\r\n",
        "    return out, lengths"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCX5zyYRZpRt"
      },
      "source": [
        "class SequenceClassificationBatchBuilder(object):\r\n",
        "    def __init__(self, vocabs, max_len=None):\r\n",
        "        self.vocabs = vocabs\r\n",
        "        self.max_len = max_len\r\n",
        "        \r\n",
        "    def __call__(self, examples):\r\n",
        "        ids_batch = [int(sentence.index) for sentence in examples]\r\n",
        "\r\n",
        "        src_examples = [\r\n",
        "            self.vocabs.src.tokens2indices(sentence.tokens[: self.max_len])\r\n",
        "            for sentence in examples\r\n",
        "        ]\r\n",
        "\r\n",
        "        tgt_examples = [\r\n",
        "            self.vocabs.tgt.token2index[sentence.label] for sentence in examples\r\n",
        "        ]\r\n",
        "\r\n",
        "        src_padded, src_lengths = pad_list(\r\n",
        "            src_examples, pad_value=self.vocabs.src.PAD.hash\r\n",
        "        )\r\n",
        "\r\n",
        "        src_batch_tuple = BatchTuple(src_padded, src_lengths, None, None)\r\n",
        "        tgt_batch_tuple = BatchTuple(tgt_examples, None, None, None)\r\n",
        "\r\n",
        "        return Batch(\r\n",
        "            indices=ids_batch, src=src_batch_tuple, tgt=tgt_batch_tuple\r\n",
        "        )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JVJRy3AZv92"
      },
      "source": [
        "# Let's instance our batch_builder, feed it into the DataLoader object alongside the training and test examples, and let's inspect a single batch of examples.\r\n",
        "\r\n",
        "batch_builder = SequenceClassificationBatchBuilder(\r\n",
        "    vocabs, max_len=max_len\r\n",
        ")\r\n",
        "\r\n",
        "train_batches = DataLoader(\r\n",
        "    train_examples,\r\n",
        "    batch_size=batch_size,\r\n",
        "    shuffle=True,\r\n",
        "    num_workers=0,\r\n",
        "    collate_fn=batch_builder,\r\n",
        ")\r\n",
        "\r\n",
        "test_batches = DataLoader(\r\n",
        "    test_examples,\r\n",
        "    batch_size=batch_size,\r\n",
        "    shuffle=False,\r\n",
        "    num_workers=0,\r\n",
        "    collate_fn=batch_builder,\r\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSk0NEeTZ1La",
        "outputId": "d16e5cbf-9bc1-4535-d07f-fbbd48371031"
      },
      "source": [
        "train_batches_iter = iter(train_batches)\r\n",
        "train_batch = next(train_batches_iter)\r\n",
        "train_batch.src.sequences"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  753,  7271,     6, ..., 65054, 65054, 65054],\n",
              "       [ 8489,    25,  1171, ..., 65054, 65054, 65054],\n",
              "       [  399,   283,  1491, ..., 65054, 65054, 65054],\n",
              "       ...,\n",
              "       [    0, 41565,     0, ..., 65054, 65054, 65054],\n",
              "       [45298,     0,     3, ..., 65054, 65054, 65054],\n",
              "       [   17,   128,     6, ..., 65054, 65054, 65054]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmAk7Sa7Z8ki"
      },
      "source": [
        "def mean_pooling(batch_hidden_states, batch_lengths):\r\n",
        "    batch_lengths = batch_lengths.float()\r\n",
        "    batch_lengths = batch_lengths.unsqueeze(1)\r\n",
        "    if batch_hidden_states.is_cuda:\r\n",
        "        batch_lengths = batch_lengths.cuda()\r\n",
        "\r\n",
        "    pooled_batch = torch.sum(batch_hidden_states, 1)\r\n",
        "    pooled_batch = pooled_batch / batch_lengths.expand_as(pooled_batch)\r\n",
        "\r\n",
        "    return pooled_batch\r\n",
        "\r\n",
        "def max_pooling(batch_hidden_states):\r\n",
        "    pooled_batch, _ = torch.max(batch_hidden_states, 1)\r\n",
        "    return pooled_batch\r\n",
        "\r\n",
        "def pack_rnn_input(embedded_sequence_batch, sequence_lengths):\r\n",
        "    sequence_lengths = sequence_lengths.cpu().numpy()\r\n",
        "\r\n",
        "    sorted_sequence_lengths = np.sort(sequence_lengths)[::-1]\r\n",
        "    sorted_sequence_lengths = torch.from_numpy(\r\n",
        "        sorted_sequence_lengths.copy()\r\n",
        "    )\r\n",
        "\r\n",
        "    idx_sort = np.argsort(-sequence_lengths)\r\n",
        "    idx_unsort = np.argsort(idx_sort)\r\n",
        "\r\n",
        "    idx_sort = torch.from_numpy(idx_sort)\r\n",
        "    idx_unsort = torch.from_numpy(idx_unsort)\r\n",
        "\r\n",
        "    if embedded_sequence_batch.is_cuda:\r\n",
        "        idx_sort = idx_sort.cuda()\r\n",
        "        idx_unsort = idx_unsort.cuda()\r\n",
        "\r\n",
        "    embedded_sequence_batch = embedded_sequence_batch.index_select(\r\n",
        "        0, idx_sort\r\n",
        "    )\r\n",
        "\r\n",
        "    packed_rnn_input = nn.utils.rnn.pack_padded_sequence(\r\n",
        "        embedded_sequence_batch, \r\n",
        "        sorted_sequence_lengths,\r\n",
        "        batch_first=True\r\n",
        "    )\r\n",
        "\r\n",
        "    return packed_rnn_input, idx_unsort\r\n",
        "\r\n",
        "  \r\n",
        "def unpack_rnn_output(packed_rnn_output, indices):\r\n",
        "    encoded_sequence_batch, _ = nn.utils.rnn.pad_packed_sequence(\r\n",
        "        packed_rnn_output, batch_first=True\r\n",
        "    )\r\n",
        "    encoded_sequence_batch = encoded_sequence_batch.index_select(0, indices)\r\n",
        "\r\n",
        "    return encoded_sequence_batch"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfd5Rh6AaO45"
      },
      "source": [
        "# Use the nn.module !\r\n",
        "\r\n",
        "class BiLSTM(nn.Module):\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        embeddings,\r\n",
        "        hidden_size,\r\n",
        "        num_labels,\r\n",
        "        input_dropout=0,\r\n",
        "        output_dropout=0,\r\n",
        "        bidirectional=True,\r\n",
        "        num_layers=2,\r\n",
        "        pooling='mean'\r\n",
        "    ):\r\n",
        "\r\n",
        "        super(BiLSTM, self).__init__()\r\n",
        "        self.embeddings = embeddings\r\n",
        "        self.pooling = pooling\r\n",
        "        self.input_dropout = nn.Dropout(input_dropout)\r\n",
        "        self.output_dropout = nn.Dropout(output_dropout)\r\n",
        "        self.bidirectional = bidirectional\r\n",
        "        self.num_layers = num_layers\r\n",
        "        self.num_labels = num_labels\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.input_size = self.embeddings.embedding_dim\r\n",
        "        self.lstm = nn.LSTM(\r\n",
        "            self.input_size,\r\n",
        "            hidden_size,\r\n",
        "            bidirectional=bidirectional,\r\n",
        "            num_layers=num_layers,\r\n",
        "            batch_first=True\r\n",
        "        )\r\n",
        "        self.total_hidden_size = self.hidden_size \r\n",
        "        if self.bidirectional:\r\n",
        "            self.total_hidden_size += self.hidden_size\r\n",
        "        self.output_layer = nn.Linear(\r\n",
        "            self.total_hidden_size,\r\n",
        "            self.num_labels)\r\n",
        "        self.loss_function = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    def forward(self, src_batch, tgt_batch=None):\r\n",
        "        src_sequences = src_batch.sequences\r\n",
        "        src_lengths = src_batch.lengths\r\n",
        "        embedded_sequence_batch = self.embeddings(src_sequences)\r\n",
        "        embedded_sequence_batch = self.input_dropout(\r\n",
        "            embedded_sequence_batch\r\n",
        "        )\r\n",
        "\r\n",
        "        packed_rnn_input, indices = pack_rnn_input(\r\n",
        "            embedded_sequence_batch, src_lengths\r\n",
        "        )\r\n",
        "\r\n",
        "        rnn_packed_output, _ = self.lstm(packed_rnn_input)\r\n",
        "        encoded_sequence_batch = unpack_rnn_output(\r\n",
        "            rnn_packed_output, indices\r\n",
        "        )\r\n",
        "\r\n",
        "        if self.pooling == \"mean\":\r\n",
        "            pooled_batch = mean_pooling(encoded_sequence_batch,\r\n",
        "                                        src_lengths)\r\n",
        "\r\n",
        "        elif self.pooling == \"max\":\r\n",
        "            pooled_batch = max_pooling(encoded_sequence_batch)\r\n",
        "        else:\r\n",
        "            raise NotImplementedError\r\n",
        "\r\n",
        "        logits = self.output_layer(pooled_batch)\r\n",
        "        _, predictions = logits.max(1)\r\n",
        "\r\n",
        "        if tgt_batch is not None:\r\n",
        "            targets = tgt_batch.sequences\r\n",
        "            loss = self.loss_function(logits, targets)\r\n",
        "        else:\r\n",
        "            loss = None\r\n",
        "\r\n",
        "        return loss, predictions, logits"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAPKBCsVaSSD"
      },
      "source": [
        "# Now we will set our hyperparameters!\r\n",
        "\r\n",
        "epochs = 5\r\n",
        "hidden_size = 300\r\n",
        "log_interval = 10\r\n",
        "num_labels = 2\r\n",
        "input_dropout = 0.5\r\n",
        "output_dropout = 0.5\r\n",
        "bidirectional = True\r\n",
        "num_layers = 2\r\n",
        "pooling = 'mean'\r\n",
        "lr = 0.001\r\n",
        "gradient_clipping = 0.25"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_17cduUan1C",
        "outputId": "5bb21948-3140-4af4-a764-86d251c2afd1"
      },
      "source": [
        "model = BiLSTM(\r\n",
        "    embeddings=embeddings,\r\n",
        "    hidden_size=hidden_size,\r\n",
        "    num_labels=num_labels,\r\n",
        "    input_dropout=input_dropout,\r\n",
        "    output_dropout=output_dropout,\r\n",
        "    bidirectional=bidirectional,\r\n",
        "    num_layers=num_layers,\r\n",
        "    pooling=pooling\r\n",
        ")\r\n",
        "\r\n",
        "model.to(device)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM(\n",
              "  (embeddings): Embedding(65055, 300, padding_idx=65054)\n",
              "  (input_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (output_dropout): Dropout(p=0.5, inplace=False)\n",
              "  (lstm): LSTM(300, 300, num_layers=2, batch_first=True, bidirectional=True)\n",
              "  (output_layer): Linear(in_features=600, out_features=2, bias=True)\n",
              "  (loss_function): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHcKX2imas3a"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akz5WAuabwHy",
        "outputId": "8bd8504a-7125-45ae-d6aa-84ab680b80dd"
      },
      "source": [
        "for epoch in range(epochs):\r\n",
        "    epoch_correct = 0\r\n",
        "    epoch_total = 0\r\n",
        "    epoch_loss = 0\r\n",
        "    i = 0\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    for batch in train_batches:\r\n",
        "        batch.to_torch_(device)\r\n",
        "        ids_batch = batch.indices\r\n",
        "        src_batch = batch.src\r\n",
        "        tgt_batch = batch.tgt\r\n",
        "        loss, predictions, logits = model.forward(\r\n",
        "            src_batch,\r\n",
        "            tgt_batch=tgt_batch\r\n",
        "        )\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        torch.nn.utils.clip_grad_norm_(\r\n",
        "            model.parameters(),\r\n",
        "            gradient_clipping)\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "        correct = (predictions == tgt_batch.sequences).long().sum()\r\n",
        "        total = tgt_batch.sequences.size(0)\r\n",
        "        epoch_correct += correct.item()\r\n",
        "        epoch_total += total\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        i += 1\r\n",
        "\r\n",
        "    accuracy  = 100 * epoch_correct / epoch_total\r\n",
        "\r\n",
        "    print('Epoch {}'.format(epoch))\r\n",
        "    print('Train Loss: {}'.format(epoch_loss / len(train_batches)))\r\n",
        "    print('Train Accuracy: {}'.format(accuracy))\r\n",
        "\r\n",
        "    test_epoch_correct = 0\r\n",
        "    test_epoch_total = 0\r\n",
        "    test_epoch_loss = 0\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    for batch in test_batches:\r\n",
        "        ids_batch = batch.indices\r\n",
        "        src_batch = batch.src\r\n",
        "        tgt_batch = batch.tgt\r\n",
        "        \r\n",
        "        batch.to_torch_(device)\r\n",
        "        loss, predictions, logits = model.forward(\r\n",
        "            src_batch,\r\n",
        "            tgt_batch=tgt_batch)\r\n",
        "\r\n",
        "        correct = (predictions == tgt_batch.sequences).long().sum()\r\n",
        "        total = tgt_batch.sequences.size(0)\r\n",
        "        test_epoch_correct += correct.item()\r\n",
        "        test_epoch_total += total\r\n",
        "        test_epoch_loss += loss.item()\r\n",
        "\r\n",
        "    test_accuracy = 100 * test_epoch_correct / test_epoch_total\r\n",
        "\r\n",
        "    print('\\n----------------------')\r\n",
        "    print('Test Loss: {}'.format(test_epoch_loss / len(test_batches)))\r\n",
        "    print('Test Accuracy: {}'.format(test_accuracy))\r\n",
        "    print('----------------------\\n')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train Loss: 0.4404130145907402\n",
            "Train Accuracy: 78.636\n",
            "\n",
            "----------------------\n",
            "Test Loss: 0.35737835097312926\n",
            "Test Accuracy: 84.908\n",
            "----------------------\n",
            "\n",
            "Epoch 1\n",
            "Train Loss: 0.302647270321846\n",
            "Train Accuracy: 87.512\n",
            "\n",
            "----------------------\n",
            "Test Loss: 0.32877466711401937\n",
            "Test Accuracy: 86.344\n",
            "----------------------\n",
            "\n",
            "Epoch 2\n",
            "Train Loss: 0.2385834891498089\n",
            "Train Accuracy: 90.132\n",
            "\n",
            "----------------------\n",
            "Test Loss: 0.3091318444907665\n",
            "Test Accuracy: 87.568\n",
            "----------------------\n",
            "\n",
            "Epoch 3\n",
            "Train Loss: 0.17792687624692916\n",
            "Train Accuracy: 93.084\n",
            "\n",
            "----------------------\n",
            "Test Loss: 0.3724753656387329\n",
            "Test Accuracy: 87.032\n",
            "----------------------\n",
            "\n",
            "Epoch 4\n",
            "Train Loss: 0.14103973174095155\n",
            "Train Accuracy: 94.596\n",
            "\n",
            "----------------------\n",
            "Test Loss: 0.3948624557703733\n",
            "Test Accuracy: 86.92\n",
            "----------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}